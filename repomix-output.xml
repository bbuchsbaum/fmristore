This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: R/**/*.R, R/**/*.r, *.Rmd, *.rmd, DESCRIPTION, tests/**/*.R, tests/**/*.r
- Files matching patterns in .gitignore are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
R/
  all_class.R
  all_generic.R
  assertions.R
  cluster_array.R
  cluster_experiment.R
  constructors.R
  h5_utils.R
  h5neurovec.R
  h5neurovol.R
  io_h5_generic.R
  io_h5_helpers.R
  io_write_h5.R
  labeled_vec.R
  latent_vec.R
  zzz_example_helpers.R
tests/
  testthat/
    test_fmristore_extras.R
    test_labeled_vec.R
    test_latent_vec_concat.R
    test_latent_vec_extra.R
    test_latent_vec.R
    test-cluster_experiment.R
    test-cluster_run_full.R
    test-cluster_run_summary.R
    test-h5classes.R
    test-readwriteh5.R
DESCRIPTION
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="R/assertions.R">
#' Assert that an argument is a non-empty numeric vector
#' 
#' @param x The value to check.
#' @param arg The name of the argument being checked (as a string).
#' @param fn The name of the calling function (as a string) for error messages.
#' @return Invisible `NULL` if check passes, otherwise stops with an error.
#' @keywords internal
#' @noRd
assert_non_empty_numeric <- function(x, arg, fn) {
  if (is.null(x) || !is.numeric(x) || length(x) == 0L) {
      stop(sprintf("[%s] Argument '%s' must be a non-empty numeric vector.", 
                   fn %||% "unknown function", 
                   arg %||% "unknown argument"))
  }
  invisible(NULL)
}

#' Assert that two objects have identical dimensions (or a subset thereof)
#' 
#' @param a First object (matrix, array, or object with a dim() method).
#' @param b Second object.
#' @param dims_to_compare Numeric vector specifying which dimensions to compare. 
#'   If `NULL` (default), compares all dimensions.
#' @param msg Optional custom error message prefix.
#' @return Invisible `NULL` if dimensions match, otherwise stops with an error.
#' @keywords internal
#' @noRd
check_same_dims <- function(a, b, dims_to_compare = NULL, msg = NULL) {
  dim_a <- dim(a)
  dim_b <- dim(b)
  
  # Select dimensions to compare
  dims_a_sub <- if (is.null(dims_to_compare)) dim_a else dim_a[dims_to_compare]
  dims_b_sub <- if (is.null(dims_to_compare)) dim_b else dim_b[dims_to_compare]
  
  # Handle potential errors if dims_to_compare is out of bounds
  if (anyNA(dims_a_sub) || anyNA(dims_b_sub)) {
     stop("check_same_dims: 'dims_to_compare' contains indices out of bounds for one or both objects.")
  }

  if (!identical(dims_a_sub, dims_b_sub)) {
     # Format dimensions for error message (show full dims for context)
     dim_a_str <- if (is.null(dim_a)) paste0("[", length(a), "]") else paste(dim_a, collapse = "x")
     dim_b_str <- if (is.null(dim_b)) paste0("[", length(b), "]") else paste(dim_b, collapse = "x")
     compare_str <- if (is.null(dims_to_compare)) "all dims" else paste0("dims ", paste(dims_to_compare, collapse=","))
     
     stop(paste0(msg %||% "Dimension mismatch: ", 
                 "Object A dims [", dim_a_str, "] vs Object B dims [", dim_b_str, "]",
                 " (comparing ", compare_str, ")"))
  }
  invisible(NULL)
}

#' Validate that two objects have identical dimensions (or a subset thereof)
#' 
#' Wraps `check_same_dims` to return NULL on success or the error message on failure.
#' Useful within validation functions that collect multiple errors.
#' 
#' @param a First object (matrix, array, or object with a dim() method).
#' @param b Second object.
#' @param dims_to_compare Numeric vector specifying which dimensions to compare. 
#'   If `NULL` (default), compares all dimensions.
#' @param msg Optional custom error message prefix passed to `check_same_dims`.
#' @return `NULL` if dimensions match, otherwise the error message string from `check_same_dims`.
#' @keywords internal
#' @noRd
validate_same_dims <- function(a, b, dims_to_compare = NULL, msg = NULL) {
  result <- tryCatch({
    check_same_dims(a = a, b = b, dims_to_compare = dims_to_compare, msg = msg)
    NULL # Return NULL explicitly on success
  }, error = function(e) {
    conditionMessage(e) # Return the error message string on failure
  })
  return(result)
}

#' Helper for default value if NULL (copied from cluster_experiment.R temporarily, should be centralized)
#' 
#' @param a First object.
#' @param b Second object.
#' @return `b` if `a` is `NULL`, otherwise `a`.
#' @keywords internal
#' @noRd
`%||%` <- function(a, b) if (is.null(a)) b else a
</file>

<file path="R/h5neurovec.R">
#' @include all_class.R
NULL

#' H5NeuroVecSource
#'
#' @description
#' Creates a source object for building \code{\link{H5NeuroVec}} instances from an HDF5 file.
#'
#' @param file_name A \code{character} string specifying the HDF5 file name.
#' @return A new \code{H5NeuroVecSource} object (internal).
#' @keywords internal
#' @noRd
H5NeuroVecSource <- function(file_name) {
  new("H5NeuroVecSource", file_name = file_name)
}

#' Read a 4D HDF5 file 
#'
#' @param file_name The path to a 4D HDF5 file.
#' @return A new \code{H5NeuroVecSource} object.
#' @keywords internal
#' @noRd
read_vec <- function(file_name) {
  assert_that(is.character(file_name))
  assert_that(file.exists(file_name))

  new("H5NeuroVecSource", file_name = file_name)
}

#' H5NeuroVec Constructor
#'
#' @description
#' Constructs an \code{\link{H5NeuroVec}} object, which represents a 4D brain image
#' stored in an HDF5 file. The HDF5 file is opened in read-only mode.
#'
#' @details
#' This constructor is used for reading existing HDF5 files that conform
#' to the H5NeuroVec specification (typically created via \code{as_h5("NeuroVec", ...)}).
#'
#' @section Lifecycle Management:
#' When an \code{H5NeuroVec} object is created by providing a \code{file_name},
#' it opens the specified HDF5 file and maintains an open handle to it.
#' **It is the user's responsibility to explicitly close this handle** when the
#' object is no longer needed to release system resources. This can be done by calling
#' \code{close(your_h5neurovec_object)}.
#'
#' Failure to close the handle may lead to issues such as reaching file handle
#' limits or problems with subsequent access to the file.
#'
#' @param file_name The path to an existing 4D HDF5 neuroimaging file.
#'
#' @return A new \code{\link{H5NeuroVec-class}} instance with an open HDF5 file handle.
#'
#' @seealso \code{\link{close.H5NeuroVec}} for closing the file handle, \code{\link[neuroim2]{NeuroVec-class}}
#'
#' @examples
#' \dontrun{
#' # Assuming "my_vec.h5" is a valid H5NeuroVec HDF5 file
#' h5vec <- H5NeuroVec("my_vec.h5")
#' # ... perform operations with h5vec ...
#' print(dim(h5vec))
#' # Important: Close the handle when done
#' close(h5vec)
#' }
#' @importFrom assertthat assert_that
#' @importFrom neuroim2 NeuroSpace
#' @export
H5NeuroVec <- function(file_name) {
  assert_that(is.character(file_name))
  assert_that(file.exists(file_name))

  h5obj <- hdf5r::H5File$new(file_name)

  # Check the 'rtype' attribute
  rtype <- try(hdf5r::h5attr(h5obj, which="rtype"), silent=TRUE)
  if (!is.character(rtype) || rtype != "DenseNeuroVec") {
    stop("Invalid HDF5 file for H5NeuroVec: ", file_name)
  }

  # Check dimension count
  if (length(h5obj[["space/dim"]][]) != 4) {
    stop(
      "Cannot create H5NeuroVec: file must have 4 dimensions; found: ",
      paste(h5obj[["space/dim"]][], collapse=" ")
    )
  }

  # Build NeuroSpace
  sp <- NeuroSpace(
    dim    = h5obj[["space/dim"]][],
    origin = h5obj[["space/origin"]][],
    trans  = h5obj[["space/trans"]][,]
  )

  new("H5NeuroVec", space=sp, obj=h5obj)
}

#' Convert DenseNeuroVec to H5NeuroVec
#'
#' @description
#' Converts a \code{DenseNeuroVec} object to an \code{H5NeuroVec} by writing it to HDF5.
#'
#' @param from A \code{DenseNeuroVec} object.
#' @return An \code{H5NeuroVec} referencing the newly created HDF5 file.
#'
#' @keywords internal
#' @name DenseNeuroVec,H5NeuroVec
setAs(
  from = "DenseNeuroVec",
  to   = "H5NeuroVec",
  def  = function(from) {
    to_nih5_vec(from, file_name=NULL, data_type="FLOAT")
  }
)

#' @export
setMethod(
  f   = "series",
  signature = signature(x="H5NeuroVec", i="integer"),
  definition = function(x, i) {
    assertthat::assert_that(max(i) <= dim(x)[4])
    assertthat::assert_that(min(i) >= 1)
    x@obj[["data"]][,,, i, drop=FALSE]
  }
)

#' @export
setMethod(
  f = "series",
  signature = signature(x="H5NeuroVec", i="numeric"),
  definition = function(x, i) {
    callGeneric(x, as.integer(i))
  }
)

#' @export
setMethod(
  f = "series",
  signature = signature(x="H5NeuroVec", i="matrix"),
  definition = function(x, i) {
    assertthat::assert_that(ncol(i) == 3)
    assertthat::assert_that(max(i) <= prod(dim(x)[1:3]))
    assertthat::assert_that(min(i) >= 1)
    x@obj[["data"]][i]
  }
)

#' @param drop Whether to drop dimensions of length 1
#' @rdname series-methods
#' @export
setMethod(
  f = "series",
  signature = signature(x="H5NeuroVec", i="integer"),
  definition = function(x, i, j, k, drop=TRUE) {
    if (missing(j) && missing(k)) {
      # i is a linear index; convert to (x,y,z)
      grid <- indexToGridCpp(i, dim(x)[1:3])
      callGeneric(x, grid)
    } else {
      # Possibly expand.grid approach
      assertthat::assert_that(
        length(i)==1 && length(j)==1 && length(k)==1,
        msg="Expecting single-voxel indices for i,j,k"
      )
      ret <- x@obj[["data/elements"]][i,j,k,]
      if (drop) drop(ret) else ret
    }
  }
)

#' @rdname series-methods
#' @export
setMethod(
  f = "series",
  signature = signature(x="H5NeuroVec", i="numeric"),
  definition = function(x, i, j, k) {
    if (missing(j) && missing(k)) {
      callGeneric(x, as.integer(i))
    } else {
      callGeneric(x, as.integer(i), as.integer(j), as.integer(k))
    }
  }
)

#' @rdname series-methods
#' @export
setMethod(
  f = "series",
  signature = signature(x="H5NeuroVec", i="matrix"),
  definition = function(x, i) {
    assertthat::assert_that(ncol(i) == 3)
    d4 <- dim(x)[4]

    # Build bounding box for i
    ir <- lapply(seq_len(ncol(i)), function(j) seq(min(i[,j]), max(i[,j])))

    # e.g. sub-block
    ret <- x@obj[["data/elements"]][
      ir[[1]][1]:ir[[1]][length(ir[[1]])],
      ir[[2]][1]:ir[[2]][length(ir[[2]])],
      ir[[3]][1]:ir[[3]][length(ir[[3]])],
      , drop=FALSE
    ]

    # flatten
    ret2 <- t(array(ret, c(prod(dim(ret)[1:3]), dim(ret)[4])))
    # check if shape matches nrow(i)
    if (ncol(ret2) != nrow(i)) {
      i2 <- apply(i, 2, function(ind) {
        ind - min(ind) + 1
      })
      i3 <- gridToIndex3DCpp(dim(ret)[1:3], i2)
      ret2[, i3, drop=FALSE]
    } else {
      ret2
    }
  }
)


#' @export
#' @rdname linear_access-methods
setMethod(
  f = "linear_access",
  signature = signature(x="H5NeuroVec", i="numeric"),
  definition = function(x, i) {
    # 1) Convert linear => (x,y,z,t)
    coords <- arrayInd(i, dim(x))
    n <- nrow(coords)

    # 2) bounding box
    minX <- min(coords[,1]); maxX <- max(coords[,1])
    minY <- min(coords[,2]); maxY <- max(coords[,2])
    minZ <- min(coords[,3]); maxZ <- max(coords[,3])
    minT <- min(coords[,4]); maxT <- max(coords[,4])

    # 3) Read sub-block
    dset <- x@obj[["data/elements"]]
    sub4d <- dset[minX:maxX, minY:maxY, minZ:maxZ, minT:maxT, drop=FALSE]

    # 4) Flatten sub4d
    sub4d_vec <- as.vector(sub4d)

    # 5) Compute local offsets
    offX <- coords[,1] - minX + 1
    offY <- coords[,2] - minY + 1
    offZ <- coords[,3] - minZ + 1
    offT <- coords[,4] - minT + 1

    lenX <- maxX - minX + 1
    lenY <- maxY - minY + 1
    lenZ <- maxZ - minZ + 1

    sub_lin_idx <- offX +
      (offY - 1L)* lenX +
      (offZ - 1L)* lenX*lenY +
      (offT - 1L)* lenX*lenY*lenZ

    out_vals <- sub4d_vec[sub_lin_idx]
    out_vals
  }
)

#' @export
#' @rdname linear_access-methods
setMethod(
  f = "linear_access",
  signature = signature(x="H5NeuroVec", i="integer"),
  definition = function(x, i) {
    callGeneric(x, as.numeric(i))
  }
)

setMethod(
  f = "[",
  signature = signature(x="H5NeuroVec", i="numeric", j="numeric", drop="ANY"),
  definition = function(x, i, j, k, l, ..., drop=TRUE) {
    # Provide defaults if missing
    if (missing(i)) i <- seq_len(dim(x)[1])
    if (missing(j)) j <- seq_len(dim(x)[2])
    if (missing(k)) k <- seq_len(dim(x)[3])
    if (missing(l)) l <- seq_len(dim(x)[4])

    # If *any* of i, j, k, l is empty => result is an empty array
    #   e.g. shape [ length(i), length(j), length(k), length(l)] with one dimension = 0
    if (!length(i) || !length(j) || !length(k) || !length(l)) {
      # Build an empty array. The dimension that's length-0
      # remains 0 in the result.
      out_dim <- c(length(i), length(j), length(k), length(l))
      out_arr <- array(numeric(0), dim=out_dim)
      if (drop) out_arr <- drop(out_arr)
      return(out_arr)
    }

    # Otherwise do normal bounding-box approach
    # -------------------------------------------------------------
    i <- as.numeric(i); j <- as.numeric(j); k <- as.numeric(k); l <- as.numeric(l)

    # Basic range checks
    dims_x <- dim(x)
    if (any(i < 1 | i > dims_x[1]) ||
        any(j < 1 | j > dims_x[2]) ||
        any(k < 1 | k > dims_x[3]) ||
        any(l < 1 | l > dims_x[4])) {
      stop("Subscript out of range in H5NeuroVec dimensions.")
    }

    minI <- min(i); maxI <- max(i)
    minJ <- min(j); maxJ <- max(j)
    minK <- min(k); maxK <- max(k)
    minL <- min(l); maxL <- max(l)

    dset <- x@obj[["data/elements"]]
    subvol <- dset[minI:maxI, minJ:maxJ, minK:maxK, minL:maxL, drop=FALSE]

    i_off <- i - minI + 1
    j_off <- j - minJ + 1
    k_off <- k - minK + 1
    l_off <- l - minL + 1

    subdimI <- maxI - minI + 1
    subdimJ <- maxJ - minJ + 1
    subdimK <- maxK - minK + 1

    subvol_vec <- as.vector(subvol)

    nI <- length(i)
    nJ <- length(j)
    nK <- length(k)
    nL <- length(l)

    out_dim <- c(nI, nJ, nK, nL)
    N <- nI * nJ * nK * nL

    idx_i <- rep(seq_len(nI), times=nJ*nK*nL)
    idx_j <- rep(rep(seq_len(nJ), each=nI), times=nK*nL)
    idx_k <- rep(rep(seq_len(nK), each=nI*nJ), times=nL)
    idx_l <- rep(seq_len(nL), each=nI*nJ*nK)

    loc_i <- i_off[idx_i]
    loc_j <- j_off[idx_j]
    loc_k <- k_off[idx_k]
    loc_l <- l_off[idx_l]

    sub_lin_idx <- loc_i +
      (loc_j - 1)* subdimI +
      (loc_k - 1)* subdimI*subdimJ +
      (loc_l - 1)* subdimI*subdimJ*subdimK

    out_vals <- subvol_vec[sub_lin_idx]

    arr_out <- array(out_vals, dim=out_dim)
    if (drop) {
      arr_out <- drop(arr_out)
    }
    arr_out
  }
)

#' Load data from an H5NeuroVecSource object
#'
#' @description
#' Loads a \code{\link{H5NeuroVec}} from an \code{H5NeuroVecSource},
#' effectively calling \code{H5NeuroVec()} on the stored file path.
#'
#' @param x A \code{H5NeuroVecSource} with \code{file_name}.
#' @return A new \code{H5NeuroVec}.
#'
#' @seealso \code{\link{H5NeuroVecSource}}, \code{\link{H5NeuroVec}}
#'
#' @noRd
setMethod(
  f = "load_data",
  signature = c("H5NeuroVecSource"),
  definition = function(x) {
    H5NeuroVec(x@file_name)
  }
)


#' @export
#' @rdname series-methods
setMethod(
  f = "series",
  signature = signature(x="H5NeuroVec", i="integer"),
  definition = function(x, i, j, k, drop=TRUE) {

    # If user provided only 'i', treat it as a vector of linear voxel indices
    if (missing(j) && missing(k)) {
      # compute total # of voxels in the 3D part
      nels <- prod(dim(x)[1:3])

      # We'll build linear indices for all timepoints
      # offsets = c(0, nels, 2*nels, ..., (nTime-1)*nels)
      offsets <- seq(0, (dim(x)[4]-1)) * nels

      # For each element in i, we add each offset => flatten
      # 'map' is from purrr or we can do a base R loop
      idx_list <- lapply(i, function(pos) pos + offsets)
      idx <- unlist(idx_list, use.names=FALSE)

      # Now we rely on x[idx] => must be valid 1D subscript
      # (which is handled by your `[.H5NeuroVec` or `linear_access()` method)
      vals <- x[idx]  # => numeric vector of length length(i)*nTime

      # Reshape => [nTime, length(i)]
      ret <- matrix(vals, nrow=dim(x)[4], ncol=length(i))
      if (drop) drop(ret) else ret

    } else {
      # If user provided i,j,k => single voxel across time
      # Confirm each is length=1
      stopifnot(length(i)==1 && length(j)==1 && length(k)==1)
      # Subset the entire time dimension
      ret <- x[i, j, k, seq_len(dim(x)[4])]
      if (drop) drop(ret) else ret
    }
  }
)

#' @export
#' @rdname series-methods
setMethod(
  f = "series",
  signature = signature(x="H5NeuroVec", i="numeric"),
  definition = function(x, i, j, k, drop=TRUE) {
    callGeneric(x, as.integer(i), as.integer(j), as.integer(k), drop=drop)
  })

#' Convert NeuroVec to HDF5 Format, returning an H5NeuroVec
#'
#' @description
#' Creates an HDF5 file from a 4D \code{\link[neuroim2]{NeuroVec}} object, storing
#' data & spatial info. Returns an \code{\link{H5NeuroVec}} referencing the file
#' for on-demand access.
#'
#' @param vec A \code{NeuroVec} (4D) to convert (coerced internally to \code{DenseNeuroVec}).
#' @param file_name Path for the output HDF5 file (if NULL, a temp file is used).
#' @param data_type Storage type (e.g., "FLOAT"). Default "FLOAT".
#' @param chunk_dim Chunk dimensions. Default c(4,4,4,dim(vec)[4]).
#' @param nbit \code{logical}: use N-bit filter? Default \code{FALSE}.
#' @param compression \code{integer} [0..9], default 6.
#'
#' @return An \code{H5NeuroVec} referencing the new HDF5 file.
#'
#' @details
#' 1. Coerces \code{vec} to \code{DenseNeuroVec}
#' 2. Writes data to \code{/data/elements} in HDF5
#' 3. Writes dimension/origin/spacing/transform in \code{/space}
#' 4. Tags file attribute "rtype" => "DenseNeuroVec"
#' 5. Returns \code{H5NeuroVec}
#'
#' @keywords internal
#' @importFrom lifecycle deprecate_warn
#' @export
to_nih5_vec <- function(vec,
                        file_name   = NULL,
                        data_type   = "FLOAT",
                        chunk_dim   = c(4,4,4, dim(vec)[4]),
                        nbit        = FALSE,
                        compression = 6)
{
  if (!requireNamespace("hdf5r", quietly=TRUE)) {
    stop("Package 'hdf5r' must be installed for HDF5 I/O.", call.=FALSE)
  }

  # 1) Coerce to DenseNeuroVec
  vec <- as(vec, "DenseNeuroVec")

  chunk_dim <- pmin(chunk_dim, dim(vec))

  # 2) Check compression
  assert_that(compression >= 0 && compression <= 9)

  # 3) If no file_name, use temp
  if (is.null(file_name)) {
    file_name <- tempfile(fileext=".h5")
  }

  # 4) Create or overwrite
  h5obj <- hdf5r::H5File$new(file_name, mode="w")

  # 5) Setup dimension
  space_ds <- hdf5r::H5S$new(dims=dim(vec), maxdims=dim(vec))
  dtype_pl  <- hdf5r::H5P_DATASET_CREATE$new()

  # 6) Data type
  h5dtype <- switch(data_type,
                    "BINARY"  = hdf5r::h5types$H5T_NATIVE_HBOOL,
                    "SHORT"   = hdf5r::h5types$H5T_NATIVE_SHORT,
                    "INT"     = hdf5r::h5types$H5T_NATIVE_INT,
                    "INTEGER" = hdf5r::h5types$H5T_NATIVE_INT,
                    "FLOAT"   = hdf5r::h5types$H5T_NATIVE_FLOAT,
                    "DOUBLE"  = hdf5r::h5types$H5T_NATIVE_DOUBLE,
                    "LONG"    = hdf5r::h5types$H5T_NATIVE_LONG,
                    NULL
  )
  if (is.null(h5dtype)) {
    stop("Unsupported 'data_type': ", data_type)
  }

  # 7) Chunk/fill/compression
  dtype_pl$set_chunk(chunk_dim)$set_fill_value(h5dtype,0)$set_deflate(compression)
  if (nbit && compression>0) {
    dtype_pl$set_nbit()
  }

  # 8) Tag the file
  hdf5r::h5attr(h5obj, "rtype") <- "DenseNeuroVec"

  # 9) /data & /space
  dgroup <- h5obj$create_group("data")
  sgroup <- h5obj$create_group("space")

  # 10) dataset "elements"
  dset <- dgroup$create_dataset(
    name             = "elements",
    space            = space_ds,
    dtype            = h5dtype,
    dataset_create_pl= dtype_pl,
    chunk_dims       = chunk_dim,
    gzip_level       = compression
  )

  # 11) space metadata
  sgroup[["dim"]]     <- dim(vec)
  sgroup[["spacing"]] <- spacing(vec)
  sgroup[["origin"]]  <- origin(space(vec))
  sgroup[["trans"]]   <- trans(space(vec))

  # 12) Write data
  dset[,,,] <- as.array(vec)

  # 13) Return new H5NeuroVec
  new("H5NeuroVec", space=space(vec), obj=h5obj)
}

#' Display an H5NeuroVec object
#'
#' @description
#' Prints a concise summary of the H5NeuroVec object, including
#' 4D dimensions, spacing, origin, and orientation.
#'
#' @param object An \code{H5NeuroVec} object to show.
#'
#' @importFrom crayon bold blue silver yellow green
#' @importFrom utils object.size
#' @export
setMethod(
  f = "show",
  signature = signature(object="H5NeuroVec"),
  definition = function(object) {
    # Basic header
    cat("\n", crayon::bold(crayon::blue("H5NeuroVec")), "\n", sep="")

    # Gather dimension info
    d <- dim(object)  # c(X, Y, Z, nVol)
    cat(crayon::bold("\n╔═ Dimensions "), crayon::silver("───────────────────────────"), "\n", sep="")
    cat("║ ", crayon::yellow("Spatial (X×Y×Z)"), " : ",
        paste(d[1:3], collapse=" × "), "\n", sep="")
    cat("║ ", crayon::yellow("Number of Volumes"), " : ", d[4], "\n", sep="")

    # Spacing, origin
    sp  <- space(object)
    cat(crayon::bold("\n╠═ Spatial Info "), crayon::silver("───────────────────────────"), "\n", sep="")
    cat("║ ", crayon::yellow("Spacing"), "       : ", paste(round(sp@spacing,2), collapse=" × "), "\n", sep="")
    cat("║ ", crayon::yellow("Origin"), "        : ", paste(round(sp@origin,2), collapse=" × "), "\n", sep="")

    # If axes are known, show them; else fallback
    if (length(sp@axes@ndim) >= 3) {
      cat("║ ", crayon::yellow("Orientation"), "   : ",
          paste(sp@axes@i@axis, sp@axes@j@axis, sp@axes@k@axis), "\n", sep="")
    } else {
      cat("║ ", crayon::yellow("Orientation"), "   : Unknown\n")
    }

    # HDF5 file info
    cat(crayon::bold("\n╚═ Storage Info "), crayon::silver("──────────────────────────"), "\n", sep="")
    if (object@obj$is_valid) {
      cat("  ", crayon::yellow("File"), " : ", object@obj$get_filename(), "\n", sep="")
    } else {
      cat("  ", crayon::yellow("File"), " : (CLOSED HDF5 File)\n", sep="")
    }
    cat("  ", crayon::yellow("Dataset"), " : /data/elements\n", sep="")
    cat("\n")
  }
)

#' Close the HDF5 file associated with an H5NeuroVec
#'
#' This method manually closes the HDF5 file handle stored within the
#' H5NeuroVec object. It uses the \code{safe_h5_close} helper.
#'
#' @param con An \code{H5NeuroVec} object.
#' @param ... Additional arguments (ignored).
#' @return Invisibly returns \code{NULL}.
#' @rdname close
#' @export
setMethod("close", "H5NeuroVec", function(con, ...) {
  if (!is.null(con@obj) && inherits(con@obj, "H5File")) {
    safe_h5_close(con@obj)
    # Do not null out con@obj, as this can cause S4 validation errors
    # The hdf5r object will become invalid upon closing.
  }
  invisible(NULL)
})
</file>

<file path="R/io_h5_generic.R">
#' @rdname as_h5-methods
#' @export
setMethod(
  f = "as_h5",
  signature = signature(object = "NeuroVec"),
  definition = function(object, file = NULL, data_type = "FLOAT",
                         chunk_dim = c(4, 4, 4, dim(object)[4]),
                         compression = 6) {
    to_nih5_vec(object, file_name = file, data_type = data_type,
                chunk_dim = chunk_dim, compression = compression)
  }
)

#' @rdname as_h5-methods
#' @export
setMethod(
  f = "as_h5",
  signature = signature(object = "LatentNeuroVec"),
  definition = function(object, file = NULL, data_type = "FLOAT",
                         compression = 6) {
    to_h5_latentvec(object, file_name = file, data_type = data_type,
                    compression = compression)
  }
)

#' @rdname as_h5-methods
#' @export
setMethod(
  f = "as_h5",
  signature = signature(object = "LabeledVolumeSet"),
  definition = function(object, file, mask, labels, compression = 4,
                         dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE,
                         chunk_size = 1024, header_values = list()) {
    write_labeled_vec(vec = object, mask = mask, labels = labels, file = file,
                      compression = compression, dtype = dtype,
                      chunk_size = chunk_size, header_values = header_values)
  }
)

#' @rdname as_h5-methods
#' @export
setMethod(
  f = "as_h5",
  signature = signature(object = "list"),
  definition = function(object, file, scan_names, mask, clusters,
                         scan_metadata, cluster_metadata = NULL,
                         summary_only = FALSE, compression = 4,
                         chunk_size = 1024) {
    write_clustered_dataset(file = file, vecs = object, scan_names = scan_names,
                           mask = mask, clusters = clusters,
                           scan_metadata = scan_metadata,
                           cluster_metadata = cluster_metadata,
                           summary_only = summary_only,
                           compression = compression,
                           chunk_size = chunk_size)
  }
)
</file>

<file path="R/io_h5_helpers.R">
# Helper functions for HDF5 I/O

#' Check if an object is an H5File handle
#' @param x Object to check
#' @return Logical TRUE if inherits from H5File, FALSE otherwise.
#' @keywords internal
is_h5file <- function(x) inherits(x, "H5File")



#' Read /mask into a LogicalNeuroVol
#' @param h5 An open `H5File` handle.
#' @param dset The path to the mask dataset (default: "/mask").
#' @param ref_space A `NeuroSpace` object representing the expected space of the mask.
#' @return A `LogicalNeuroVol` object corresponding to the mask dataset.
#' @keywords internal
#' @importFrom neuroim2 LogicalNeuroVol NeuroSpace space
#' @importFrom assertthat assert_that
read_h5_mask_to_LogicalNeuroVol <- function(h5, dset = "/mask", ref_space) {
  assert_that(inherits(ref_space, "NeuroSpace"), msg = "read_h5_mask_to_LogicalNeuroVol: ref_space must be a NeuroSpace object")
  assert_that(h5$exists(dset), msg = sprintf("read_h5_mask_to_LogicalNeuroVol: mask dataset not found at %s", dset))

  mask_dset <- h5[[dset]]
  mask_data_raw <- NULL
  dims <- NULL
  tryCatch({
      # Get dimensions FIRST from the dataset object
      dims <- mask_dset$dims
      if (is.null(dims) || length(dims) != 3) { # Expecting 3D mask
          stop(sprintf("Dataset '%s' does not have expected 3 dimensions. Found dimensions: %s", 
                       dset, paste(dims, collapse="x")))
      }
      
      # Validate dataset dimensions against the reference space
      ref_dims <- dim(ref_space)
      if (!identical(dims, ref_dims)) {
          stop(sprintf(
              "Mask dimensions in HDF5 (%s) do not match reference space dimensions (%s) for dataset '%s'", 
              paste(dims, collapse="x"), 
              paste(ref_dims, collapse="x"), 
              dset
          ))
      }
      
      # Now read the data (might be flattened)
      mask_data_raw <- mask_dset$read()
  }, finally = {
      if (!is.null(mask_dset) && mask_dset$is_valid) try(mask_dset$close(), silent=TRUE)
  })
  
  if (is.null(mask_data_raw)) stop("Failed to read mask data from ", dset)

  # Reshape potentially flattened data using stored dimensions, coerce to logical
  mask_data_logical <- array(as.logical(mask_data_raw), dim = dims)
  
  # Construct and return LogicalNeuroVol using the reference space
  LogicalNeuroVol(mask_data_logical, ref_space)
}

#' Read /cluster_map (+ coords) into ClusteredNeuroVol
#' @keywords internal
#' @importFrom neuroim2 ClusteredNeuroVol space
read_h5_clusters_to_ClusteredNeuroVol <- function(h5, mask,
                                                  map_dset = "/cluster_map") {

                                              
  stopifnot(h5$exists(map_dset))
  cmap_dset <- h5[[map_dset]]
  vec <- NULL
  tryCatch({
      # Use $read() here too for consistency, although it's expected to be 1D
      vec <- as.integer(cmap_dset$read())
  }, finally = {
      if (!is.null(cmap_dset) && cmap_dset$is_valid) try(cmap_dset$close(), silent=TRUE)
  })
  if (is.null(vec)) stop("Failed to read cluster map from ", map_dset)
  
  if (length(vec) != sum(mask)) {
    stop(sprintf(
        "Length of %s (%d) does not equal number of TRUE voxels in mask (%d)", 
        map_dset, length(vec), sum(mask)
    ))
  }
  # Pass vector and space (extracted from mask) to constructor
  ClusteredNeuroVol(mask, vec) 
} 

#' Safely close an HDF5 object (file, group, dataset, attribute, dataspace, property list)
#'
#' Checks if the object is valid and not NULL before attempting to close.
#'
#' @keywords internal
close_h5_safely <- function(h5obj) {
  if (!is.null(h5obj) && h5obj$is_valid) {
    try(h5obj$close(), silent = TRUE)
  }
}
</file>

<file path="tests/testthat/test_latent_vec_concat.R">
# ------------------------------------------------------------------------------
# test_latent_vec_concat.R - Test suite for concat() method on LatentNeuroVec
# ------------------------------------------------------------------------------
context("LatentNeuroVec :: concat()")

# Helper to create test objects with configurable time points
create_test_latent_vecs <- function(t1 = 10, t2 = 5, compatible = TRUE) {
  # Create spatial components - same for both objects to ensure compatibility
  dims_3d <- c(6, 6, 4)
  dims_4d_1 <- c(dims_3d, t1)
  dims_4d_2 <- c(dims_3d, t2)
  
  sp1 <- NeuroSpace(dims_4d_1)
  sp2 <- NeuroSpace(dims_4d_2)
  
  # Create same mask for both objects
  mask_arr <- array(TRUE, dim = dims_3d)
  # Make some voxels FALSE to test mask handling
  mask_arr[1:2, 1:2, 1] <- FALSE
  mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(dims_3d))
  
  n_vox <- sum(mask_vol)
  k <- 3 # Number of components
  
  # Create basis matrices with appropriate time dimensions
  basis1 <- Matrix(rnorm(t1 * k), nrow = t1, ncol = k)
  basis2 <- Matrix(rnorm(t2 * k), nrow = t2, ncol = k)
  
  # Use same loadings for compatibility
  loadings <- Matrix(rnorm(n_vox * k), nrow = n_vox, ncol = k)
  
  # Create a second loadings matrix for incompatible test
  loadings2 <- if (compatible) loadings else Matrix(rnorm(n_vox * k), nrow = n_vox, ncol = k)
  
  # Create offset
  offset <- rnorm(n_vox)
  
  # Create LatentNeuroVec objects
  lvec1 <- LatentNeuroVec(
    basis = basis1,
    loadings = loadings,
    space = sp1,
    mask = mask_vol,
    offset = offset,
    label = "test_lvec_1"
  )
  
  lvec2 <- LatentNeuroVec(
    basis = basis2,
    loadings = loadings2, # Use either same or different loadings
    space = sp2,
    mask = mask_vol,
    offset = offset,
    label = "test_lvec_2"
  )
  
  list(lvec1 = lvec1, lvec2 = lvec2)
}

test_that("concat produces LatentNeuroVec when spatial components match", {
  # Create compatible test objects
  test_vecs <- create_test_latent_vecs(t1 = 10, t2 = 5, compatible = TRUE)
  lvec1 <- test_vecs$lvec1
  lvec2 <- test_vecs$lvec2
  
  # Concatenate and check result
  result <- concat(lvec1, lvec2)
  
  # Should return a LatentNeuroVec
  expect_s4_class(result, "LatentNeuroVec")
  
  # Check dimensions - time should be sum of input times
  expect_equal(dim(result)[4], 15) # 10 + 5 = 15
  
  # Check basis shape
  expect_equal(dim(result@basis), c(15, 3)) # 15 timepoints, 3 components
  
  # Check structure is preserved
  expect_equal(dim(space(result))[1:3], dim(space(lvec1))[1:3]) # Spatial dims match
  expect_equal(as.array(mask(result)), as.array(mask(lvec1))) # Mask unchanged
  
  # Check basis concatenation - first part should match lvec1
  expect_equal(
    as.matrix(result@basis[1:10,]), 
    as.matrix(lvec1@basis),
    tolerance = 1e-12
  )
  
  # Second part should match lvec2
  expect_equal(
    as.matrix(result@basis[11:15,]), 
    as.matrix(lvec2@basis),
    tolerance = 1e-12
  )
  
  # Loadings should be identical to the first object
  expect_equal(
    as.matrix(result@loadings), 
    as.matrix(lvec1@loadings),
    tolerance = 1e-12
  )
})

test_that("concat falls back to NeuroVecSeq when spatial components don't match", {
  # Create incompatible test objects
  test_vecs <- create_test_latent_vecs(t1 = 8, t2 = 7, compatible = FALSE)
  lvec1 <- test_vecs$lvec1
  lvec2 <- test_vecs$lvec2
  
  # Concatenate and check result
  result <- concat(lvec1, lvec2)
  
  # Should return a NeuroVecSeq
  expect_s4_class(result, "NeuroVecSeq")
  
  # Check that both objects are stored
  expect_equal(length(result@vecs), 2)
})

test_that("concat handles mixed object types by falling back to NeuroVecSeq", {
  # Create a LatentNeuroVec
  test_vecs <- create_test_latent_vecs(t1 = 8, t2 = 7, compatible = TRUE)
  lvec1 <- test_vecs$lvec1
  
  # Create a different type of object (e.g., DenseNeuroVol)
  dims_3d <- c(6, 6, 4)
  sp <- NeuroSpace(dims_3d)
  vol_data <- array(rnorm(prod(dims_3d)), dim = dims_3d)
  dense_vol <- DenseNeuroVol(vol_data, sp)
  
  # Concatenate and check result
  result <- concat(lvec1, dense_vol)
  
  # Should return a NeuroVecSeq
  expect_s4_class(result, "DenseNeuroVec")
})

test_that("concat handles three or more objects correctly", {
  # Create three compatible test objects
  dims_3d <- c(6, 6, 4)
  sp1 <- NeuroSpace(c(dims_3d, 5))
  sp2 <- NeuroSpace(c(dims_3d, 3))
  sp3 <- NeuroSpace(c(dims_3d, 4))
  
  mask_arr <- array(TRUE, dim = dims_3d)
  mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(dims_3d))
  
  n_vox <- sum(mask_vol)
  k <- 2 # Number of components
  
  loadings <- Matrix(rnorm(n_vox * k), nrow = n_vox, ncol = k)
  offset <- rnorm(n_vox)
  
  basis1 <- Matrix(rnorm(5 * k), nrow = 5, ncol = k)
  basis2 <- Matrix(rnorm(3 * k), nrow = 3, ncol = k)
  basis3 <- Matrix(rnorm(4 * k), nrow = 4, ncol = k)
  
  lvec1 <- LatentNeuroVec(basis1, loadings, sp1, mask_vol, offset, "lvec1")
  lvec2 <- LatentNeuroVec(basis2, loadings, sp2, mask_vol, offset, "lvec2")
  lvec3 <- LatentNeuroVec(basis3, loadings, sp3, mask_vol, offset, "lvec3")
  
  # Concatenate all three
  result <- concat(lvec1, lvec2, lvec3)
  
  # Should return a LatentNeuroVec
  expect_s4_class(result, "LatentNeuroVec")
  
  # Check dimensions - time should be sum of input times
  expect_equal(dim(result)[4], 12) # 5 + 3 + 4 = 12
  
  # Check basis shape 
  expect_equal(dim(result@basis), c(12, 2)) # 12 timepoints, 2 components
})

test_that("concat falls back to NeuroVecSeq when masks are incompatible", {
  # Create two objects with different masks but otherwise compatible
  dims_3d <- c(6, 6, 4)
  k <- 3
  n_vox_total <- prod(dims_3d)
  
  # Mask 1 (more sparse)
  mask_arr1 <- array(TRUE, dim = dims_3d)
  mask_arr1[1:3, 1:3, 1:2] <- FALSE
  mask_vol1 <- LogicalNeuroVol(mask_arr1, NeuroSpace(dims_3d))
  n_vox1 <- sum(mask_vol1)
  
  # Mask 2 (less sparse)
  mask_arr2 <- array(TRUE, dim = dims_3d)
  mask_arr2[1,1,1] <- FALSE
  mask_vol2 <- LogicalNeuroVol(mask_arr2, NeuroSpace(dims_3d))
  n_vox2 <- sum(mask_vol2)
  
  # Create loadings compatible with *their respective* masks
  loadings1 <- Matrix(rnorm(n_vox1 * k), nrow = n_vox1, ncol = k)
  loadings2 <- Matrix(rnorm(n_vox2 * k), nrow = n_vox2, ncol = k) # Different loadings due to mask size
  
  # Create basis matrices
  t1 <- 5; t2 <- 4
  basis1 <- Matrix(rnorm(t1 * k), nrow = t1, ncol = k)
  basis2 <- Matrix(rnorm(t2 * k), nrow = t2, ncol = k)

  # Create space objects
  sp1 <- NeuroSpace(c(dims_3d, t1))
  sp2 <- NeuroSpace(c(dims_3d, t2))
  
  # Create LatentNeuroVec objects
  lvec1 <- LatentNeuroVec(basis1, loadings1, sp1, mask_vol1, offset=numeric(0), "lvec_mask1")
  lvec2 <- LatentNeuroVec(basis2, loadings2, sp2, mask_vol2, offset=numeric(0), "lvec_mask2")
  
  # Concatenate - should fall back because masks (and thus loadings rows) differ
  result <- concat(lvec1, lvec2)
  
  # Should return a NeuroVecSeq
  expect_s4_class(result, "NeuroVecSeq")
  
  # Check that both original objects are stored within the sequence
  expect_equal(length(result@vecs), 2)
  expect_identical(result@vecs[[1]], lvec1)
  expect_identical(result@vecs[[2]], lvec2)
})
</file>

<file path="tests/testthat/test_latent_vec_extra.R">
library(testthat)
library(hdf5r)
library(neuroim2)
library(Matrix)
library(fmristore)

# Helper to create a simple deterministic LatentNeuroVec
make_simple_lvec <- function() {
  dims <- c(3, 3, 2, 4)  # small volume
  sp <- NeuroSpace(dims)
  mask <- array(FALSE, dim = dims[1:3])
  mask[2:3, , ] <- TRUE
  mask_vol <- LogicalNeuroVol(mask, drop_dim(sp))

  k <- 2
  nt <- dims[4]
  nv <- sum(mask)

  basis <- Matrix(matrix(seq_len(nt * k), nrow = nt, ncol = k))
  loadings <- Matrix(matrix(seq_len(nv * k) / 10, nrow = nv, ncol = k))
  offset <- seq_len(nv)

  LatentNeuroVec(basis, loadings, sp, mask_vol, offset)
}


test_that("as.array reconstructs the full 4D data correctly", {
  lvec <- make_simple_lvec()
  arr <- as.array(lvec)
  expect_equal(dim(arr), dim(space(lvec)))

  basis_mat <- as.matrix(lvec@basis)
  loadings_mat <- as.matrix(lvec@loadings)
  offset_vec <- lvec@offset
  mask_idx <- which(as.logical(as.array(lvec@mask)))
  dims <- dim(lvec)

  expected <- array(0, dim = dims)
  for (t in seq_len(dims[4])) {
    vals <- as.vector(tcrossprod(basis_mat[t, , drop = FALSE], loadings_mat)) + offset_vec
    vol <- array(0, dim = dims[1:3])
    vol[mask_idx] <- vals
    expected[,,, t] <- vol
  }

  expect_equal(arr, expected, tolerance = 1e-12)
})


test_that("linear_access returns correct values and errors out-of-range", {
  lvec <- make_simple_lvec()
  full_arr <- as.array(lvec)
  nels <- prod(dim(full_arr))
  set.seed(123)
  idx <- sample(nels, 10)

  expect_equal(linear_access(lvec, idx), full_arr[idx], tolerance = 1e-12)
  expect_error(linear_access(lvec, 0), "out of bounds")
  expect_error(linear_access(lvec, nels + 1), "out of bounds")
})
</file>

<file path="tests/testthat/test-readwriteh5.R">
context("HDF5 read/write functions")

test_that("round trip write/read preserves data integrity", {
  # Create test data with explicit dimensions
  dims <- c(10,10,10)
  nvols <- 10
  space <- NeuroSpace(c(dims, nvols), spacing=c(2,2,2), origin=c(1,1,1))
  
  # Create volumes with known pattern
  vols <- array(0, dim=c(dims, nvols))
  for(i in 1:nvols) {
    vols[,,,i] <- array(i + seq_len(prod(dims))/1000, dim=dims)
  }
  
  # Create NeuroVec and labels
  vec <- NeuroVec(vols, space)
  labels <- paste0("vol_", 1:nvols)
  
  # Create mask where values > 0.5 in first volume
  mask_array <- array(as.logical(vols[,,,1] > 0.5), dim=dims)  # Ensure 3D logical array
  mask_space <- NeuroSpace(dims, spacing=spacing(space)[1:3], origin=origin(space))
  mask <- LogicalNeuroVol(mask_array, mask_space)
  expect_true(inherits(mask, "LogicalNeuroVol"))
  expect_equal(dim(mask), dims)  # Verify mask dimensions
  
  # Write to temporary file
  tmp <- tempfile(fileext=".h5")
  on.exit(unlink(tmp))
  
  write_labeled_vec(vec, mask=mask, labels=labels, file=tmp)
  
  # Read back
  result <- read_labeled_vec(tmp)
  
  # Test space properties
 
  expect_equal(spacing(space(vec)), spacing(space(result[[1]])))
  expect_equal(trans(vec), trans(result[[1]]))
  
  # Test labels
  expect_equal(labels, names(result))
  
  # Test mask preservation
  expect_true(!is.null(attr(result, "mask")))
  expect_equal(dim(attr(result, "mask")), dims)  # Verify read mask dimensions
  expect_equal(as.array(mask@.Data), as.array(attr(result, "mask")@.Data))
  
  # Test volume data
  for(i in 1:nvols) {
    expect_equal(as.array(vec[[i]]@.Data), as.array(result[[i]]@.Data))
  }
})
</file>

<file path="R/h5neurovol.R">
#' @include all_class.R
#' @importFrom assertthat assert_that
#' @importFrom neuroim2 NeuroSpace space origin trans
#' @importFrom lifecycle deprecate_warn
#' @importFrom hdf5r h5attr H5File h5types
#' @importFrom crayon bold blue silver yellow green italic red
NULL


#' H5NeuroVol Constructor
#'
#' @description
#' Constructs an \code{\link{H5NeuroVol}} object representing a 3D brain volume
#' stored in an HDF5 file. The HDF5 file is opened in read-only mode.
#' 
#' @details
#' This constructor is typically used for reading existing HDF5 files that conform
#' to the H5NeuroVol specification.
#'
#' @section Lifecycle Management:
#' When an \code{H5NeuroVol} object is created by providing a \code{file_name},
#' it opens the specified HDF5 file and maintains an open handle to it.
#' **It is the user's responsibility to explicitly close this handle** when the
#' object is no longer needed to release system resources. This can be done by calling
#' \code{close(your_h5neurovol_object)}.
#'
#' Failure to close the handle may lead to issues such as reaching file handle
#' limits or problems with subsequent access to the file.
#'
#' @param file_name A \code{character} string giving the path to an existing 3D HDF5 neuroimaging file.
#' @return A new \code{\link{H5NeuroVol-class}} instance with an open HDF5 file handle.
#'
#' @seealso \code{\link{close.H5NeuroVol}} for closing the file handle, \code{\link[neuroim2]{NeuroVol-class}}
#'
#' @examples
#' \dontrun{
#' # Assuming "my_volume.h5" is a valid H5NeuroVol HDF5 file
#' h5vol <- H5NeuroVol("my_volume.h5")
#' # ... perform operations with h5vol ...
#' print(dim(h5vol))
#' # Important: Close the handle when done
#' close(h5vol)
#' }
#' @export
H5NeuroVol <- function(file_name) {
  assert_that(is.character(file_name))
  assert_that(file.exists(file_name))

  h5obj <- hdf5r::H5File$new(file_name)

  # Check the "rtype" attribute
  rtype <- try(hdf5r::h5attr(h5obj, which="rtype"), silent=TRUE)
  if (!is.character(rtype) || rtype != "DenseNeuroVol") {
    stop("Invalid HDF5 file for H5NeuroVol: ", file_name)
  }

  # Check dimension count
  if (length(h5obj[["space/dim"]][]) != 3) {
    stop(
      "Cannot create H5NeuroVol: file must have 3 dimensions; found: ",
      paste(h5obj[["space/dim"]][], collapse=" ")
    )
  }

  # Build NeuroSpace
  sp <- NeuroSpace(
    dim    = h5obj[["space/dim"]][],
    origin = h5obj[["space/origin"]][],
    trans  = h5obj[["space/trans"]][,]
  )

  new("H5NeuroVol", space=sp, h5obj=h5obj)
}

#' Convert a NeuroVol to HDF5 Format (as_h5 Method)
#'
#' @description
#' Saves a NeuroVol to an HDF5 file with minimal necessary metadata
#' to reconstruct an H5NeuroVol.
#'
#' @param object A NeuroVol object (3D)
#' @param file Path to the output file (if NULL, uses tempfile)
#' @param data_type Character: "FLOAT", "DOUBLE", "INT", etc.
#' @param chunk_dim Numeric vector specifying chunk sizes
#' @param compression Integer [1..9], default 6
#'
#' @return A new \code{H5NeuroVol} referencing the written file.
#'   The returned object contains an open read-mode HDF5 handle.
#'   **Important:** The user is responsible for closing this handle using
#'   \code{close()} on the returned object when finished.
#'
#' @export
setMethod(
  f = "as_h5",
  signature = signature(object = "NeuroVol"),
  definition = function(object, file = NULL, data_type = "FLOAT",
                         chunk_dim = NULL, compression = 6) { 
    
    # --- Determine output file path --- 
    out_file <- file
    if (is.null(out_file)) {
        out_file <- tempfile(fileext = ".h5")
        message("Output file not specified, using temp file: ", out_file)
    }
    
    # --- Map data_type string to HDF5 type object --- 
    h5dtype_obj <- switch(toupper(data_type),
                           "FLOAT"   = hdf5r::h5types$H5T_NATIVE_FLOAT,
                           "DOUBLE"  = hdf5r::h5types$H5T_NATIVE_DOUBLE,
                           "INT"     = hdf5r::h5types$H5T_NATIVE_INT32,
                           "INTEGER" = hdf5r::h5types$H5T_NATIVE_INT32,
                           "SHORT"   = hdf5r::h5types$H5T_NATIVE_INT16,
                           "CHAR"    = hdf5r::h5types$H5T_NATIVE_CHAR,
                           "UINT8"   = hdf5r::h5types$H5T_NATIVE_UCHAR,
                           stop("Unsupported data_type: ", data_type)
                          )
    
    # --- Write Phase: Open, Write, Explicitly Close --- 
    fh_write <- NULL 
    h5_write_obj <- NULL
    write_success <- FALSE
    tryCatch({
        fh_write <- open_h5(out_file, mode = "w")
        h5_write_obj <- fh_write$h5
        
        # Write Root Attribute
        hdf5r::h5attr(h5_write_obj, "rtype") <- "DenseNeuroVol"
        
        # Write Space Group 
        sp <- space(object)
        sp_dims <- dim(sp)
        if (length(sp_dims) != 3) stop("Input NeuroVol must be 3-dimensional.")
        
        # --- Debug: Print trans matrix before writing ---
        current_trans <- trans(sp)
        #message("DEBUG: trans matrix BEFORE writing:")
        print(current_trans)
        
        h5_write(h5_write_obj, "/space/dim", as.integer(sp_dims), overwrite = TRUE)
        h5_write(h5_write_obj, "/space/origin", as.double(origin(sp)), overwrite = TRUE)
        h5_write(h5_write_obj, "/space/spacing", as.double(spacing(sp)), overwrite = TRUE)
        h5_write(h5_write_obj, "/space/trans", current_trans, overwrite = TRUE) 
        
        # Write Data Group
        data_arr <- as.array(object) 
        final_chunk_dim <- chunk_dim
        if (is.null(final_chunk_dim) && compression > 0) {
             final_chunk_dim <- pmin(sp_dims, c(32L, 32L, 32L))
        }
        h5_write(h5_write_obj, "/data/elements", data_arr,
                 dtype = h5dtype_obj,
                 chunk_dims = final_chunk_dim, 
                 compression = compression,
                 overwrite = TRUE)
        
        write_success <- TRUE # Mark success if we reached here
        message("Successfully wrote NeuroVol to: ", out_file)
        
    }, error = function(e) {
        # Error occurred during writing
        stop("Failed during HDF5 write phase for ", out_file, ": ", e$message)
    }, finally = {
        # Always attempt to close the write handle
        if (!is.null(fh_write) && fh_write$owns) {
             safe_h5_close(h5_write_obj)
        }
    })
    
    # If writing didn't complete successfully, stop before reopening
    if (!write_success) {
        stop("HDF5 write failed for ", out_file, ", cannot create H5NeuroVol object.")
    }
    
    # --- Reopen in Read Mode and Return H5NeuroVol --- 
    h5_read_obj <- NULL
    tryCatch({
        # Write handle is now closed. Reopen the file in read mode.
        h5_read_obj <- hdf5r::H5File$new(out_file, mode = "r")
        
        # Read space info for the new object
        trans_data <- h5_read_obj[["/space/trans"]]$read()
       
        spacing <- diag(trans_data)[1:3]
        sp_read <- NeuroSpace(
            dim    = h5_read_obj[["/space/dim"]]$read(),
            spacing= h5_read_obj[["/space/spacing"]]$read(),   
            origin = h5_read_obj[["/space/origin"]]$read(), 
            trans  = trans_data
        )
      
        
        # Return the H5NeuroVol with the NEW, OPEN, read-mode handle
        new("H5NeuroVol", space=sp_read, h5obj=h5_read_obj)
        
    }, error = function(e) {
        # If reopening or reading space fails, ensure the read handle is closed if it opened
        if (!is.null(h5_read_obj) && h5_read_obj$is_valid) safe_h5_close(h5_read_obj)
        stop("Failed to reopen/read HDF5 file ", out_file, " to create H5NeuroVol: ", e$message)
    })
  }
)


#' @export
#' @rdname linear_access-methods
setMethod(
  f = "linear_access",
  signature = signature(x="H5NeuroVol", i="numeric"),
  definition = function(x, i) {

    # 1) Check range
    n_vox <- prod(dim(x))  # total number of voxels in 3D
    if (any(i < 1 | i > n_vox)) {
      stop("Some linear indices are out of range 1..", n_vox)
    }
    # If you also want an error if i==0 is found, the above condition will catch it.

    # 2) If i is empty => return empty
    if (length(i) == 0) {
      return(numeric(0))
    }

    # 3) Convert linear -> (x,y,z)
    coords <- arrayInd(i, dim(x))  # Nx3

    # 4) bounding box
    minx <- min(coords[,1]); maxx <- max(coords[,1])
    miny <- min(coords[,2]); maxy <- max(coords[,2])
    minz <- min(coords[,3]); maxz <- max(coords[,3])

    # 5) Read that bounding box from the dataset
    dset <- NULL # Initialize for finally
    subvol <- NULL
    tryCatch({
        dset <- x@h5obj[["data/elements"]]
        if (is.null(dset)) stop("Could not open dataset '/data/elements' for linear_access")
        subvol <- dset[minx:maxx, miny:maxy, minz:maxz, drop=FALSE]
    }, finally = {
        if (!is.null(dset) && inherits(dset, "H5D") && dset$is_valid) {
            close_h5_safely(dset)
        }
    })
    if (is.null(subvol)) stop("Failed to read sub-volume data for linear_access.")
    # shape => (maxx - minx + 1) x (maxy - miny + 1) x (maxz - minz + 1)

    # 6) Offset coords to index subvol
    off_coords <- cbind(coords[,1] - minx + 1,
                        coords[,2] - miny + 1,
                        coords[,3] - minz + 1)

    # 7) Gather values
    n <- nrow(coords)
    out_vals <- numeric(n)
    for (k in seq_len(n)) {
      out_vals[k] <- subvol[ off_coords[k,1],
                             off_coords[k,2],
                             off_coords[k,3] ]
    }
    out_vals
  }
)

#' @export
setMethod(
  f = "linear_access",
  signature = signature(x="H5NeuroVol", i="integer"),
  definition = function(x, i) {
    callGeneric(x, as.numeric(i))  # passes off to the numeric method above
  }
)



#' 3D bracket subsetting for H5NeuroVol (handles partial arguments)
#'
#' @description
#' Allows \code{h5vol[i, j, k]} where each of \code{i,j,k} may be missing.
#' Missing arguments default to the entire range in that dimension.
#' Zero-length arguments immediately yield an empty array of the correct shape.
#'
#' @param x An \code{H5NeuroVol} instance
#' @param i,j,k Numeric (or integer) index vectors for each dimension. If missing,
#'   we take the full range in that dimension.
#' @param drop Logical: whether to drop dimensions of size 1. Default \code{TRUE}.
#' @param ... Unused
#'
#' @return A numeric \code{array} of shape \code{c(length(i), length(j), length(k))},
#'   or fewer dims if \code{drop=TRUE}.
#'
#' @export
setMethod(
  f = "[",
  signature = signature(x="H5NeuroVol"),
  definition = function(x, i, j, k, ..., drop=TRUE) {

    # 1) Determine dimension of the underlying volume
    dimx <- dim(x)  # c(X, Y, Z)
    if (length(dimx) != 3) {
      stop("H5NeuroVol is not 3D? Found dim=", paste(dimx, collapse="x"))
    }

    # 2) If i, j, k are missing, default them
    if (missing(i)) {
      i <- seq_len(dimx[1])
    }
    if (missing(j)) {
      j <- seq_len(dimx[2])
    }
    if (missing(k)) {
      k <- seq_len(dimx[3])
    }

    # Convert to numeric in case user gave integer
    i <- as.numeric(i)
    j <- as.numeric(j)
    k <- as.numeric(k)

    # 3) If any index has length=0 => return an empty array right away
    if (length(i)==0 || length(j)==0 || length(k)==0) {
      outdim <- c(length(i), length(j), length(k))
      empty_arr <- array(numeric(0), dim=outdim)
      if (drop) empty_arr <- drop(empty_arr)
      return(empty_arr)
    }

    # 4) Check out-of-range
    if (min(i)<1 || max(i)>dimx[1]) {
      stop("Subscript 'i' out of range for dimension 1")
    }
    if (min(j)<1 || max(j)>dimx[2]) {
      stop("Subscript 'j' out of range for dimension 2")
    }
    if (min(k)<1 || max(k)>dimx[3]) {
      stop("Subscript 'k' out of range for dimension 3")
    }

    # 5) Determine bounding box
    minI <- floor(min(i)); maxI <- ceiling(max(i))
    minJ <- floor(min(j)); maxJ <- ceiling(max(j))
    minK <- floor(min(k)); maxK <- ceiling(max(k))

    # 6) Read the bounding box from the dataset
    dset <- NULL # Initialize for finally
    subvol <- NULL
    tryCatch({
        dset <- x@h5obj[["data/elements"]]
        if (is.null(dset)) stop("Could not open dataset '/data/elements'")
        subvol <- dset[minI:maxI, minJ:maxJ, minK:maxK, drop=FALSE]
    }, finally = {
        if (!is.null(dset) && inherits(dset, "H5D") && dset$is_valid) {
            close_h5_safely(dset)
        }
    })
    if (is.null(subvol)) stop("Failed to read sub-volume data.")
    # shape => c((maxI-minI+1), (maxJ-minJ+1), (maxK-minK+1))

    # 7) We then re-map i,j,k into local sub-box coords
    i_off <- i - minI + 1
    j_off <- j - minJ + 1
    k_off <- k - minK + 1

    subdimI <- maxI - minI + 1
    subdimJ <- maxJ - minJ + 1

    # We'll build the output array
    out_dim <- c(length(i), length(j), length(k))
    out_vals <- numeric(prod(out_dim))

    # Flatten subvol
    subvol_vec <- as.vector(subvol)

    # Build a systematic index
    N  <- length(i) * length(j) * length(k)
    ix_i <- rep(seq_along(i), times = length(j)*length(k))
    ix_j <- rep(rep(seq_along(j), each=length(i)), times=length(k))
    ix_k <- rep(seq_along(k), each = length(i)*length(j))

    loc_i <- i_off[ix_i]
    loc_j <- j_off[ix_j]
    loc_k <- k_off[ix_k]

    # local 3D => linear index in subvol
    sub_lin_idx <- loc_i +
      (loc_j-1)* subdimI +
      (loc_k-1)* subdimI * subdimJ

    out_vals <- subvol_vec[sub_lin_idx]
    arr_out  <- array(out_vals, dim=out_dim)

    # 8) drop dims if requested
    if (drop) {
      arr_out <- drop(arr_out)
    }
    arr_out
  }
)

#' Convert DenseNeuroVol to H5NeuroVol
#'
#' @description
#' Converts a \code{DenseNeuroVol} to \code{H5NeuroVol} by writing it to disk in HDF5 format.
#'
#' @param from A \code{DenseNeuroVol} object.
#' @return An \code{H5NeuroVol} referencing the resulting HDF5 file.
#'
#' @keywords internal
#' @name DenseNeuroVol,H5NeuroVol
setAs(
  from = "DenseNeuroVol",
  to   = "H5NeuroVol",
  def  = function(from) {
    to_nih5_vol(from, file_name=NULL, data_type="FLOAT")
  }
)

#' Show Method for H5NeuroVol
#' 
#' Displays a summary of the H5NeuroVol object, including dimensions,
#' spacing, origin, and HDF5 file information, without reading voxel data.
#' 
#' @param object The H5NeuroVol object to display.
#' @importFrom crayon bold blue silver yellow green italic
#' @export
setMethod(
  f = "show",
  signature = "H5NeuroVol",
  definition = function(object) {
    cat("\n", crayon::bold(crayon::blue("H5NeuroVol Object")), "\n")
    cat(crayon::silver("═══════════════════\n"))
    
    # Display spatial info from the space slot
    sp <- object@space
    dims <- dim(sp)
    spacing_str <- paste(round(neuroim2::spacing(sp), 2), collapse = " × ")
    origin_str <- paste(round(neuroim2::origin(sp), 2), collapse = " × ")
    
    cat(crayon::yellow("Dimensions:"), crayon::green(paste(dims, collapse = " × ")), "\n")
    cat(crayon::yellow("Spacing:"), crayon::green(spacing_str), "\n")
    cat(crayon::yellow("Origin:"), crayon::green(origin_str), "\n")
    
    # Display HDF5 file info
    h5f <- object@h5obj
    file_status <- "<Invalid Handle>"
    file_path <- "<unknown>"
    
    if (!is.null(h5f) && inherits(h5f, "H5File")) {
      is_valid <- tryCatch(h5f$is_valid, error = function(e) FALSE)
      if (is_valid) {
        file_path <- tryCatch(h5f$get_filename(), error = function(e) "<error getting path>")
        file_status <- paste(crayon::green("VALID handle"), "for file:", crayon::italic(file_path))
      } else {
        # Try to get filename even if closed
        file_path <- tryCatch(h5f$get_filename(), error = function(e) "<unknown path>")
        file_status <- paste(crayon::red("CLOSED handle"), "for file:", crayon::italic(file_path))
      }
    } else {
       file_status <- crayon::red("INVALID H5File object slot")
    }
    
    cat(crayon::yellow("HDF5 Source:"), file_status, "\n")
    cat(crayon::silver("═══════════════════\n"))
    cat("Access data using standard array indexing (e.g., object[1:10, 1:10, 1])\n")
    invisible(NULL)
  }
)

#' Close the HDF5 file associated with an H5NeuroVol
#'
#' This method manually closes the HDF5 file handle stored within the
#' H5NeuroVol object. It uses the \code{safe_h5_close} helper.
#'
#' @param con An \code{H5NeuroVol} object.
#' @param ... Additional arguments (ignored).
#' @return Invisibly returns \code{NULL}.
#' @rdname close
#' @export
setMethod("close", "H5NeuroVol", function(con, ...) {
  if (!is.null(con@h5obj)) {
    safe_h5_close(con@h5obj)
    # Nulling out the reference is problematic as the slot expects H5File object.
    # The hdf5r object itself will become invalid after closing.
    # con@h5obj <- NULL # This line causes S4 validation error
  }
  invisible(NULL)
})
</file>

<file path="R/io_write_h5.R">
#' @include all_class.R io_h5_helpers.R
#' @import hdf5r
#' @import neuroim2
#' @importFrom methods is
#' @importFrom utils write.table
#' @importFrom hdf5r H5T_STRING H5S h5types
NULL

# Contains functions for writing fmristore HDF5 structures.

# Helper function to validate the runs_data list structure
validate_runs_data <- function(rd) {
  for (i in seq_along(rd)) {
    el <- rd[[i]]
    stopifnot(
      is.list(el),
      is.character(el$scan_name), length(el$scan_name) == 1, nzchar(el$scan_name),
      el$type %in% c("full", "summary"),
      is.list(el$data) || is.matrix(el$data) # Allow list for full, matrix for summary
      # Add check: if type == "full", data must be list; if type == "summary", data must be matrix
      # Add check: if type == "full", names(data) should be cluster_XXX
    )
    if (el$type == "full" && !is.list(el$data)) stop(sprintf("Run %d ('%s'): type is 'full' but data is not a list.", i, el$scan_name))
    if (el$type == "summary" && !is.matrix(el$data)) stop(sprintf("Run %d ('%s'): type is 'summary' but data is not a matrix.", i, el$scan_name))
    # TODO: Could add validation that full data list names match cluster IDs expected
  }
}

#' Write Clustered Experiment Data to HDF5
#'
#' @description
#' Writes neuroimaging data structured according to the H5ClusterExperiment
#' specification into an HDF5 file.
#'
#' This function takes R objects representing the mask, cluster definitions,
#' run-specific data (either full voxel-level or summary time series), and
#' associated metadata, and creates the corresponding HDF5 groups and datasets.
#'
#' @param filepath Character string: the path to the HDF5 file to create.
#'   If the file exists, it will be overwritten.
#' @param mask A `LogicalNeuroVol` object representing the brain mask.
#' @param clusters A `ClusteredNeuroVol` object containing cluster assignments
#'   for voxels within the mask.
#' @param runs_data A list where each element represents a single run/scan.
#'   Each element must be a list containing:
#'   \itemize{
#'     \item `scan_name`: (character) Unique identifier for the scan.
#'     \item `type`: (character) Either "full" or "summary".
#'     \item `data`: 
#'       \itemize{
#'         \item If `type` is "full", `data` must be a list where names are `cluster_<cid>`
#'               (e.g., `cluster_1`, `cluster_2`) and values are matrices 
#'               `[nVoxelsInCluster, nTime]` containing the time series for that cluster.
#'         \item If `type` is "summary", `data` must be a single matrix
#'               `[nTime, nClusters]` containing the summary time series.
#'       }
#'     \item `metadata`: (Optional) A list of key-value pairs for scan-specific metadata.
#'                     Can include `n_time` explicitly, otherwise it's inferred from data.
#'   }
#' @param cluster_metadata (Optional) A `data.frame` containing metadata for the clusters.
#'   Must contain at least a column named `cluster_id` matching the unique IDs in `clusters`.
#'   Other columns will be written as part of a compound dataset.
#' @param overwrite Logical: If `TRUE`, overwrite the file if it exists. Default `FALSE`.
#' @param compress Logical: If `TRUE`, apply GZIP compression to data arrays. Default `TRUE`.
#' @param verbose Logical: Print progress messages? Default `TRUE`.
#'
#' @return Invisibly returns `NULL`. Called for its side effect of creating the HDF5 file.
#' @export
#' @family H5ClusteredIO
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("write_clustered_experiment_h5", where = "package:fmristore") &&
#'     !is.null(fmristore:::create_minimal_LogicalNeuroVol) &&
#'     !is.null(fmristore:::create_minimal_ClusteredNeuroVol)) {
#'
#'   temp_h5_file <- NULL
#'   
#'   tryCatch({
#'     # 1. Create a temporary file path
#'     temp_h5_file <- tempfile(fileext = ".h5")
#'     
#'     # 2. Create minimal mask and clusters using helpers
#'     mask_vol <- fmristore:::create_minimal_LogicalNeuroVol(dims = c(5L, 5L, 2L))
#'     # Ensure clusters are within the mask and have some content
#'     # Create clusters that align with the mask's space
#'     clust_vol <- fmristore:::create_minimal_ClusteredNeuroVol(
#'       space = neuroim2::space(mask_vol), # Use mask's space
#'       mask = mask_vol@.Data,            # Use mask's data
#'       num_clusters = 2L
#'     )
#'     
#'     # 3. Prepare minimal runs_data
#'     # Get cluster IDs and number of voxels per cluster from clust_vol
#'     unique_cids <- sort(unique(clust_vol@clusters[clust_vol@clusters > 0]))
#'     n_time_run1 <- 10L
#'     n_time_run2 <- 8L
#'     
#'     # Run 1: Full data type
#'     run1_data_list <- list()
#'     if (length(unique_cids) > 0) {
#'       for (cid in unique_cids) {
#'         n_vox_in_cluster <- sum(clust_vol@clusters == cid)
#'         if (n_vox_in_cluster > 0) {
#'            # Data: nVoxInCluster x nTime
#'           run1_data_list[[paste0("cluster_", cid)]] <- matrix(
#'             rnorm(n_vox_in_cluster * n_time_run1), 
#'             nrow = n_vox_in_cluster, 
#'             ncol = n_time_run1
#'           )
#'         }
#'       }
#'     }
#'     
#'     run1 <- list(
#'       scan_name = "ScanA_Full",
#'       type = "full",
#'       data = run1_data_list,
#'       metadata = list(subject_id = "sub-01", task = "rest", n_time = n_time_run1)
#'     )
#'     
#'     # Run 2: Summary data type
#'     # Data: nTime x nClusters
#'     run2_summary_matrix <- matrix(
#'       rnorm(n_time_run2 * length(unique_cids)), 
#'       nrow = n_time_run2, 
#'       ncol = length(unique_cids)
#'     )
#'     colnames(run2_summary_matrix) <- paste0("cluster_", unique_cids) # Optional: for clarity
#'     
#'     run2 <- list(
#'       scan_name = "ScanB_Summary",
#'       type = "summary",
#'       data = run2_summary_matrix,
#'       metadata = list(subject_id = "sub-01", task = "task", n_time = n_time_run2)
#'     )
#'     
#'     runs_data_list <- list(run1, run2)
#'     
#'     # 4. Prepare minimal cluster_metadata (optional)
#'     cluster_meta_df <- NULL
#'     if (length(unique_cids) > 0) {
#'       cluster_meta_df <- data.frame(
#'         cluster_id = unique_cids,
#'         name = paste0("Region_", LETTERS[1:length(unique_cids)]),
#'         size_vox = sapply(unique_cids, function(id) sum(clust_vol@clusters == id))
#'       )
#'     }
#'     
#'     # 5. Call the function
#'     write_clustered_experiment_h5(
#'       filepath = temp_h5_file,
#'       mask = mask_vol,
#'       clusters = clust_vol,
#'       runs_data = runs_data_list,
#'       cluster_metadata = cluster_meta_df,
#'       overwrite = TRUE,
#'       verbose = FALSE 
#'     )
#'     
#'     # Verify file was created
#'     if (file.exists(temp_h5_file)) {
#'       cat("Successfully wrote clustered experiment to:", temp_h5_file, "\\n")
#'       # Optional: Basic check of the HDF5 file structure
#'       # h5f <- hdf5r::H5File$new(temp_h5_file, mode="r")
#'       # print(h5f$ls(recursive=TRUE))
#'       # h5f$close_all()
#'     }
#'     
#'   }, error = function(e) {
#'     message("write_clustered_experiment_h5 example failed: ", e$message)
#'     if (!is.null(temp_h5_file)) message("Temporary file was: ", temp_h5_file)
#'   }, finally = {
#'     # Clean up temporary file
#'     if (!is.null(temp_h5_file) && file.exists(temp_h5_file)) {
#'       unlink(temp_h5_file)
#'     }
#'   })
#' } else {
#'   message("Skipping write_clustered_experiment_h5 example: dependencies or helpers not available.")
#' }
write_clustered_experiment_h5 <- function(filepath,
                                          mask,
                                          clusters,
                                          runs_data,
                                          cluster_metadata = NULL,
                                          overwrite = FALSE,
                                          compress = TRUE, # Added compress argument
                                          verbose = TRUE) {

  # --- Input Validation --- 
  if (!is(mask, "LogicalNeuroVol")) stop("`mask` must be a LogicalNeuroVol object.")
  if (!is(clusters, "ClusteredNeuroVol")) stop("`clusters` must be a ClusteredNeuroVol object.")
  # Use helper for dimension check
  check_same_dims(mask, clusters, dims_to_compare = 1:3, 
                  msg = "Dimensions of mask and clusters must match.")
  if (!is.list(runs_data)) stop("`runs_data` must be a list.")
  if (file.exists(filepath) && !overwrite) stop("File exists and overwrite is FALSE: ", filepath)
  if (file.exists(filepath) && overwrite) file.remove(filepath)

  n_vox_mask <- sum(mask)
  if (length(clusters@clusters) != n_vox_mask) {
      stop(sprintf("Length of clusters vector (%d) does not match number of voxels in mask (%d).", length(clusters@clusters), n_vox_mask))
  }
  
  # Validate runs_data structure
  validate_runs_data(runs_data)
  
  # Validate cluster_metadata if provided
  if (!is.null(cluster_metadata)) {
      if (!is.data.frame(cluster_metadata)) stop("`cluster_metadata` must be a data.frame.")
      if (!("cluster_id" %in% names(cluster_metadata))) stop("`cluster_metadata` must contain a 'cluster_id' column.")
      # TODO: Check if cluster_ids in metadata match unique(clusters@clusters)?
  }

  # --- File Creation --- 
  h5f <- NULL
  gzip_level <- if (compress) 4L else 0L # Set gzip level based on argument
  
  tryCatch({
    if (verbose) message("Creating HDF5 file: ", filepath)
    h5f <- H5File$new(filepath, mode = "w")
    
    # --- Write Global Structures --- 
    if (verbose) message("Writing global structures (mask, clusters, header)... ")
    # Mask - use h5_write
    h5_write(h5f, "/mask", as.array(mask), dtype = h5types$H5T_NATIVE_UCHAR, overwrite = TRUE)
    
    # Cluster Map - use h5_write
    h5_write(h5f, "/cluster_map", clusters@clusters, dtype = h5types$H5T_NATIVE_INT32, overwrite = TRUE)
    
    # Voxel Coordinates - use h5_write
    h5_write(h5f, "/voxel_coords", which(as.array(mask), arr.ind = TRUE), 
             dtype = h5types$H5T_NATIVE_INT32, overwrite = TRUE)

    # Header - use h5_write for each field
    # hdr_grp <- h5f$create_group("header") # h5_write creates parent
    sp <- space(mask)
    dims_vol <- dim(sp)
    hdr_dim <- c(4L, dims_vol[1], dims_vol[2], dims_vol[3], length(runs_data), 1L, 1L, 1L) 
    hdr_pixdim <- c(0.0, spacing(sp)[1], spacing(sp)[2], spacing(sp)[3], 0.0, 0.0, 0.0, 0.0)
    q_info <- tryCatch(neuroim2::matrixToQuatern(sp@trans), error=function(e) NULL)
    h5_write(h5f, "/header/dim", hdr_dim, overwrite = TRUE)
    h5_write(h5f, "/header/pixdim", hdr_pixdim, overwrite = TRUE)
    h5_write(h5f, "/header/quatern_b", q_info$qb %||% 0.0, overwrite = TRUE)
    h5_write(h5f, "/header/quatern_c", q_info$qc %||% 0.0, overwrite = TRUE)
    h5_write(h5f, "/header/quatern_d", q_info$qd %||% 0.0, overwrite = TRUE)
    h5_write(h5f, "/header/qoffset_x", q_info$qx %||% 0.0, overwrite = TRUE)
    h5_write(h5f, "/header/qoffset_y", q_info$qy %||% 0.0, overwrite = TRUE)
    h5_write(h5f, "/header/qoffset_z", q_info$qz %||% 0.0, overwrite = TRUE)
    h5_write(h5f, "/header/qfac", q_info$qfac %||% 1.0, overwrite = TRUE)
    # hdr_grp$close() # No longer needed

    # Global Clusters Group
    global_clus_grp <- h5f$create_group("clusters")
    unique_cluster_ids <- sort(unique(clusters@clusters))
    # Write cluster_ids using h5_write
    h5_write(global_clus_grp, "cluster_ids", unique_cluster_ids, 
             dtype = h5types$H5T_NATIVE_INT32, overwrite = TRUE)
    
    # Write cluster_metadata using h5_write
    if (!is.null(cluster_metadata)) {
      if (verbose) message("Writing global cluster metadata...")
      # meta_grp <- global_clus_grp$create_group("cluster_meta") # h5_write creates parent
      tryCatch({
          cluster_metadata_filtered <- cluster_metadata[cluster_metadata$cluster_id %in% unique_cluster_ids, , drop = FALSE]
          if (nrow(cluster_metadata_filtered) != nrow(cluster_metadata)) {
              warning("Provided cluster_metadata contained IDs not present in the clusters object; only metadata for existing IDs was written.")
          }
          for (cn in names(cluster_metadata_filtered)) {
            vec <- cluster_metadata_filtered[[cn]]
            col_path <- file.path("/clusters/cluster_meta", cn)
            h5_write(h5f, col_path, vec, 
                     dtype = guess_h5_type(vec), # guess_h5_type handles string type creation
                     overwrite = TRUE)
          }
      }, error = function(e) {
          warning("Failed during writing of cluster metadata: ", e$message)
      }) # No finally needed for meta_grp
    }
    # global_clus_grp$close() # Close group handle

    # --- Write Scans --- 
    if (verbose) message("Writing scan data...")
    scans_grp <- h5f$create_group("scans")

    # Validate that all runs_data have the same type before proceeding
    if (length(runs_data) > 0) {
      all_run_types <- vapply(runs_data, function(run) run$type, character(1))
      unique_run_types <- unique(all_run_types)
      if (length(unique_run_types) > 1) {
        stop(sprintf("All items in 'runs_data' must have the same 'type' (either 'full' or 'summary'). Found mixed types: %s", 
                     paste(unique_run_types, collapse=", ")))
      }
      # All types are the same, so the first one is representative
      summary_only <- (unique_run_types[1] == "summary")
    } else {
      # No runs, default summary_only to FALSE (or could be NA/absent, but FALSE is safer for attribute)
      summary_only <- FALSE 
    }
    hdf5r::h5attr(scans_grp, "summary_only") <- as.logical(summary_only)
    
    for (i in seq_along(runs_data)) {
        run <- runs_data[[i]]
        sname <- run$scan_name
        stype <- run$type
        sdata <- run$data
        smeta <- run$metadata %||% list()
        
        if (verbose) message(sprintf("  Writing scan: %s (type: %s)", sname, stype))
        # scan_grp <- scans_grp$create_group(sname) # h5_write creates parent
        
        # Write metadata using h5_write
        if (length(smeta) > 0) {
           # meta_grp <- scan_grp$create_group("metadata") # h5_write creates parent
           tryCatch({
             for (mname in names(smeta)) {
                mval <- smeta[[mname]]
                meta_path <- file.path("/scans", sname, "metadata", mname)
                h5_write(h5f, meta_path, mval, 
                         dtype = guess_h5_type(mval), 
                         overwrite = TRUE)
             }
           }, error = function(e) {
               warning(sprintf("Failed during writing of metadata for scan '%s': %s", sname, e$message))
           }) # No finally needed
        }
        
        # Write data based on type
        if (stype == "full") {
           # scan_clus_grp <- scan_grp$create_group("clusters") # h5_write creates parent
           tryCatch({
               # --- Determine nTime ---
               nTime <- NA_integer_
               # 1. Prioritize from metadata if available and valid
               if (!is.null(smeta$n_time) && is.numeric(smeta$n_time) && length(smeta$n_time) == 1 && smeta$n_time > 0) {
                   nTime <- as.integer(smeta$n_time)
                   if (verbose) message(sprintf("    Scan '%s': Using nTime %d from metadata.", sname, nTime))
               }

               # 2. If not in metadata, infer from data matrices in sdata
               if (is.na(nTime)) {
                   if (verbose) message(sprintf("    Scan '%s': nTime not in metadata, attempting to infer from sdata list.", sname))
                   if (length(sdata) > 0) {
                       for (data_item_name in names(sdata)) {
                           data_item <- sdata[[data_item_name]]
                           if (is.matrix(data_item) && ncol(data_item) > 0) {
                               nTime <- ncol(data_item)
                               if (verbose) message(sprintf("    Scan '%s': Inferred nTime %d from sdata item '%s'.", sname, nTime, data_item_name))
                               break # Found nTime, exit loop
                           }
                       }
                   }
               }
               
               if (is.na(nTime)) {
                   stop(sprintf("Could not determine a valid nTime for full data write in scan '%s'. Provide in metadata or ensure sdata contains valid matrices.", sname))
               }
               
               for (nm in names(sdata)) { # e.g. "cluster_17"
                 mat <- sdata[[nm]]
                 if (!is.matrix(mat) || !is.numeric(mat))
                   stop("run ", sname, ": ", nm, " is not numeric matrix")
                 dims <- dim(mat)
                 if (any(dims < 0) || length(dims) != 2) stop("Invalid dimensions for matrix in ", nm)
                 
                 chunk_dims_full <- if(prod(dims)>0) pmin(dims, c(1024L, 128L)) else NULL
                 data_path <- file.path("/scans", sname, "clusters", nm)
                 h5_write(h5f, data_path, mat, 
                          dtype = h5types$H5T_IEEE_F32LE, 
                          chunk_dims = chunk_dims_full,
                          compression = if(prod(dims)>0) gzip_level else 0L,
                          overwrite = TRUE)
               }
           }, error = function(e) {
               stop(sprintf("Failed writing full data for scan '%s': %s", sname, e$message))
           }) # No finally needed
           
        } else if (stype == "summary") {
           # scan_summary_grp <- scan_grp$create_group("clusters_summary") # h5_write creates parent
           tryCatch({
               mat <- sdata # validated above
               if (!is.matrix(mat) || !is.numeric(mat))
                 stop("run ", sname, ": summary data is not numeric matrix")
               dims <- dim(mat)
               if (any(dims < 0) || length(dims) != 2) stop("Invalid dimensions for summary matrix")
               
               # Check matrix cols match unique cluster IDs
               if (dims[2] != length(unique_cluster_ids)) {
                   stop(sprintf("Summary matrix for scan '%s' has %d columns, but %d unique cluster IDs exist.", sname, dims[2], length(unique_cluster_ids)))
               }
               
               chunk_dims_summary <- if(prod(dims)>0) pmin(dims, c(128L, 256L)) else NULL
               summary_path <- file.path("/scans", sname, "clusters_summary", "summary_data")
               h5_write(h5f, summary_path, mat, 
                        dtype = h5types$H5T_IEEE_F32LE,
                        chunk_dims = chunk_dims_summary,
                        compression = if(prod(dims)>0) gzip_level else 0L,
                        overwrite = TRUE)
           }, error = function(e) {
               stop(sprintf("Failed writing summary data for scan '%s': %s", sname, e$message))
           }) # No finally needed
           
        } else {
           warning(sprintf("Unknown run type '%s' for scan '%s'. Skipping data write.", stype, sname))
        }
        
        # No need to close scan_grp explicitly
    }
    # No need to close scans_grp explicitly

  }, error = function(e) {
     if (!is.null(h5f) && h5f$is_valid) try(h5f$close_all(), silent=TRUE)
     if (file.exists(filepath)) file.remove(filepath) # Clean up partial file on error
     stop("Failed during HDF5 file writing: ", e$message)
  }, finally = {
     if (!is.null(h5f) && h5f$is_valid) try(h5f$close_all(), silent=TRUE)
  })
  
  invisible(NULL)
}
</file>

<file path="tests/testthat/test-cluster_run_summary.R">
# test-cluster_run_summary.R

library(testthat)
library(hdf5r)
library(neuroim2)
# Assuming fmristore classes/methods are loaded via NAMESPACE or devtools::load_all()

# Helper function to create a dummy HDF5 file with SUMMARY data for testing
create_dummy_clustered_summary_h5 <- function(filepath,
                                              dims = c(4, 4, 2), # x,y,z
                                              n_time = 10,
                                              scan_name = "scan_summary1",
                                              cluster_ids = 1:3,
                                              summary_dset_name = "summary_data") {
  
  n_clusters <- length(cluster_ids)
  
  # Create basic mask (needed for context, but data isn't voxel-wise)
  mask_vol <- LogicalNeuroVol(array(TRUE, dim = dims), NeuroSpace(dims))

  # Create cluster map (clusters arg can be NULL for make_run_summary, 
  # but let's create one for potential consistency checks if needed later)
  # This cluster map structure might not perfectly align with the summary data,
  # as the summary data is just [nTime, nClusters].
  clus_map_arr <- array(rep(cluster_ids, length.out = prod(dims)), dim = dims)
  clusters_vol <- ClusteredNeuroVol(mask_vol, clusters = clus_map_arr[mask_vol])

  # Create predictable summary data [nTime, nClusters]
  summary_mat <- matrix(0, nrow = n_time, ncol = n_clusters)
  colnames(summary_mat) <- paste0("Cluster", cluster_ids)
  for (cl_idx in 1:n_clusters) {
      cid <- cluster_ids[cl_idx]
      # Data: cluster_id * 100 + timepoint
      summary_mat[, cl_idx] <- cid * 100 + 1:n_time
  }

  # Create HDF5 file
  file <- H5File$new(filepath, mode = "w")
  

  # Create structure /scans/<scan_name>/clusters_summary/
  scans_top_grp <- file$create_group("scans")
  scan_grp <- scans_top_grp$create_group(scan_name)
  summary_grp <- scan_grp$create_group("clusters_summary")

  # Write the summary dataset
  summary_grp[[summary_dset_name]] <- summary_mat
  
  # Optionally add cluster_names/ids as attributes if format supports it
  # summary_grp$create_attr("cluster_names", dtype=H5T_STRING, space=H5S_SCALAR) $write(paste0("Cluster", cluster_ids))

  file$close_all()
  
  return(list(filepath = filepath, 
              mask = mask_vol, 
              clusters = clusters_vol, # May be NULL in actual use case
              dims = dims, 
              n_time = n_time, 
              scan_name = scan_name,
              summary_data = summary_mat, 
              cluster_ids = cluster_ids,
              cluster_names = colnames(summary_mat),
              summary_dset_name = summary_dset_name
              ))
}



test_that("H5ClusterRunSummary constructor works with file path", {
  setup_info <- create_dummy_clustered_summary_h5(tempfile(fileext = ".h5"))
  on.exit(unlink(setup_info$filepath), add = TRUE)

  # Use new constructor
  run_summary <- H5ClusterRunSummary(file = setup_info$filepath,
                                       scan_name = setup_info$scan_name,
                                       mask = setup_info$mask,
                                       clusters = setup_info$clusters,
                                       cluster_names = setup_info$cluster_names,
                                       cluster_ids = setup_info$cluster_ids,
                                       summary_dset = setup_info$summary_dset_name
                                       )

  expect_s4_class(run_summary, "H5ClusterRunSummary")
  expect_equal(run_summary@scan_name, setup_info$scan_name)
  expect_equal(run_summary@n_time, setup_info$n_time)
  expect_equal(run_summary@cluster_names, setup_info$cluster_names)
  expect_equal(run_summary@cluster_ids, setup_info$cluster_ids)
  expect_equal(run_summary@summary_dset, setup_info$summary_dset_name)
  expect_true(run_summary@obj$is_valid)
  
  # Close handle managed by object
  h5file(run_summary)$close_all()
})

test_that("H5ClusterRunSummary constructor works with open H5File handle (via make_run_summary)", {
  setup_info <- create_dummy_clustered_summary_h5(tempfile(fileext = ".h5"))
  on.exit(unlink(setup_info$filepath), add = TRUE)

  h5f <- H5File$new(setup_info$filepath, mode = "r")
  on.exit(if(h5f$is_valid) h5f$close_all(), add = TRUE)

  # Keep using make_run_summary for this test, expecting a deprecation warning
  expect_warning(
    run_summary_open <- make_run_summary(file_source = h5f,
                                       scan_name = setup_info$scan_name,
                                       mask = setup_info$mask,
                                       clusters = setup_info$clusters,
                                       cluster_names = setup_info$cluster_names,
                                       cluster_ids = setup_info$cluster_ids,
                                       summary_dset = setup_info$summary_dset_name
                                       ),
    "deprecated"
  )

  expect_s4_class(run_summary_open, "H5ClusterRunSummary")
  expect_true(run_summary_open@obj$is_valid)
  expect_equal(h5file(run_summary_open)$get_filename(), h5f$get_filename())
})

test_that("H5ClusterRunSummary constructor errors work", {
  setup_info <- create_dummy_clustered_summary_h5(tempfile(fileext = ".h5"))
  on.exit(unlink(setup_info$filepath), add = TRUE)
  
  # Need an open handle to test internal errors
  h5f_err <- H5File$new(setup_info$filepath, mode = "r")
  on.exit(if(h5f_err$is_valid) h5f_err$close_all(), add = TRUE)

  # Use new constructor for error checks
  expect_error(H5ClusterRunSummary(file = "nonexistent.h5", scan_name = "s1", mask = setup_info$mask, 
  clusters = setup_info$clusters), regexp="does not exist")
  # Test invalid scan_name within an existing file
  expect_error(H5ClusterRunSummary(file = setup_info$filepath, 
                                     scan_name = "wrong_scan_name", mask = setup_info$mask, 
                                     clusters = setup_info$clusters), 
                                     "Summary dataset not found")
  # Test invalid summary_dset name
  expect_error(H5ClusterRunSummary(file = setup_info$filepath, scan_name = setup_info$scan_name, mask = setup_info$mask, clusters = setup_info$clusters, summary_dset="wrong_name"), "Summary dataset not found")
})

test_that("H5ClusterRunSummary cluster name/ID reconciliation works", {
  setup_info <- create_dummy_clustered_summary_h5(tempfile(fileext = ".h5"))
  on.exit(unlink(setup_info$filepath), add = TRUE)
  
  h5f <- H5File$new(setup_info$filepath, mode = "r")
  on.exit(if(h5f$is_valid) h5f$close_all(), add = TRUE)
  
  # Case 1: Provided names/IDs match dataset columns
  # Use new constructor (need file path)
  run_summary <- H5ClusterRunSummary(file = setup_info$filepath,
                                       scan_name = setup_info$scan_name,
                                       mask = setup_info$mask,
                                       clusters = setup_info$clusters,
                                       cluster_names = paste0("clus_", 1:3), # Explicitly correct
                                       cluster_ids = 1:3, 
                                       summary_dset = setup_info$summary_dset_name)
  expect_equal(run_summary@cluster_names, paste0("clus_", 1:3))
  expect_equal(run_summary@cluster_ids, 1:3)
  h5file(run_summary)$close_all() # Close object handle

  # Case 2: Mismatched provided names (warning, reset to Col_X)
  # Use new constructor
  expect_warning(
      run_summary_badnames <- H5ClusterRunSummary(file = setup_info$filepath,
                                                scan_name = setup_info$scan_name,
                                                mask = setup_info$mask,
                                                clusters = setup_info$clusters,
                                                cluster_names = c("A", "B"), # Mismatch cols (3)
                                                cluster_ids = 1:3, # Match cols
                                                summary_dset = setup_info$summary_dset_name),
      "Final number of cluster names.*does not match dataset columns"
  )
  expect_warning( # Second warning about resetting
      H5ClusterRunSummary(file = setup_info$filepath,
                         scan_name = setup_info$scan_name,
                         mask = setup_info$mask,
                         clusters = setup_info$clusters,
                         cluster_names = c("A", "B"), # Mismatch cols (3)
                         cluster_ids = 1:3, # Match cols
                         summary_dset = setup_info$summary_dset_name),
      "Resetting names/IDs to Col_X/sequential"
  )
  expect_equal(run_summary_badnames@cluster_names, paste0("Col_", 1:3))
  expect_equal(run_summary_badnames@cluster_ids, 1:3)
  h5file(run_summary_badnames)$close_all() # Close object handle
  
  # Case 3: No names/IDs provided, derive from clusters object
  # Use new constructor
  run_summary_derive <- H5ClusterRunSummary(file = setup_info$filepath,
                                            scan_name = setup_info$scan_name,
                                            mask = setup_info$mask,
                                            clusters = setup_info$clusters,
                                            cluster_names = character(), # Explicitly empty
                                            cluster_ids = integer(),   # Explicitly empty
                                            summary_dset = setup_info$summary_dset_name)
  expect_equal(run_summary_derive@cluster_names, paste0("Clus_", 1:3)) # Derived from unique(clusters@clusters)
  expect_equal(run_summary_derive@cluster_ids, 1:3)
  h5file(run_summary_derive)$close_all()

  # Case 4: No names/IDs/clusters provided, derive from dataset cols (Col_X)
  # Use new constructor
  expect_warning(
      run_summary_defaults <- H5ClusterRunSummary(file = setup_info$filepath,
                                                scan_name = setup_info$scan_name,
                                                mask = setup_info$mask,
                                                clusters = NULL, # No clusters object
                                                cluster_names = character(), 
                                                cluster_ids = integer(),   
                                                summary_dset = setup_info$summary_dset_name),
      "generated default column names \\(Col_X\\)"
  )
  expect_equal(run_summary_defaults@cluster_names, paste0("Col_", 1:3))
  expect_equal(run_summary_defaults@cluster_ids, 1:3)
  h5file(run_summary_defaults)$close_all()
  
})

test_that("as.matrix method for H5ClusterRunSummary works", {
  setup_info <- create_dummy_clustered_summary_h5(tempfile(fileext = ".h5"))
  on.exit(unlink(setup_info$filepath), add = TRUE)
  
  h5f <- H5File$new(setup_info$filepath, mode = "r")
  on.exit(h5f$close_all(), add = TRUE)
  
  run_summary <- make_run_summary(file_source = h5f,
                                  scan_name = setup_info$scan_name,
                                  mask = setup_info$mask,
                                  clusters = setup_info$clusters,
                                  cluster_names = setup_info$cluster_names,
                                  cluster_ids = setup_info$cluster_ids,
                                  summary_dset = setup_info$summary_dset_name
                                  )
                                  
  # Call as.matrix
  mat <- as.matrix(run_summary)
  
  # Check dimensions
  expect_equal(nrow(mat), setup_info$n_time)
  expect_equal(ncol(mat), length(setup_info$cluster_ids))
  
  # Check column names
  expect_equal(colnames(mat), setup_info$cluster_names)
  
  # Check content
  expect_equal(mat, setup_info$summary_data)
  
  # Test case where cluster_names don't match columns (should warn)
   run_summary_badnames <- make_run_summary(file_source = h5f,
                                  scan_name = setup_info$scan_name,
                                  mask = setup_info$mask,
                                  clusters = setup_info$clusters,
                                  cluster_names = c("Wrong", "Names"), # Incorrect number
                                  cluster_ids = setup_info$cluster_ids
                                  )
  expect_warning(mat_badnames <- as.matrix(run_summary_badnames), "Length of cluster_names")
  expect_null(colnames(mat_badnames)) # Names should not be set
  expect_equal(mat_badnames, setup_info$summary_data, check.attributes = FALSE) # Content still matches
  
})

test_that("make_run_summary generates default names/ids and as.data.frame works", {
  setup_info <- create_dummy_clustered_summary_h5(tempfile(fileext = ".h5"))
  on.exit(unlink(setup_info$filepath), add = TRUE)
  
  h5f <- H5File$new(setup_info$filepath, mode = "r")
  on.exit(h5f$close_all(), add = TRUE)
  
  # Call constructor without names or ids
  run_summary_defaults <- NULL
  
    expect_warning(
       run_summary_defaults <- make_run_summary(file_source = h5f,
                                     scan_name = setup_info$scan_name,
                                     mask = setup_info$mask,
                                     clusters = NULL # Pass NULL to test default generation
                                     # Omit cluster_names and cluster_ids
                                     ),
      regexp="Generated default" # Warning for names
    )
  expect_s4_class(run_summary_defaults, "H5ClusterRunSummary")
  n_clusters_expected <- ncol(setup_info$summary_data)
  expected_names <- paste0("Col_", seq_len(n_clusters_expected))
  expected_ids <- seq_len(n_clusters_expected)
  
  # Check if defaults were set in the object
  expect_equal(run_summary_defaults@cluster_names, expected_names)
  expect_equal(run_summary_defaults@cluster_ids, expected_ids)
  
  # Test as.data.frame
  df <- as.data.frame(run_summary_defaults)
  expect_s3_class(df, "data.frame")
  expect_equal(nrow(df), setup_info$n_time)
  expect_equal(ncol(df), n_clusters_expected)
  expect_equal(names(df), expected_names) # Column names should be the generated defaults
  
  # Check content consistency with matrix form
  expect_equal(as.matrix(df), setup_info$summary_data, check.attributes = FALSE)
})
</file>

<file path="R/all_generic.R">
#' Get the number of scans
#'
#' This generic returns the number of scans in an object (e.g. a sequence of fMRI scans).
#'
#' @param x The object from which to retrieve the number of scans
#' @return An integer representing the number of scans
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5ClusterExperiment", where = "package:fmristore") &&
#'     exists("n_scans", where = "package:fmristore") &&
#'     !is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     # Create a minimal H5ClusterExperiment which contains runs (scans)
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Get the number of scans
#'     num_scans <- n_scans(exp_obj)
#'     print(num_scans) # Should be 2 based on the helper
#'     
#'   }, error = function(e) {
#'     message("n_scans example failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping n_scans example: dependencies or helper not available.")
#' }
#'
#' @export
setGeneric("n_scans", function(x) standardGeneric("n_scans"))

#' Get the scan names
#'
#' This generic returns a character vector of scan names or labels.
#'
#' @param x The object from which to retrieve the scan names
#' @return A character vector of scan names
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5ClusterExperiment", where = "package:fmristore") &&
#'     exists("scan_names", where = "package:fmristore") &&
#'     !is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     # Create a minimal H5ClusterExperiment
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Get the scan names
#'     s_names <- scan_names(exp_obj)
#'     print(s_names) # Should be c("Run1_Full", "Run2_Summary") or similar
#'     
#'   }, error = function(e) {
#'     message("scan_names example failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping scan_names example: dependencies or helper not available.")
#' }
#'
#' @export
setGeneric("scan_names", function(x) standardGeneric("scan_names"))

#' Get scan metadata
#'
#' This generic returns any available metadata associated with each scan.
#'
#' @param x The object from which to retrieve the scan metadata
#' @return A list (or other structure) containing metadata for each scan
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5ClusterExperiment", where = "package:fmristore") &&
#'     exists("scan_metadata", where = "package:fmristore") &&
#'     !is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     # Create a minimal H5ClusterExperiment
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Get the scan metadata
#'     s_meta <- scan_metadata(exp_obj)
#'     print(s_meta)
#'     # The helper currently doesn't add rich scan_metadata, 
#'     # so this might be an empty list or list of NULLs by default.
#'     # length(s_meta) == n_scans(exp_obj) # This should hold TRUE
#'     
#'   }, error = function(e) {
#'     message("scan_metadata example failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping scan_metadata example: dependencies or helper not available.")
#' }
#'
#' @export
setGeneric("scan_metadata", function(x) standardGeneric("scan_metadata"))

#' Get cluster metadata
#'
#' This generic returns any available metadata associated with clusters in an object.
#'
#' @param x The object from which to retrieve the cluster metadata
#' @return A data frame or other structure containing metadata for each cluster
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5ClusterExperiment", where = "package:fmristore") &&
#'     exists("cluster_metadata", where = "package:fmristore") &&
#'     !is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     # Create a minimal H5ClusterExperiment
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Get the cluster metadata
#'     c_meta <- cluster_metadata(exp_obj)
#'     print(c_meta)
#'     # The helper currently doesn't add rich cluster_metadata, 
#'     # so this is likely an empty data.frame or one with default cluster names/IDs.
#'     
#'   }, error = function(e) {
#'     message("cluster_metadata example failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping cluster_metadata example: dependencies or helper not available.")
#' }
#'
#' @export
setGeneric("cluster_metadata", function(x) standardGeneric("cluster_metadata"))

#' Get the HDF5 file object
#'
#' This generic returns the HDF5 file object associated with the object.
#'
#' @param x The object from which to retrieve the HDF5 file object
#' @return The HDF5 file object (from package hdf5r)
#'
#' @examples
#' if (!is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Get the H5File object
#'     h5f <- h5file(exp_obj)
#'     print(h5f)
#'     # if (requireNamespace("hdf5r", quietly = TRUE)) print(h5f$is_valid)
#'     
#'   }, error = function(e) {
#'     message("h5file example failed: ", e$message)
#'   }, finally = {
#'     # Closing exp_obj will close the h5file handle it owns
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping h5file example: helper not available.")
#' }
#'
#' @export
setGeneric("h5file", function(x) standardGeneric("h5file"))

#' @keywords internal
setGeneric("write_dataset", function(x, name, data) standardGeneric("write_dataset"))

#' @keywords internal
setGeneric("read_dataset", function(x, name) standardGeneric("read_dataset"))

#' @keywords internal
setGeneric("has_dataset", function(x, name) standardGeneric("has_dataset"))


# --- Generics for LatentNeuroVec Accessors --- 

#' Get the basis matrix (temporal components)
#' @param x An object, likely a LatentNeuroVec or similar
#' @param ... Additional arguments
#' @return The basis matrix (typically time x components)
#' 
#' @examples
#' # For LatentNeuroVec:
#' if (!is.null(fmristore:::create_minimal_LatentNeuroVec)) {
#'   lnv <- NULL
#'   tryCatch({
#'     lnv <- fmristore:::create_minimal_LatentNeuroVec(
#'       space_dims = c(4L, 4L, 2L), 
#'       n_time = 10L, 
#'       n_comp = 3L
#'     )
#'     b_matrix <- basis(lnv)
#'     print(dim(b_matrix)) # Should be n_time x n_comp (e.g., 10x3)
#'   }, error = function(e) {
#'     message("basis example for LatentNeuroVec failed: ", e$message)
#'   })
#' } else {
#'   message("Skipping basis example for LatentNeuroVec: helper not available.")
#' }
#' 
#' @export
#' @rdname basis-methods
setGeneric("basis", function(x, ...) standardGeneric("basis"))

#' Get the loadings matrix (spatial components)
#' @param x An object, likely a LatentNeuroVec or similar
#' @param ... Additional arguments
#' @return The loadings matrix (typically voxels x components)
#' 
#' @examples
#' # For LatentNeuroVec:
#' if (!is.null(fmristore:::create_minimal_LatentNeuroVec)) {
#'   lnv <- NULL
#'   tryCatch({
#'     # Helper creates a mask, n_mask_voxels determined internally or by arg
#'     lnv <- fmristore:::create_minimal_LatentNeuroVec(
#'       space_dims = c(4L, 4L, 2L), 
#'       n_time = 10L, 
#'       n_comp = 3L
#'     )
#'     l_matrix <- loadings(lnv)
#'     # Dimensions should be n_voxels_in_mask x n_comp
#'     print(dim(l_matrix)) 
#'   }, error = function(e) {
#'     message("loadings example for LatentNeuroVec failed: ", e$message)
#'   })
#' } else {
#'   message("Skipping loadings example for LatentNeuroVec: helper not available.")
#' }
#' 
#' @export
#' @rdname loadings-methods
setGeneric("loadings", function(x, ...) standardGeneric("loadings"))

#' Get the offset vector
#' @param x An object, likely a LatentNeuroVec or similar
#' @param ... Additional arguments
#' @return The offset vector
#' 
#' @examples
#' # For LatentNeuroVec:
#' if (!is.null(fmristore:::create_minimal_LatentNeuroVec)) {
#'   lnv <- NULL
#'   tryCatch({
#'     lnv <- fmristore:::create_minimal_LatentNeuroVec(
#'       space_dims = c(3L, 3L, 2L), 
#'       n_mask_voxels = 4L # Specify a small number of voxels in mask
#'     )
#'     off_vector <- offset(lnv)
#'     print(head(off_vector))
#'     # The neuroim2::LatentNeuroVec constructor (used by helper) defaults to zero offset.
#'     # Length should be n_voxels_in_mask.
#'     if (inherits(lnv, "LatentNeuroVec")) {
#'        # Assuming lnv@mask is a LogicalNeuroVol created by the helper
#'        # The offset vector length should match the number of TRUE voxels in the mask.
#'        # print(length(off_vector) == sum(lnv@mask@.Data))
#'     }
#'   }, error = function(e) {
#'     message("offset example for LatentNeuroVec failed: ", e$message)
#'   })
#' } else {
#'   message("Skipping offset example for LatentNeuroVec: helper not available.")
#' }
#' 
#' @export
#' @rdname offset-methods
setGeneric("offset", function(x, ...) standardGeneric("offset"))

#' Get the mask volume
#' @param x An object with a mask, like LatentNeuroVec or H5ClusterExperiment
#' @param ... Additional arguments
#' @return The mask object (e.g., a LogicalNeuroVol)
#' 
#' @examples
#' # For LatentNeuroVec:
#' if (!is.null(fmristore:::create_minimal_LatentNeuroVec)) {
#'   lnv <- NULL
#'   tryCatch({
#'     lnv <- fmristore:::create_minimal_LatentNeuroVec(space_dims = c(3L,3L,2L))
#'     mask_vol <- mask(lnv)
#'     print(mask_vol)
#'     # if (requireNamespace("neuroim2", quietly=TRUE)) print(is(mask_vol, "LogicalNeuroVol"))
#'   }, error = function(e) {
#'     message("mask example for LatentNeuroVec failed: ", e$message)
#'   })
#' } else {
#'   message("Skipping mask example for LatentNeuroVec: helper not available.")
#' }
#' 
#' # For H5ClusterExperiment:
#' if (!is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     mask_vol_exp <- mask(exp_obj)
#'     print(mask_vol_exp)
#'   }, error = function(e) {
#'     message("mask example for H5ClusterExperiment failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping mask example for H5ClusterExperiment: helper not available.")
#' }
#' 
#' @export
#' @rdname mask-methods
setGeneric("mask", function(x, ...) standardGeneric("mask"))

#' Get the index map volume
#' @param x An object with an index map, like LatentNeuroVec
#' @param ... Additional arguments
#' @return The index map object (e.g., an IndexLookupVol from neuroim2)
#' 
#' @examples
#' # For LatentNeuroVec:
#' if (!is.null(fmristore:::create_minimal_LatentNeuroVec)) {
#'   lnv <- NULL
#'   tryCatch({
#'     lnv <- fmristore:::create_minimal_LatentNeuroVec(space_dims = c(3L,3L,2L))
#'     map_vol <- map(lnv)
#'     print(map_vol)
#'     # if (requireNamespace("neuroim2", quietly=TRUE)) print(is(map_vol, "IndexLookupVol"))
#'   }, error = function(e) {
#'     message("map example for LatentNeuroVec failed: ", e$message)
#'   })
#' } else {
#'   message("Skipping map example for LatentNeuroVec: helper not available.")
#' }
#' 
#' @export
#' @rdname map-methods
setGeneric("map", function(x, ...) standardGeneric("map"))

#' Get the cluster map object
#' @param x An object with cluster assignments (e.g., H5ClusterExperiment)
#' @param ... Additional arguments
#' @return The clusters object (e.g., a ClusteredNeuroVol)
#' 
#' @examples
#' # For H5ClusterExperiment:
#' if (!is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment(
#'       master_mask_dims = c(4L,4L,3L), 
#'       num_master_clusters = 2L
#'     )
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Get the master cluster map from the experiment
#'     cluster_vol <- clusters(exp_obj)
#'     print(cluster_vol)
#'     # if (requireNamespace("neuroim2", quietly=TRUE)) print(is(cluster_vol, "ClusteredNeuroVol"))
#'     
#'     # Individual runs also have cluster information, potentially accessible via their own methods
#'     # run1 <- runs(exp_obj)[["Run1_Full"]]
#'     # run1_clusters <- clusters(run1) # Assuming a method for H5ClusterRun
#'     # print(run1_clusters)
#'     
#'   }, error = function(e) {
#'     message("clusters example for H5ClusterExperiment failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping clusters example for H5ClusterExperiment: helper not available.")
#' }
#' 
#' @export
#' @rdname clusters-methods
setGeneric("clusters", function(x, ...) standardGeneric("clusters"))


# --- Generics for H5ClusterExperiment Helpers --- 

#' Concatenate Voxel Time Series Across Runs (Generic)
#' 
#' @param experiment The experiment object (typically \code{\link{H5ClusterExperiment-class}}).
#' @param mask_idx Indices of voxels within the mask of the experiment.
#' @param run_indices Optional: A numeric or character vector specifying which runs to include.
#' @param ... Additional arguments for methods.
#' @return A concatenated matrix (typically time x voxels).
#' 
#' @examples
#' if (!is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment(
#'       master_mask_dims = c(4L,4L,2L), # Smaller mask for example
#'       n_time_run1 = 5L # Shorter time series for Run1_Full
#'     )
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Get some valid voxel indices from the mask
#'     # The master mask is created by the helper.
#'     # exp_mask <- mask(exp_obj) 
#'     # valid_mask_indices <- which(exp_mask@.Data) 
#'     # For simplicity, let's assume first few indices if mask is not empty.
#'     # If mask has at least 2 voxels: 
#'     # selected_vox_indices <- valid_mask_indices[1:min(2, length(valid_mask_indices))]
#'     # A more robust way for an example without directly loading mask here:
#'     # The H5ClusterExperiment has a mask, and its linear indices are 1:sum(mask_data)
#'     # For a 4x4x2 mask, if 50% are true, there are 16 true voxels. Indices are 1 to 16.
#'     # Let's pick first 2 voxels in the mask's internal indexing (1-based).
#'     
#'     # Note: series_concat typically works on runs with full data (H5ClusterRun)
#'     # The helper creates "Run1_Full".
#'     # concatenated_series <- series_concat(exp_obj, mask_idx = c(1, 2), run_indices = "Run1_Full")
#'     # print(dim(concatenated_series)) # Should be n_time_run1 x 2
#'     # print(head(concatenated_series))
#'     
#'     # For a fully runnable example, need to ensure mask_idx is valid for the created object.
#'     # Since the helper creates a mask, we can try to use mask_idx = 1 (first voxel in mask).
#'     # The `series_concat` method for H5ClusterExperiment handles this.
#'     if (n_voxels(exp_obj) > 0) { # n_voxels from H5ClusteredArray slot
#'        conc_series <- series_concat(exp_obj, mask_idx = 1, run_indices = "Run1_Full")
#'        print(paste("Dimensions of concatenated series for voxel 1 from Run1_Full:", 
#'                    paste(dim(conc_series), collapse="x"))))
#'     } else {
#'        message("Skipping series_concat demonstration as experiment mask is empty.")
#'     }
#'     
#'   }, error = function(e) {
#'     message("series_concat example failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping series_concat example: helper not available.")
#' }
#' 
#' @export
setGeneric("series_concat", 
           function(experiment, mask_idx, run_indices = NULL, ...) standardGeneric("series_concat"))

#' Concatenate Cluster Summary Matrices Across Runs (Generic)
#' 
#' @param experiment The experiment object (typically \code{\link{H5ClusterExperiment-class}}).
#' @param run_indices Optional: A numeric or character vector specifying which runs to include.
#'   If NULL, uses all runs that are of summary type or can produce a summary matrix.
#' @param ... Additional arguments for methods.
#' @return A concatenated matrix (typically time x clusters).
#' 
#' @examples
#' if (!is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   tryCatch({
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment(
#'       n_time_run2 = 6L # Shorter time series for Run2_Summary
#'     )
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Note: matrix_concat typically works on runs with summary data (H5ClusterRunSummary)
#'     # The helper creates "Run2_Summary".
#'     # concatenated_matrix <- matrix_concat(exp_obj, run_indices = "Run2_Summary")
#'     # print(dim(concatenated_matrix)) 
#'     # Should be n_time_run2 x n_master_clusters (e.g., 6x3 if num_master_clusters is 3)
#'     # print(head(concatenated_matrix))
#'
#'     # The method should correctly identify and use the summary run.
#'     # If only one summary run is present, run_indices can often be omitted.
#'     conc_matrix <- matrix_concat(exp_obj, run_indices = "Run2_Summary")
#'     print(paste("Dimensions of concatenated matrix from Run2_Summary:", 
#'                 paste(dim(conc_matrix), collapse="x")))
#'
#'     # Example with all compatible runs (should pick up Run2_Summary)
#'     # conc_matrix_all <- matrix_concat(exp_obj)
#'     # print(paste("Dimensions of concatenated matrix from all compatible runs:", 
#'     #             paste(dim(conc_matrix_all), collapse="x")))
#'
#'   }, error = function(e) {
#'     message("matrix_concat example failed: ", e$message)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping matrix_concat example: helper not available.")
#' }
#' 
#' @export
setGeneric("matrix_concat", 
           function(experiment, run_indices = NULL, ...) standardGeneric("matrix_concat"))


#' Get HDF5 Dataset Path (Internal Generic)
#'
#' @description
#' An internal generic function used to determine the HDF5 path to the dataset
#' corresponding to a specific cluster ID within a given run object.
#' Concrete subclasses (like `H5ClusterRun`) must implement a method for this generic.
#'
#' @param x An object inheriting from `H5ClusteredArray`.
#' @param cid The cluster ID (integer).
#' @param ... Additional arguments (not typically used).
#'
#' @return A character string representing the HDF5 dataset path.
#' @keywords internal
setGeneric(".dataset_path",
           function(x, cid, ...) standardGeneric(".dataset_path"))


#' Generic function to convert R objects to HDF5 format
#'
#' @description
#' A generic function for converting various types of R objects to HDF5 format,
#' providing a standardized interface for serialization to HDF5.
#'
#' @param object The R object to convert to HDF5 (e.g., a \code{NeuroVol} or \code{NeuroVec}).
#' @param file The path to the HDF5 file to create or modify.
#' @param ... Additional arguments specific to the particular method (e.g., \code{dataset_name}).
#'
#' @section Methods:
#' \describe{
#'   \item{\code{signature(object = "NeuroVec")}}{
#'     Creates an HDF5 file from a 4D NeuroVec object.
#'     Additional parameters:
#'     \describe{
#'       \item{\code{data_type}}{Storage type (e.g., "FLOAT"). Default "FLOAT"}
#'       \item{\code{chunk_dim}}{Chunk dimensions. Default depends on input dimensions}
#'       \item{\code{compression}}{Integer [0..9], default 6}
#'     }
#'     Returns an H5NeuroVec referencing the new HDF5 file.
#'   }
#'   \item{\code{signature(object = "LatentNeuroVec")}}{
#'     Saves a LatentNeuroVec to an HDF5 file in BasisEmbeddingSpec format.
#'     Additional parameters:
#'     \describe{
#'       \item{\code{data_type}}{Storage type (e.g., "FLOAT"). Default "FLOAT"}
#'       \item{\code{compression}}{Integer [1..9], default 6}
#'     }
#'     Returns an HDF5 file object.
#'   }
#'   \item{\code{signature(object = "LabeledVolume")}}{
#'     Saves a LabeledVolume to an HDF5 file.
#'     Additional parameters:
#'     \describe{
#'       \item{\code{mask}}{The mask to use (LogicalNeuroVol)}
#'       \item{\code{labels}}{Character vector of labels}
#'       \item{\code{compression}}{Integer [0..9], default 4}
#'       \item{\code{dtype}}{HDF5 data type for values. Default H5T_NATIVE_DOUBLE}
#'       \item{\code{chunk_size}}{Integer chunk size for HDF5, default 1024}
#'       \item{\code{header_values}}{List of additional header values}
#'     }
#'     Returns an HDF5 file object.
#'   }
#'   \item{\code{signature(object = "list")}}{
#'     Writes a cluster-based time-series dataset to an HDF5 file.
#'     Additional parameters:
#'     \describe{
#'       \item{\code{scan_names}}{Character vector of scan names}
#'       \item{\code{mask}}{LogicalNeuroVol for 3D geometry}
#'       \item{\code{clusters}}{ClusteredNeuroVol with cluster IDs}
#'       \item{\code{scan_metadata}}{List of metadata lists, one per scan}
#'       \item{\code{cluster_metadata}}{Optional data.frame with cluster descriptions}
#'       \item{\code{summary_only}}{Logical; if TRUE, store only summary data}
#'       \item{\code{compression}}{Integer [0..9], default 4}
#'       \item{\code{chunk_size}}{Chunk dimension for 2D writes, default 1024}
#'     }
#'     Returns an HDF5-backed object representing the clustered dataset (e.g., H5ClusterExperiment).
#'   }
#' }
#'
#' @return An object representing the HDF5 storage, typically of a class
#'   corresponding to the input type (e.g., \code{H5NeuroVol} for \code{NeuroVol} input,
#'   \code{H5NeuroVec} for \code{NeuroVec} input).
#'
#' @examples
#' # Example 1: NeuroVec (DenseNeuroVec) to HDF5
#' # Ensure helper function is available and as_h5 exists
#' # if (!is.null(fmristore:::create_minimal_DenseNeuroVec) &&
#' #     exists("as_h5", where = "package:fmristore")) {
#'   
#' dvec <- fmristore:::create_minimal_DenseNeuroVec(dims = c(3L,3L,2L,4L))
#' temp_h5_file <- tempfile(fileext = ".h5")
#' h5_obj <- NULL
#'   
#' tryCatch({
#'     # Convert DenseNeuroVec to an HDF5 file and get an H5NeuroVec object back
#'     h5_obj <- as_h5(dvec, file = temp_h5_file, 
#'                     data_type = "FLOAT",
#'                     chunk_dim = c(2, 2, 2, 4),
#'                     compression = 4)
#'     
#'     print(h5_obj) # Should be an H5NeuroVec
#'     
#' }, error = function(e) {
#'     message("as_h5 NeuroVec example failed: ", e$message)
#' }, finally = {
#'     if (!is.null(h5_obj)) try(close(h5_obj), silent = TRUE)
#'     if (file.exists(temp_h5_file)) {
#'       unlink(temp_h5_file)
#'     }
#' })
#' # }
#' 
#' # Example 2: LatentNeuroVec to HDF5
#' # if (!is.null(fmristore:::create_minimal_LatentNeuroVec) &&
#' #     exists("as_h5", where = "package:fmristore")) {
#'   
#' lnv <- fmristore:::create_minimal_LatentNeuroVec(
#'     space_dims = c(4L, 4L, 2L),
#'     n_time = 6L,
#'     n_comp = 2L
#' )
#' temp_h5_file_lnv <- tempfile(fileext = ".h5") # Use a different temp file name
#' h5_obj_lnv <- NULL # Use a different object name
#'   
#' tryCatch({
#'     # Convert LatentNeuroVec to HDF5
#'     h5_obj_lnv <- as_h5(lnv, file = temp_h5_file_lnv, compression = 4)
#'     
#'     # File should exist and h5_obj_lnv should be a valid H5File object
#'     if (file.exists(temp_h5_file_lnv)) {
#'       print("LatentNeuroVec HDF5 file created successfully: ", temp_h5_file_lnv)
#'     }
#'     
#' }, error = function(e) {
#'     message("as_h5 LatentNeuroVec example failed: ", e$message)
#' }, finally = {
#'     if (!is.null(h5_obj_lnv) && inherits(h5_obj_lnv, "H5File") && h5_obj_lnv$is_valid) {
#'       try(h5_obj_lnv$close_all(), silent = TRUE)
#'     }
#'     if (file.exists(temp_h5_file_lnv)) {
#'       unlink(temp_h5_file_lnv)
#'     }
#' })
#' # }
#'
#' # Example 3: Clustered dataset (list) to HDF5
#' # (This requires more complex setup, so we use a simplified theoretical example)
#' \donttest{
#'   # Removed: if (requireNamespace("neuroim2", quietly = TRUE) && ... 
#'   
#'   # In practice, you would:
#'   # 1. Create a list of NeuroVec/DenseNeuroVec objects (scan data)
#'   # 2. Create a LogicalNeuroVol mask
#'   # 3. Create a ClusteredNeuroVol defining clusters
#'   # 4. Define scan names and metadata
#'   # 5. Call as_h5() with these components
#'   
#'   # Note: This example is simplified for documentation and won't execute
#'   # as the actual clustered dataset structure is more complex
#'   
#'   message("Example usage for clustered dataset (as_h5 for list method):")
#'   message("  # vecs_list <- ... list of NeuroVec objects ...")
#'   message("  # mask_obj <- fmristore:::create_minimal_LogicalNeuroVol(...)")
#'   message("  # clusters_obj <- fmristore:::create_minimal_ClusteredNeuroVol(...)")
#'   message("  # scan_meta <- list(list(TR=2), list(TR=2))")
#'   message("  # h5_clust_obj <- as_h5(vecs_list, file = tempfile(fileext=\".h5\"), ")
#'   message("  #                       scan_names = c('run1', 'run2'), ")
#'   message("  #                       mask = mask_obj, clusters = clusters_obj, ")
#'   message("  #                       scan_metadata = scan_meta)")
#'   message("  # print(h5_clust_obj)")
#'   message("  # if (!is.null(h5_clust_obj)) try(close(h5_clust_obj), silent=TRUE)")
#'   message("  # if (file.exists(attr(h5_clust_obj, \"filepath\"))) unlink(attr(h5_clust_obj, \"filepath\"))")
#' 
#' }
#'
#' @export
#' @rdname as_h5-methods
setGeneric("as_h5", function(object, file, ...) {
  standardGeneric("as_h5")
})

#' Extract Time Series Data
#'
#' @description
#' A generic function to extract time series data from an object.
#' Specific methods will determine how data is extracted based on the object's class
#' and the indices provided.
#'
#' @param x The object from which to extract series data.
#' @param i An index, typically specifying voxels or elements.
#'          The interpretation of `i` depends on the specific method.
#'          It can be numeric indices, a matrix of coordinates, etc.
#' @param j Optional y-coordinate or further index specification used by some methods.
#' @param k Optional z-coordinate or further index specification used by some methods.
#' @param ... Additional arguments passed to specific methods (e.g., `drop`).
#'
#' @return A matrix or vector containing the time series data. The exact format
#'   (e.g., time x voxels, or a simple vector for a single voxel) depends on the
#'   method and arguments.
#'
#' @seealso Methods for this generic are available for classes like
#'   \code{\link{H5ClusterRun}}.
#'
#' @examples
#' # This is a generic function; examples are provided with its specific methods.
#' # For example, see help("series,H5ClusterRun-method")
#'
#' @export
#' @rdname series-methods 
#' @name series # Keep @name if you want the generic itself to be findable by ?series
series <- neuroim2::series

#' Linear Access to Neuroimaging Data (Methods for neuroim2 Generic)
#'
#' @description
#' These methods provide 4D linear access to data for specific `fmristore` classes,
#' implementing the \code{\link[neuroim2:linear_access]{linear_access}} generic function
#' from the \code{neuroim2} package.
#'
#' The `linear_access` generic allows direct access to data elements using a single
#' numeric index that spans the entire 4D space of the object (X, Y, Z, Time).
#' Refer to the documentation for \code{linear_access} in the \code{neuroim2} package
#' for general details about the generic concept.
#'
#' @param x An object for which a `linear_access` method is defined (e.g., `H5ClusterRun`).
#' @param i A numeric vector of 4D linear indices.
#' @param ... Additional arguments, not typically used by `fmristore` methods for `linear_access`
#'   but may be relevant for other methods of this generic.
#'
#' @return A numeric vector of values corresponding to the provided linear indices.
#'   The order of values in the returned vector matches the order of indices in `i`.
#'
#' @seealso \code{neuroim2::\link[neuroim2]{linear_access}}, specific methods like
#'   \code{\link{linear_access,H5ClusterRun-method}}.
#'
#' @name linear_access-methods
#' @aliases linear_access
#' @importFrom neuroim2 linear_access
#' @keywords internal
linear_access <- neuroim2::linear_access



#' Get Dimensions of an Object (Methods for base R Generic)
#'
#' @description
#' These methods retrieve the dimensions of `fmristore` specific objects,
#' implementing the S4 generic function \code{\link[base]{dim}} from the `base` package.
#'
#' For objects that represent neuroimaging data, this typically returns a numeric
#' vector indicating the size of each dimension (e.g., X, Y, Z, Time).
#'
#' @param x An object for which a `dim` method is defined.
#'
#' @return A numeric vector of dimensions. The length and interpretation of the
#'   vector depend on the specific class of `x`.
#'
#' @seealso \code{base::\link[base]{dim}}, specific methods like
#'   \code{dim,H5ClusterRun-method} (if defined and exported).
#'
#' @rdname dim-methods
#' @name dim
#' @aliases dim
#' @keywords internal
NULL


#' Convert to Matrix 
#'
#' @description
#' These methods convert `fmristore` specific objects to matrices,
#' implementing the S4 generic function \code{\link[base]{as.matrix}} from the `base` package.
#'
#' @param x An object for which a `as.matrix` method is defined.
#' @rdname as.matrix-methods
#' @name as.matrix
as.matrix <- neuroim2::as.matrix



#' Convert to Data Frame
#' 
#' @description
#' These methods convert `fmristore` specific objects to data frames,
#' implementing the S4 generic function \code{\link[base]{as.data.frame}} from the `base` package.
#' 
#' @param x An object for which a `as.data.frame` method is defined.
#' @rdname as.data.frame-methods
#' @name as.data.frame
as.data.frame <- base::as.data.frame
</file>

<file path="R/labeled_vec.R">
#' Write a NeuroVec to an HDF5 file with NIfTI-like quaternions
#'
#' @description
#' Creates an HDF5 file following a NIfTI-like header layout, storing:
#' \itemize{
#'   \item \code{/header/dim} => \code{[4, X, Y, Z, nVols, 1,1,1]}
#'   \item \code{/header/pixdim} => \code{[0.0, dx, dy, dz, ...]} (Note: qfac stored in /header/qfac)
#'   \item \code{/header/quatern_b,c,d} and \code{qoffset_x,y,z}
#'   \item \code{/header/qfac} => Quaternion factor (±1)
#'   \item \code{/mask} => 3D dataset \code{[X, Y, Z]} (0/1) at root level
#'   \item \code{/labels} => array of label strings at root level
#'   \item \code{/data/<label>} => 1D array (length = number of nonzero mask voxels)
#'         storing the sub-volume values
#' }
#'
#' @details
#' The 4×4 matrix in \code{trans(space(vec))} is passed to
#' \code{\link[neuroim2]{matrixToQuatern}}, which returns a list containing:
#' \itemize{
#'   \item \code{quaternion = c(b, c, d)} (the three imaginary parts)
#'   \item \code{qfac} (±1 sign)
#' }
#' This function stores \code{qfac} in \code{/header/qfac} and sets \code{/header/pixdim[0]=0}.
#' We also gather voxel spacing (dx,dy,dz) from \code{spacing(space(vec))} and
#' the origin from \code{origin(space(vec))}.
#'
#' We store a subset of NIfTI-like header fields in the \code{/header} group.
#' The user can supply \code{header_values} (a named list) to override or
#' augment *some* additional fields (e.g., \code{qform_code=1L}). See implementation
#' notes for which fields are protected.
#'
#' @section Lifecycle Management:
#' The HDF5 file is opened in write mode and the resulting handle is returned
#' without being automatically closed. **It is the caller's responsibility to
#' close this handle** (via \code{h5file$close_all()} or \code{close()}) when
#' finished working with the file.
#'
#' @param vec A 4D \code{\link[neuroim2]{NeuroVec}} with dimension \code{[X,Y,Z,nVols]}.
#' @param mask A \code{\link[neuroim2]{LogicalNeuroVol}} of shape \code{[X,Y,Z]}
#'   (the same 3D shape as \code{vec}).
#' @param labels A character vector of length \code{nVols}, labeling each 4D sub-volume.
#' @param file Either a character path to the HDF5 file to create or
#'   an open \code{\link[hdf5r]{H5File}} in write mode.
#' @param compression Integer \code{0-9} for gzip level; default \code{4}.
#' @param dtype An HDF5 data type object (e.g., \code{hdf5r::h5types$H5T_NATIVE_FLOAT}).
#'   Default is \code{hdf5r::h5types$H5T_NATIVE_DOUBLE}.
#' @param chunk_size If non-NULL, the chunk dimension for the 1D datasets. Default is \code{1024}.
#' @param header_values A named list of optional overrides for fields in the header
#'   (e.g., \code{list(qform_code=1L, sform_code=2L)}). Note that fields derived from
#'   `vec` or `mask` (like `dim`, `pixdim`, quaternion fields) cannot be overridden here.
#' @param verbose Logical, whether to print verbose messages during processing.
#'
#' @return Invisibly returns the open \code{\link[hdf5r]{H5File}} handle
#'   containing the written data. The caller should close this handle when
#'   finished.
#'
#' @seealso
#' \code{\link[neuroim2]{matrixToQuatern}} for how the quaternion is derived,
#' \code{\link[neuroim2]{quaternToMatrix}} for reconstructing the 4×4,
#' \code{\link{read_labeled_vec}} for reading the file back in.
#'
#' @import hdf5r
#' @importFrom neuroim2 spacing space origin trans matrixToQuatern
#' @importFrom hdf5r H5T_STRING H5S
#' @importFrom lifecycle deprecate_warn
#' @export
write_labeled_vec <- function(vec,
                              mask,
                              labels,
                              file,
                              compression = 4,
                              dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE,
                              chunk_size = 1024,
                              header_values = list(),
                              verbose = FALSE)
{
  # === Pre-flight checks before opening file ===
  
  # 1. Validate mask object and get mask array
  stopifnot(inherits(mask, "LogicalNeuroVol"))
  mask_arr <- as.array(mask)
  stopifnot(length(dim(mask_arr)) == 3) # Ensure mask is 3D
  
  # 2. Check if mask is empty - fail fast
  # Now works correctly with logical mask_arr
  idx_nonzero <- which(mask_arr == TRUE)
  n_nonzero   <- length(idx_nonzero)
  if (n_nonzero == 0) {
    stop("Mask is empty (all FALSE). Cannot write a LabeledVolumeSet with no valid voxels.")
  }
  
  # 3. Validate vec dimensions against mask
  nd <- dim(vec)  # [X, Y, Z, nVols]
  stopifnot(length(nd) == 4)
  if (!all(dim(mask_arr) == nd[1:3])) {
     stop("Mask dimensions [", paste(dim(mask_arr), collapse=","), 
          "] do not match first 3 dimensions of vec [", paste(nd[1:3], collapse=","), "]")
  }
  nVols <- nd[4]

  # 4. Check labels length
  if (length(labels) != nVols) {
    stop("Length of 'labels' (", length(labels), ") must match the 4th dimension of 'vec' (", nVols, ").")
  }
  
  # 5. Sanitize labels and check for duplicates
  # Perform basic sanitization first
  basic_safe_labels <- vapply(labels, function(lbl) gsub("[^A-Za-z0-9_.-]", "_", lbl), character(1))
  
  # Check for duplicates *after* basic sanitization
  if (length(unique(basic_safe_labels)) != length(basic_safe_labels)) {
    stop("Duplicate labels detected after basic sanitization (gsub). Check input labels.")
  }
  
  # Use the basic sanitized labels (now guaranteed unique after gsub)
  safe_labels <- basic_safe_labels
  
  # === Open file using helper ===
  fh <- open_h5(file, mode = "w") # Use write mode
  h5obj <- fh$h5
  
  # Get dimensions (already validated above)
  X <- nd[1]; Y <- nd[2]; Z <- nd[3]

  # Extract 4×4 transformation matrix from NeuroSpace
  tmat <- trans(space(vec))          # e.g. a 4×4
  # Convert to quaternion + qfac - Harden error message
  q <- tryCatch(
      matrixToQuatern(tmat),
      error = function(e) {
          stop("Invalid NeuroSpace in 'vec' – cannot convert transformation matrix to quaternion: ", e$message)
      }
  )

  # Gather spacing & origin
  sp  <- spacing(space(vec))         # c(dx, dy, dz)
  org <- origin(space(vec))          # c(ox, oy, oz)
  if (length(sp) < 3) {
    sp  <- c(sp, rep(1, 3 - length(sp)))
  }
  if (length(org) < 3) {
    org <- c(org, rep(0, 3 - length(org)))
  }

  # Validate dtype argument - must be a single H5T object
  if (!inherits(dtype, "H5T")) {
    stop("'dtype' argument must be a single H5T object (e.g., hdf5r::h5types$H5T_NATIVE_FLOAT).")
  }
  # single_dtype is now just dtype, used for NIFTI header mapping
  single_dtype <- dtype

  # --- Use map_dtype function to get NIFTI codes ---
  # Call the centralized function using :::
  databit <- map_dtype(single_dtype)
  nifti_datatype_code <- databit[1]
  nifti_bitpix <- databit[2]
  if (nifti_datatype_code == 0L) {
    warning("Could not map HDF5 dtype to NIfTI codes. Header datatype/bitpix may be incorrect.")
  }
  # --- End mapping section ---
  
  # Build minimal NIfTI-like header fields
  # Add common unused fields, initialized
  hdr_default <- list(
    sizeof_hdr  = 348L,
    data_type   = "", # Unused
    db_name     = "", # Unused
    extents     = 0L, # Unused
    session_error = 0L, # Unused
    regular     = 0L, # Unused
    dim_info    = 0L, # Unused
    dim         = c(4L, X, Y, Z, nVols, 1L, 1L, 1L),
    intent_p1   = 0.0, intent_p2 = 0.0, intent_p3 = 0.0, # Unused
    intent_code = 0L, # Unused
    datatype    = nifti_datatype_code,
    bitpix      = nifti_bitpix,
    slice_start = 0L, # Unused
    pixdim      = c(0.0, sp[1], sp[2], sp[3], 0, 0, 0, 0), # Keep pixdim[1]=0, qfac stored elsewhere
    vox_offset  = 0.0, # Unused
    scl_slope   = 1.0, # Unused
    scl_inter   = 0.0, # Unused
    slice_end   = 0L, # Unused
    slice_code  = 0L, # Unused
    xyzt_units  = 0L, # Unused
    cal_max     = 0.0, cal_min = 0.0, # Unused
    slice_duration = 0.0, # Unused
    toffset     = 0.0, # Unused
    glmax       = 0L, glmin = 0L, # Unused
    descrip     = "fmristore labeled volume set", # Basic description
    aux_file    = "", # Unused
    qform_code  = 1L, # Default to NIFTI_XFORM_SCANNER_ANAT
    sform_code  = 0L, # Default to NIFTI_XFORM_UNKNOWN
    quatern_b   = q$quaternion[1],
    quatern_c   = q$quaternion[2],
    quatern_d   = q$quaternion[3],
    qoffset_x   = org[1],
    qoffset_y   = org[2],
    qoffset_z   = org[3],
    srow_x      = c(tmat[1,1], tmat[1,2], tmat[1,3], tmat[1,4]), # Store sform rows
    srow_y      = c(tmat[2,1], tmat[2,2], tmat[2,3], tmat[2,4]),
    srow_z      = c(tmat[3,1], tmat[3,2], tmat[3,3], tmat[3,4]),
    intent_name = "", # Unused
    magic       = "n+1" # Keep writing variable length for now
  )

  # Merge user overrides from header_values, preventing overwrite of critical fields
  protected_fields <- c("dim", "pixdim", "quatern_b", "quatern_c", "quatern_d",
                        "qoffset_x", "qoffset_y", "qoffset_z", "sizeof_hdr", "magic",
                        "datatype", "bitpix")
  for (nm in names(header_values)) {
    if (nm %in% protected_fields) {
        warning("Ignoring attempt to override protected header field: '", nm, "'")
    } else {
        hdr_default[[nm]] <- header_values[[nm]]
    }
  }
  
  # 1) Write header fields into /header group
  # Create group first, checking existence
  if (!h5obj$exists("/header")) h5obj$create_group("/header")
  # Write each header field using h5_write
  for (nm in names(hdr_default)) {
      h5_write(h5obj, file.path("/header", nm), hdr_default[[nm]], overwrite = TRUE)
  }
  # Store qfac separately using h5_write
  h5_write(h5obj, "/header/qfac", q$qfac, overwrite = TRUE)

  # 2) Write /mask => shape [X, Y, Z] at root level
  h5_write(h5obj, "/mask", mask_arr, dtype = hdf5r::h5types$H5T_NATIVE_UCHAR, overwrite = TRUE)

  # 3) Write ORIGINAL labels => /labels (variable-length string array) at root level
  h5_write(h5obj, "/labels", labels, dtype = H5T_STRING$new(size = Inf), overwrite = TRUE)

  # 4) /data => subdatasets for each volume, storing masked data
  # Create parent group /data, checking existence
  if (!h5obj$exists("/data")) h5obj$create_group("/data") 
  # idx_nonzero and n_nonzero already calculated above

  # Set valid chunk dimensions for 1D data
  chunk_dims_1d <- if (!is.null(chunk_size) && n_nonzero > 0) c(min(chunk_size, n_nonzero)) else NULL

  for (i in seq_len(nVols)) {
    # Control verbosity
    if (verbose) message("Writing label: ", labels[i], " (safe name: ", safe_labels[i], ")")
    
    # Data path uses SANITIZED label
    data_path <- file.path("/data", safe_labels[i])
    
    # --- Corrected data extraction ---
    # Extract the 3D data for the current volume i
    # Note: NeuroVec subsetting might return a NeuroVol, ensure we get the array data
    vol_i_data <- as.array(vec[[i]]) 
    # Extract only the values at the mask locations (idx_nonzero)
    vol_1d_masked <- vol_i_data[idx_nonzero]
    # --- End corrected data extraction ---
    
    # Write using h5_write
    h5_write(
      h5 = h5obj,
      path = data_path,
      data = vol_1d_masked, # Write the correctly masked 1D data
      dtype = dtype, # Use the single dtype for all datasets
      chunk_dims = chunk_dims_1d,
      compression = compression,
      overwrite = TRUE # Assume overwrite within this function's scope
    )
  }

  invisible(h5obj)
}



#' Read a Labeled Neuroimaging Volume Set from HDF5
#'
#' @description
#' Reads an HDF5 file, typically one previously created by 
#' \code{\link{write_labeled_vec}} (now deprecated), and constructs a 
#' \code{\link{LabeledVolumeSet-class}} object.
#' The HDF5 file is opened in read-only mode.
#'
#' @details
#' This function is the primary way to load data into a \code{LabeledVolumeSet}
#' from its HDF5 disk representation. The structure of the HDF5 file is expected
#' to follow the specification laid out by \code{write_labeled_vec}.
#'
#' @section Lifecycle Management:
#' When a \code{LabeledVolumeSet} object is created by this function,
#' it opens the specified HDF5 file and the returned object takes ownership of
#' this open file handle.
#' **It is the user's responsibility to explicitly close this handle** when the
#' object is no longer needed to release system resources. This can be done by calling
#' \code{close(your_labeled_volume_set_object)}.
#'
#' Failure to close the handle may lead to issues such as reaching file handle
#' limits or problems with subsequent access to the file.
#'
#' @param file_path Character string: path to the HDF5 file.
#' @return A \code{\link{LabeledVolumeSet-class}} object with an open HDF5 file handle.
#'
#' @seealso
#' \code{\link{write_labeled_vec}} for the (deprecated) writing function,
#' \code{\link{LabeledVolumeSet-class}} for details on the object structure,
#' \code{\link{close.LabeledVolumeSet}} for closing the file handle.
#'
#' @examples
#' \dontrun{
#' # Assuming "my_labeled_set.h5" is a valid HDF5 file for LabeledVolumeSet
#' lvs <- read_labeled_vec("my_labeled_set.h5")
#' # ... perform operations with lvs ...
#' print(dim(lvs))
#' print(labels(lvs))
#' # Important: Close the handle when done
#' close(lvs)
#' }
#' @import hdf5r
#' @export
read_labeled_vec <- function(file_path) {
  # --- 1. Handle File Source ---
  # Call simplified open_h5 (no auto_close)
  fh <- open_h5(file_path, mode = "r") 
  h5obj <- fh$h5
  # CRITICAL: Remove on.exit/defer call for h5obj. 
  # Closing is now the responsibility of the user via the close() method 
  # if fh$owns is TRUE.
  # Example removed call: 
  # if (fh$owns) on.exit(safe_h5_close(h5obj), add = TRUE)

  hdr_grp <- NULL # Initialize for finally block
  hdr_values <- list()

  # Helper to read dataset from header group if present
  .rd_hdr <- function(nm) {
    h5_read(h5obj, file.path("/header", nm), missing_ok = TRUE)
  }
  # Helper to read dataset from root group if present
   .rd_root <- function(nm) {
    h5_read(h5obj, paste0("/", nm), missing_ok = TRUE)
  }

  # Read required header fields
  dims       <- .rd_hdr("dim")         # c(4, X, Y, Z, nVols, 1,1,1)
  pixdim     <- .rd_hdr("pixdim")      # c(0.0, dx, dy, dz, ...)
  qb         <- .rd_hdr("quatern_b")
  qc         <- .rd_hdr("quatern_c")
  qd         <- .rd_hdr("quatern_d")
  qx         <- .rd_hdr("qoffset_x")
  qy         <- .rd_hdr("qoffset_y")
  qz         <- .rd_hdr("qoffset_z")
  qfac       <- .rd_hdr("qfac")        # Read qfac from header

  # Read labels from root level
  labels_arr <- .rd_root("labels")
  if (is.null(labels_arr)) {
      stop("Mandatory '/labels' dataset not found at file root.")
  }
  
  # Check and validate dimensions
  if (is.null(dims) || length(dims)<5 || dims[1]!=4) {
    stop("Invalid or missing 'dim' in /header/dim")
  }
  X <- dims[2]; Y <- dims[3]; Z <- dims[4]
  nVols <- dims[5]
  
  # Check labels consistency after reading from root
  if (length(labels_arr) != nVols) {
    stop("Mismatch: #labels (", length(labels_arr), ") != nVols specified in header/dim (", nVols, ").")
  }

  # read /mask => 3D from root level
  mask_arr <- h5_read(h5obj, "/mask", missing_ok = FALSE)
  # Ensure mask_arr is 3D
  if (length(dim(mask_arr)) != 3) {
      stop("Read /mask dataset is not 3-dimensional.")
  }
  if (!all(dim(mask_arr) == c(X,Y,Z))) {
      stop("Dimensions of /mask [", paste(dim(mask_arr), collapse=","),
           "] do not match dimensions specified in header/dim [", X, ",", Y, ",", Z, "]")
  }

  # Rebuild 4x4 transform from quaternion
  # Use default qfac=1 if not found
  qfac_val <- if (!is.null(qfac)) qfac else 1.0

  if (!is.null(pixdim) && length(pixdim) >= 4) {
    # pixdim[0] is ignored (should be 0)
    dx   <- pixdim[2]
    dy   <- pixdim[3]
    dz   <- pixdim[4]
  } else {
    warning("Missing or incomplete 'pixdim' in header. Using default spacing (1,1,1).")
    dx <- 1; dy <- 1; dz <- 1
  }

  if (!all(sapply(list(qb, qc, qd, qx, qy, qz), function(x) !is.null(x) && is.numeric(x)))) {
     warning("Missing or non-numeric quaternion parameters in header. Using identity transform.")
     mat <- diag(4)
     mat[1,1] <- dx
     mat[2,2] <- dy
     mat[3,3] <- dz
  } else {
     mat <- tryCatch(
         neuroim2::quaternToMatrix(
             quat     = c(qb,qc,qd),
             origin   = c(qx,qy,qz),
             stepSize = c(dx,dy,dz),
             qfac     = qfac_val
         ),
         error = function(e) {
             warning("Error calling quaternToMatrix: ", e$message, ". Using identity transform.")
             mat_fallback <- diag(4)
             mat_fallback[1,1] <- dx; mat_fallback[2,2] <- dy; mat_fallback[3,3] <- dz
             mat_fallback
         }
     )
  }

  # build space => just do a 3D NeuroSpace
  spc <- NeuroSpace(dim=c(X,Y,Z), spacing=c(dx,dy,dz), trans=mat)

  # build mask
  mask_vol <- LogicalNeuroVol(as.logical(mask_arr), space=spc)

  # --- 6. Prepare Lazy Loading Environment --- 
  load_env <- new.env(parent=emptyenv())
  # The H5File handle is stored in the main object's obj slot
  load_env$mask_idx <- which(as.logical(mask_arr)==TRUE) # Ensure logical mask used
  load_env$dims     <- c(X,Y,Z)
  load_env$space    <- spc
  # Remove sanitize function from environment
  # load_env$sanitize_label <- function(lbl) { gsub("[^A-Za-z0-9_.-]", "_", lbl) }

  # Define sanitize function locally instead
  sanitize_label_func <- function(lbl) { gsub("[^A-Za-z0-9_.-]", "_", lbl) }

  # Define the core loading logic which needs the parent object
  internal_loader <- function(i, parent_obj) {
    h5f <- parent_obj@obj # Get handle from parent S4 object
    if (!inherits(h5f, "H5File") || !h5f$is_valid) {
      stop("HDF5 file handle associated with this LabeledVolumeSet is invalid or closed.")
    }

    tryCatch({
      # Get the ORIGINAL label name using the numeric index
      lab <- parent_obj@labels[i]
      
      # --- Corrected: Sanitize label to find data path ---
      # Use the local sanitize function
      safe_lab <- sanitize_label_func(lab)
      # Problem: make.unique suffixing might be needed if collisions occurred during write
      # This requires storing the safe_labels from writer, or re-implementing make.unique logic
      # Simplification for now: Assume basic sanitization is enough or paths were unique
      # TODO: Revisit this if make.unique collisions cause read failures
      data_path <- file.path("/data", safe_lab)
      
      # Read the data using h5_read
      val1 <- h5_read(h5f, data_path, missing_ok = FALSE)
      
      # Create the 3D array using dims stored in load_env
      vol <- array(0, dim=parent_obj@load_env$dims)
      
      current_mask_idx <- parent_obj@load_env$mask_idx
      if (length(current_mask_idx) == 0) {
        warning("Internal inconsistency: Mask indices are empty but file exists for label ", lab)
        # Use space from load_env
        return(DenseNeuroVol(vol, space=parent_obj@load_env$space)) 
      }
      
      if (length(current_mask_idx) != length(val1)) {
        # Check if data length exceeds mask length (potential corruption)
        if (length(val1) > length(current_mask_idx)) {
             stop(paste0("Data length mismatch for label ", sQuote(lab), ". Stored data (", 
                        length(val1), ") exceeds mask size (", length(current_mask_idx), "). File may be corrupt."))
        } else {
             warning(paste0("Data length mismatch for label ", sQuote(lab), ". Expected ",
                            length(current_mask_idx), " values based on mask, but found ", length(val1), ". Padding with zeros."))
             # Fill available data, rest remains zero
             vol[current_mask_idx[1:length(val1)]] <- val1
        }
      } else {
        vol[current_mask_idx] <- val1 # Fill using mask indices
      }
      # Use space from load_env
      DenseNeuroVol(vol, space=parent_obj@load_env$space)
      
    }, error=function(e) {
        # Simplify error message to reveal original error
        stop(sprintf("Error loading data for label '%s': %s", 
                     lab %||% "(unknown)", 
                     conditionMessage(e)))
    })
  }

  # --- 7. Create LabeledVolumeSet Object --- 
  lvol <- new("LabeledVolumeSet",
              obj = h5obj, # Assign handle directly to obj slot
              mask = mask_vol,
              labels = labels_arr, # Store original labels
              load_env = load_env # Minimal load_env
              # Removed h5_wrapper assignment
              )

  # Assign the loader function - creates a closure capturing lvol
  load_env$loader <- function(i) {
      internal_loader(i, lvol) # 'lvol' is the parent object here
  }

  # If file was opened internally, the defer handler takes care of closing.
  # If user provided handle, they manage its lifecycle.

  return(lvol)
}

#' 4D Array-like subsetting for LabeledVolumeSet
#'
#' @param x A \code{LabeledVolumeSet} object.
#' @param i Numeric indices for the 1st dimension (x).
#' @param j Numeric indices for the 2nd dimension (y).
#' @param k Numeric indices for the 3rd dimension (z).
#' @param l Numeric indices for the 4th dimension (label).
#' @param drop Logical, whether to drop singleton dimensions.
#' @param ... Ignored.
#'
#' @return An R array after subsetting, or a lower-dimensional array if \code{drop=TRUE}.
#' @export
setMethod(
  f = "[",
  signature = signature(x="LabeledVolumeSet"),
  definition = function(x, i, j, k, l, ..., drop=TRUE) {
    # Validity check is implicitly handled by the loader now

    # 1) Figure out any missing dims => use full range
    dims_3d <- dim(space(x@mask))
    nVols   <- length(x@labels)
    # Handle missing i/j/k/l by using full ranges
    if (missing(i)) i <- seq_len(dims_3d[1])
    if (missing(j)) j <- seq_len(dims_3d[2])
    if (missing(k)) k <- seq_len(dims_3d[3])
    if (missing(l)) l <- seq_len(nVols)

    # Coerce to integer indices
    i <- as.integer(i)
    j <- as.integer(j)
    k <- as.integer(k)
    l <- as.integer(l)

    # Basic bounds check
    if (any(i < 1 | i > dims_3d[1])) {
      stop("Subscript i out of range 1..", dims_3d[1])
    }
    if (any(j < 1 | j > dims_3d[2])) {
      stop("Subscript j out of range 1..", dims_3d[2])
    }
    if (any(k < 1 | k > dims_3d[3])) {
      stop("Subscript k out of range 1..", dims_3d[3])
    }
    if (any(l < 1 | l > nVols)) {
      stop("Subscript l out of range 1..", nVols)
    }

    out_dims <- c(length(i), length(j), length(k), length(l))
    result <- array(0, dim=out_dims)

    # 2) Read each volume in l, subset in memory
    loader_func <- x@load_env$loader
    if (!is.function(loader_func)) stop("Internal error: loader function not found in LabeledVolumeSet environment.")

    out_l_pos <- 1
    for (lv in l) {
      vol_3d <- tryCatch(loader_func(lv), error = function(e) {
        stop("Failed to load volume ", lv, " (label: '", x@labels[lv], "'): ", e$message)
      })
      vol_arr <- as.array(vol_3d) # Convert DenseNeuroVol to array
      subcube <- vol_arr[i, j, k, drop=FALSE]
      result[,,, out_l_pos] <- subcube
      out_l_pos <- out_l_pos + 1
    }

    if (drop) {
      result <- drop(result)
    }
    result
  }
)


#' @rdname linear_access-methods  
#' @importFrom neuroim2 linear_access
#' @export
setMethod(
  f = "linear_access",
  signature = signature(x="LabeledVolumeSet", i="numeric"),
  definition = function(x, i) {
    # Validity check is implicitly handled by the loader

    dims_3d <- dim(space(x@mask))
    nVols   <- length(x@labels)
    bigDim  <- c(dims_3d, nVols)
    total   <- prod(bigDim)

    i <- as.integer(i)
    if (any(i < 1 | i > total)) {
      stop("Some indices out of range 1..", total)
    }

    sub_4d <- arrayInd(i, .dim=bigDim)
    vol_groups <- split(seq_len(nrow(sub_4d)), sub_4d[,4])
    out <- numeric(length(i))

    loader_func <- x@load_env$loader
    if (!is.function(loader_func)) stop("Internal error: loader function not found in LabeledVolumeSet environment.")

    for (v_str in names(vol_groups)) {
      v_idx <- as.integer(v_str)
      these_rows <- vol_groups[[v_str]]
      coords <- sub_4d[these_rows, , drop=FALSE]

      # Call the loader
      vol_3d <- tryCatch(loader_func(v_idx), error = function(e) {
         stop("Failed to load volume ", v_idx, " (label: '", x@labels[v_idx], "'): ", e$message)
      })
      vol_arr <- as.array(vol_3d)

      for (row_i in seq_len(nrow(coords))) {
        rx <- coords[row_i,1]
        ry <- coords[row_i,2]
        rz <- coords[row_i,3]
        val <- vol_arr[rx, ry, rz]
        out_idx <- these_rows[row_i]
        out[out_idx] <- val
      }
    }
    out
  }
)

#' @export
setMethod(
  f = "[[",
  signature = signature(x="LabeledVolumeSet", i="numeric"),
  definition = function(x, i, j, ...) {
    # Validity check implicit in loader

    if (length(i) != 1) {
      stop("Must provide a single index i.")
    }
    nLabels <- length(x@labels)
    if (i < 1 || i > nLabels) {
      stop("Index i out of range 1..", nLabels)
    }

    loader_func <- x@load_env$loader
    if (!is.function(loader_func)) stop("Internal error: loader function not found in LabeledVolumeSet environment.")

    # use the environment's loader to get that one volume
    vol <- tryCatch(loader_func(i), error = function(e) {
        stop("Failed to load volume ", i, " (label: '", x@labels[i], "'): ", e$message)
    })
    vol
  }
)




# The next method is the one for character indexing `[[`, which should remain.

#' @export
setMethod(
  f = "names",
  signature = signature(x = "LabeledVolumeSet"),
  definition = function(x) {
    x@labels
  }
)

#' show method for LabeledVolumeSet
#'
#' Displays essential info about a \code{LabeledVolumeSet}, including spatial dims,
#' number of volumes, label previews, spacing, origin, orientation (if known),
#' and storage paths.
#'
#' @param object A \code{\link{LabeledVolumeSet}} instance
#' @importFrom crayon bold blue silver yellow green italic
#' @importFrom methods show
#' @export
setMethod(
  f = "show",
  signature = "LabeledVolumeSet",
  definition = function(object) {
    # Check file validity for showing file path
    file_status <- "Unknown"
    file_path_or_handle <- object@obj # Accessing the S4 slot directly

    if (inherits(file_path_or_handle, "H5File")) {
        # It's an H5File handle
        if (file_path_or_handle$is_valid) {
            file_status <- tryCatch(file_path_or_handle$get_filename(), error = function(e) "Valid Handle (path unavailable)")
        } else {
            file_status <- "CLOSED Handle"
        }
    } else if (is.character(file_path_or_handle)) {
        # It's a file path string
        file_status <- file_path_or_handle
        # We could add a check here if the file exists, but might be slow/unnecessary for 'show'
    }

    cat("\n", crayon::bold(crayon::blue("LabeledVolumeSet")), "\n", sep="")

    sp   <- space(object@mask)
    nd3  <- dim(sp)
    nvol <- length(object@labels)

    cat(crayon::bold("\n╔═ Volume Info "), crayon::silver("───────────────────────────"), "\n", sep="")
    cat("║ ", crayon::yellow("3D Dimensions"), " : ", paste(nd3, collapse=" × "), "\n", sep="")
    cat("║ ", crayon::yellow("Total Volumes"), " : ", nvol, "\n", sep="")

    lbl_preview <- object@labels[1:min(3, nvol)]
    cat("║ ", crayon::yellow("Labels"), "        : ",
        paste(lbl_preview, collapse=", "),
        if (nvol > 3) crayon::silver(paste0(" ... (", nvol - 3, " more)")), "\n", sep="")

    cat(crayon::bold("\n╠═ Spatial Info "), crayon::silver("───────────────────────────"), "\n", sep="")
    cat("║ ", crayon::yellow("Spacing"), "       : ", paste(round(sp@spacing,2), collapse=" × "), "\n", sep="")
    cat("║ ", crayon::yellow("Origin"), "        : ", paste(round(sp@origin, 2), collapse=" × "), "\n", sep="")

    if (length(sp@axes@ndim) == 1 && sp@axes@ndim >= 3) {
      cat("║ ", crayon::yellow("Orientation"), "   : ",
          paste(sp@axes@i@axis, sp@axes@j@axis, sp@axes@k@axis), "\n", sep="")
    } else {
      cat("║ ", crayon::yellow("Orientation"), "   : Unknown / Not specified\n")
    }

    cat(crayon::bold("\n╚═ Storage Info "), crayon::silver("──────────────────────────"), "\n", sep="")
    cat("  ", crayon::yellow("HDF5 File"), "    : ", file_status, "\n", sep="")
    cat("  ", crayon::yellow("Data Path"), "    : /data/<label>\n", sep="")
    cat("  ", crayon::yellow("Mask Path"), "    : /mask\n", sep="")
    cat("  ", crayon::yellow("Labels Path"), "  : /labels\n", sep="") # Added label path info

    cat("\n")
  }
)



# Define the method for character index `i`
#' @export
setMethod(
  f = "[[",
  signature = signature(x = "LabeledVolumeSet", i = "character"),
  definition = function(x, i, j, ...) {
    # Validity check implicit in loader
    
    if (length(i) != 1) {
      stop("Must provide a single label name for character index.")
    }
    
    # Find the numeric index corresponding to the label
    numeric_idx <- match(i, x@labels)
    
    if (is.na(numeric_idx)) {
      stop("Label '", i, "' not found in LabeledVolumeSet.")
    }
    
    # Call the loader function directly with the numeric index
    loader_func <- x@load_env$loader
    if (!is.function(loader_func)) stop("Internal error: loader function not found in LabeledVolumeSet environment.")
    
    vol <- tryCatch(loader_func(numeric_idx), error = function(e) {
      stop("Failed to load volume for label '", i, "' (index: ", numeric_idx, "): ", e$message)
    })
    
    vol
  }
)

#' Close the HDF5 file associated with a LabeledVolumeSet
#'
#' This method manually closes the HDF5 file handle stored within the
#' LabeledVolumeSet object. It uses the \\code{safe_h5_close} helper to
#' ensure the handle is valid before attempting to close. After closing,
#' the internal handle reference is nulled to prevent accidental reuse.
#'
#' **Important:** If this \code{LabeledVolumeSet} object was created from
#' a file path using \code{\link{read_labeled_vec}}, the user is responsible
#' for calling this \code{close} method when finished with the object to release
#' the file handle. Failure to do so will leave the file open until the R session ends.
#' If the object was created using an existing \code{H5File} handle, closing
#' remains the responsibility of the code that originally opened the handle.
#'
#' @param con A \\code{LabeledVolumeSet} object.
#' @param ... Additional arguments (ignored).
#' @return Invisibly returns \\code{NULL}.
#' @rdname close
#' @export
setMethod("close", "LabeledVolumeSet", function(con, ...) {
  if (!is.null(con@obj)) {
    safe_h5_close(con@obj)
    # Nulling out the reference is problematic if the slot expects an H5File object.
    # The hdf5r object itself will become invalid after closing.
    # con@obj <- NULL # This line can cause S4 validation error
  }
  invisible(NULL)
})
</file>

<file path="R/zzz_example_helpers.R">
# Helper functions for creating minimal objects for Roxygen examples
# These are internal and intended to be called via fmristore:::

#' Create a minimal LogicalNeuroVol for examples
#'
#' @param dims 3D dimensions, e.g., c(3L, 3L, 2L).
#' @param true_voxels A list of 3-element integer vectors for TRUE voxels, 
#'   e.g., list(c(1L,1L,1L), c(2L,1L,1L)). If NULL, creates a small default pattern.
#' @return A \code{LogicalNeuroVol} object.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE)) {
#'   logi_vol <- fmristore:::create_minimal_LogicalNeuroVol()
#'   print(logi_vol)
#'   logi_vol2 <- fmristore:::create_minimal_LogicalNeuroVol(dims = c(2,2,2), 
#'                                                          true_voxels = list(c(1L,1L,1L)))
#'   print(logi_vol2)
#' }
create_minimal_LogicalNeuroVol <- function(dims = c(3L, 3L, 2L), true_voxels = NULL) {
  if (!requireNamespace("neuroim2", quietly = TRUE)) {
    stop("Package 'neuroim2' is needed for this helper function.")
  }
  space <- neuroim2::NeuroSpace(dims)
  arr <- array(FALSE, dim = dims)
  if (is.null(true_voxels)) {
    # Default pattern: first few voxels
    arr[1L,1L,1L] <- TRUE
    if (prod(dims) > 1 && dims[1] >= 2) arr[2L,1L,1L] <- TRUE
  } else {
    for (v_coord in true_voxels) {
      if (length(v_coord) == 3 && all(v_coord >= 1L) && all(v_coord <= dims)) {
        arr[v_coord[1], v_coord[2], v_coord[3]] <- TRUE
      } else {
        warning("Skipping invalid true_voxel coordinate: ", paste(v_coord, collapse=", "))
      }
    }
  }
  return(neuroim2::LogicalNeuroVol(arr, space))
}

#' Create a minimal DenseNeuroVec for examples
#'
#' @param dims 4D dimensions, e.g., c(3L, 3L, 2L, 4L).
#' @return A \code{DenseNeuroVec} object with minimal sequential data.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE)) {
#'   dvec <- fmristore:::create_minimal_DenseNeuroVec()
#'   print(dvec)
#' }
create_minimal_DenseNeuroVec <- function(dims = c(3L, 3L, 2L, 4L)) {
  if (!requireNamespace("neuroim2", quietly = TRUE)) {
    stop("Package 'neuroim2' is needed for this helper function.")
  }
  space <- neuroim2::NeuroSpace(dims)
  arr <- array(seq_len(prod(dims)), dim = dims) 
  return(neuroim2::DenseNeuroVec(arr, space))
}

#' Create a minimal ClusteredNeuroVol for examples
#'
#' @param mask_vol A \code{LogicalNeuroVol} to use as the mask. 
#'   If \code{NULL}, a default one is created.
#' @param num_clusters Integer, number of clusters to create.
#' @return A \code{ClusteredNeuroVol} object.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE)) {
#'   clust_vol <- fmristore:::create_minimal_ClusteredNeuroVol()
#'   print(clust_vol)
#'   
#'   custom_mask <- fmristore:::create_minimal_LogicalNeuroVol(dims = c(5,5,3))
#'   clust_vol2 <- fmristore:::create_minimal_ClusteredNeuroVol(mask_vol = custom_mask, 
#'                                                              num_clusters = 3L)
#'   print(clust_vol2)
#' }
create_minimal_ClusteredNeuroVol <- function(mask_vol = NULL, num_clusters = 2L) {
  if (!requireNamespace("neuroim2", quietly = TRUE)) {
    stop("Package 'neuroim2' is needed for this helper function.")
  }
  if (is.null(mask_vol)) {
    mask_vol <- create_minimal_LogicalNeuroVol(dims = c(4L,4L,3L), 
                                               true_voxels = list(c(1L,1L,1L), c(2L,1L,1L), 
                                                                  c(1L,2L,1L), c(2L,2L,1L),
                                                                  c(1L,1L,2L), c(2L,1L,2L)))
  }
  
  n_vox_in_mask <- sum(mask_vol@.Data)
  if (n_vox_in_mask == 0) {
    stop("Provided or default mask_vol is empty. Cannot create ClusteredNeuroVol.")
  }
  if (num_clusters <= 0L) {
    stop("'num_clusters' must be positive.")
  }
  
  cluster_data <- rep_len(seq_len(num_clusters), n_vox_in_mask)
  cluster_labels <- paste0("Cluster", seq_len(num_clusters))
  label_map <- stats::setNames(as.list(seq_len(num_clusters)), cluster_labels)
  
  return(neuroim2::ClusteredNeuroVol(mask_vol, clusters = cluster_data, label_map = label_map))
}

#' Create a minimal HDF5 file suitable for H5NeuroVol examples
#'
#' This function creates a temporary HDF5 file with the minimal structure
#' expected by the \code{H5NeuroVol} constructor.
#'
#' @param dims 3D dimensions, e.g., c(3L, 3L, 2L).
#' @param file_path Optional: path to HDF5 file. If \code{NULL}, a temp file is created.
#' @return Path to the created HDF5 file.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE) && 
#'     requireNamespace("hdf5r", quietly = TRUE)) {
#'   temp_file <- fmristore:::create_minimal_h5_for_H5NeuroVol()
#'   print(temp_file)
#'   # Example usage:
#'   # h5vol <- H5NeuroVol(temp_file)
#'   # print(h5vol)
#'   # close(h5vol)
#'   if (file.exists(temp_file)) unlink(temp_file)
#' }
create_minimal_h5_for_H5NeuroVol <- function(dims = c(3L, 3L, 2L), file_path = NULL) {
  if (!requireNamespace("neuroim2", quietly = TRUE) || !requireNamespace("hdf5r", quietly = TRUE)) {
    stop("Packages \'neuroim2\' and \'hdf5r\' are needed for this helper function.")
  }
  if (is.null(file_path)) {
    out_file <- tempfile(fileext = ".h5vol_example.h5")
  } else {
    out_file <- file_path
  }

  h5info <- NULL
  h5f <- NULL
  tryCatch({
    h5info <- open_h5(out_file, mode = "w")
    h5f <- h5info$h5
    
    hdf5r::h5attr(h5f, "rtype") <- "DenseNeuroVol"
    
    sp <- neuroim2::NeuroSpace(dims)
    
    space_grp <- h5f$create_group("space")
    space_grp$create_dataset("dim", robj = as.integer(dim(sp)), 
                             dtype = hdf5r::h5types$H5T_NATIVE_INT)
    space_grp$create_dataset("origin", robj = as.double(neuroim2::origin(sp)), 
                             dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    space_grp$create_dataset("trans", robj = neuroim2::trans(sp), 
                             dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    # space_grp$create_dataset("spacing", robj = as.double(neuroim2::spacing(sp)), 
    #                          dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)

    data_grp <- h5f$create_group("data")
    data_arr <- array(stats::rnorm(prod(dims)), dim = dims)
    data_grp$create_dataset("elements", robj = data_arr, 
                            dtype = hdf5r::h5types$H5T_NATIVE_FLOAT)
    
  }, error = function(e) {
    # Ensure file is closed if error occurs during creation, then rethrow
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) try(h5f$close_all(), silent = TRUE)
    stop(sprintf("Error creating minimal HDF5 for H5NeuroVol at %s: %s", out_file, e$message))
  }, finally = {
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) {
      try(h5f$close_all(), silent = TRUE)
    }
  })
  return(out_file)
}

#' Create a minimal HDF5 file suitable for H5NeuroVec examples
#'
#' This function creates a temporary HDF5 file with the minimal structure
#' expected by the \code{H5NeuroVec} constructor (rtype, /space, /data).
#'
#' @param dims 4D dimensions, e.g., c(3L, 3L, 2L, 5L).
#' @param file_path Optional: path to HDF5 file. If \code{NULL}, a temp file is created.
#' @return Path to the created HDF5 file.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE) && 
#'     requireNamespace("hdf5r", quietly = TRUE)) {
#'   temp_file <- fmristore:::create_minimal_h5_for_H5NeuroVec()
#'   print(temp_file)
#'   # Example usage:
#'   # h5vec <- H5NeuroVec(temp_file)
#'   # print(h5vec)
#'   # close(h5vec)
#'   if (file.exists(temp_file)) unlink(temp_file)
#' }
create_minimal_h5_for_H5NeuroVec <- function(dims = c(3L, 3L, 2L, 5L), file_path = NULL) {
  if (!requireNamespace("neuroim2", quietly = TRUE) || !requireNamespace("hdf5r", quietly = TRUE)) {
    stop("Packages \'neuroim2\' and \'hdf5r\' are needed for this helper function.")
  }
  if (length(dims) != 4) stop("\'dims\' must be a 4-element vector for H5NeuroVec.")

  if (is.null(file_path)) {
    out_file <- tempfile(fileext = ".h5vec_example.h5")
  } else {
    out_file <- file_path
  }

  h5info <- NULL
  h5f <- NULL
  tryCatch({
    h5info <- open_h5(out_file, mode = "w")
    h5f <- h5info$h5
    
    hdf5r::h5attr(h5f, "rtype") <- "DenseNeuroVec" # Assuming H5NeuroVec is a DenseNeuroVec
    
    sp <- neuroim2::NeuroSpace(dims)
    
    space_grp <- h5f$create_group("space")
    space_grp$create_dataset("dim", robj = as.integer(dim(sp)), 
                             dtype = hdf5r::h5types$H5T_NATIVE_INT)
    space_grp$create_dataset("origin", robj = as.double(neuroim2::origin(sp)), 
                             dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    space_grp$create_dataset("trans", robj = neuroim2::trans(sp), 
                             dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)

    data_grp <- h5f$create_group("data")
    data_arr <- array(stats::rnorm(prod(dims)), dim = dims)
    data_grp$create_dataset("elements", robj = data_arr, 
                            dtype = hdf5r::h5types$H5T_NATIVE_FLOAT) 
    
  }, error = function(e) {
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) try(h5f$close_all(), silent = TRUE)
    stop(sprintf("Error creating minimal HDF5 for H5NeuroVec at %s: %s", out_file, e$message))
  }, finally = {
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) {
      try(h5f$close_all(), silent = TRUE)
    }
  })
  return(out_file)
}

#' Create a minimal LatentNeuroVec for examples
#'
#' @param space_dims 3D spatial dimensions for the underlying space, e.g., c(10L, 10L, 3L).
#' @param n_time Number of time points (columns in basis).
#' @param n_comp Number of components (rows in basis, columns in loadings).
#' @param n_mask_voxels Number of voxels to include in the mask. If NULL, ~20% of space_dims.
#' @return A \code{LatentNeuroVec} object.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE)) {
#'   lnv <- fmristore:::create_minimal_LatentNeuroVec()
#'   print(lnv)
#' }
create_minimal_LatentNeuroVec <- function(space_dims = c(6L, 6L, 3L), 
                                          n_time = 10L, 
                                          n_comp = 3L, 
                                          n_mask_voxels = NULL) {
  if (!requireNamespace("neuroim2", quietly = TRUE)) {
    stop("Package 'neuroim2' is needed for this helper function.")
  }

  # Create a mask
  if (is.null(n_mask_voxels)) {
    n_mask_voxels <- floor(prod(space_dims) * 0.2)
    if (n_mask_voxels == 0 && prod(space_dims) > 0) n_mask_voxels <- 1L
  }
  
  mask_arr <- array(FALSE, dim = space_dims)
  if (n_mask_voxels > 0 && n_mask_voxels <= prod(space_dims)) {
    mask_indices <- sample(prod(space_dims), n_mask_voxels)
    mask_arr[mask_indices] <- TRUE
  }
  mask_space <- neuroim2::NeuroSpace(space_dims)
  mask_vol <- neuroim2::LogicalNeuroVol(mask_arr, mask_space)

  # Create basis functions (time x components)
  basis_mat <- matrix(stats::rnorm(n_time * n_comp), nrow = n_time, ncol = n_comp)

  # Create loadings (masked voxels x components)
  # Number of rows in loadings must match number of TRUE voxels in mask
  actual_n_mask_voxels <- sum(mask_vol@.Data)
  if (actual_n_mask_voxels == 0 && n_mask_voxels > 0) {
      # This case should ideally not be hit if mask creation is robust
      stop("Mask creation resulted in zero voxels, cannot create LatentNeuroVec loadings.")
  }
  if (actual_n_mask_voxels > 0) {
    loadings_mat <- matrix(stats::rnorm(actual_n_mask_voxels * n_comp), 
                         nrow = actual_n_mask_voxels, ncol = n_comp)
  } else {
    # Handle case of empty mask (e.g. if n_mask_voxels was 0)
    loadings_mat <- matrix(0, nrow = 0, ncol = n_comp)
  }
  
  # Create LatentNeuroVec
  # The constructor takes the full NeuroSpace for the 4D representation, not just the mask's space.
  # The 4th dimension is n_time.
  full_space_dims <- c(space_dims, n_time)
  full_space <- neuroim2::NeuroSpace(dim = full_space_dims, 
                                     spacing = neuroim2::spacing(mask_space), 
                                     origin = neuroim2::origin(mask_space), 
                                     axes = neuroim2::axes(mask_space)) # Retain spatial axes info
                                    
  # The LatentNeuroVec constructor from neuroim2 might be:
  # LatentNeuroVec(mask, loadings, basis, space)
  # where 'space' is the 4D space.  Need to check the fmristore definition or neuroim2.
  # Assuming fmristore's LatentNeuroVec is similar or identical to neuroim2's.
  # If fmristore has its own definition, this might need adjustment.
  
  # From R/latent_vec.R, the constructor seems to be new("LatentNeuroVec", ...) or LatentNeuroVec(...),
  # which takes `mask`, `loadings`, `basis`, `space`.
  
  return(neuroim2::LatentNeuroVec(mask = mask_vol, 
                                  loadings = loadings_mat, 
                                  basis = basis_mat, 
                                  space = full_space))
}

#' Create a minimal HDF5 file suitable for LabeledVolumeSet examples (via read_labeled_vec)
#'
#' This function creates a temporary HDF5 file with a minimal structure 
#' that can be read by \code{read_labeled_vec} to produce a \code{LabeledVolumeSet}.
#'
#' Refer to \code{write_labeled_vec} and \code{read_labeled_vec} for the expected structure.
#'
#' @param vol_dims 3D spatial dimensions for each volume, e.g., c(4L, 4L, 3L).
#' @param labels A character vector of labels, e.g., c("ConditionA", "ConditionB").
#' @param num_vols_per_label Integer, number of volumes to generate for each label.
#' @param file_path Optional: path to HDF5 file. If \code{NULL}, a temp file is created.
#' @return Path to the created HDF5 file.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE) && 
#'     requireNamespace("hdf5r", quietly = TRUE) && 
#'     exists("read_labeled_vec", where = "package:fmristore")) {
#'   temp_file <- fmristore:::create_minimal_h5_for_LabeledVolumeSet()
#'   print(temp_file)
#'   # Example usage (now should work with read_labeled_vec):
#'   lvs <- NULL
#'   tryCatch({
#'     lvs <- fmristore::read_labeled_vec(temp_file)
#'     print(lvs)
#'     print(labels(lvs))
#'   }, finally = {
#'     if (!is.null(lvs)) try(close(lvs), silent = TRUE)
#'     if (file.exists(temp_file)) unlink(temp_file)
#'   })
#' }
create_minimal_h5_for_LabeledVolumeSet <- function(vol_dims = c(4L, 4L, 3L), 
                                                   labels = c("Set1", "Set2"), 
                                                   num_vols_per_label = 1L, # Simplified: 1 vol per label for minimal example
                                                   file_path = NULL) {
  if (!requireNamespace("neuroim2", quietly = TRUE) || !requireNamespace("hdf5r", quietly = TRUE)) {
    stop("Packages 'neuroim2' and 'hdf5r' are needed for this helper function.")
  }
  if (length(vol_dims) != 3) stop("'vol_dims' must be a 3-element vector.")
  if (num_vols_per_label <= 0) stop("'num_vols_per_label' must be positive.")
  if (length(labels) == 0) stop("'labels' must not be empty.")

  if (is.null(file_path)) {
    out_file <- tempfile(fileext = ".lvs_example.h5")
  } else {
    out_file <- file_path
  }

  # Total number of distinct volumes/datasets to create = number of labels * num_vols_per_label
  # For simplicity with read_labeled_vec which expects one dataset per unique label in /labels,
  # this helper will now create one dataset per entry in `labels`.
  # If num_vols_per_label > 1, the labels vector should reflect that (e.g. c("CondA_1", "CondA_2"))
  # For this revised helper, let's assume `labels` are unique and num_vols_per_label implies
  # that if you want multiple volumes for "Set1", you should provide e.g. labels = c("Set1_run1", "Set1_run2")
  # For true minimality for read_labeled_vec, num_vols_per_label = 1 is assumed per unique label.
  # The `labels` argument now directly defines the datasets to be created.

  h5info <- NULL
  h5f <- NULL
  tryCatch({
    h5info <- open_h5(out_file, mode = "w")
    h5f <- h5info$h5
    
    # Root attributes (mimicking write_labeled_vec)
    hdf5r::h5attr(h5f, "class") <- "LabeledVolumeSet" # Or as expected by read_labeled_vec if different
    hdf5r::h5attr(h5f, "version") <- "0.1" 

    # Create a simple mask (all TRUE for simplicity)
    mask_data_arr <- array(TRUE, dim = vol_dims)
    # Ensure mask is written as UCHAR for compatibility if read_labeled_vec expects it
    h5f$create_dataset("mask", robj = as.integer(mask_data_arr), dtype = hdf5r::h5types$H5T_NATIVE_UCHAR)

    # Labels dataset (vector of unique labels provided)
    # Use H5T_STRING$new(type="c") for variable length C strings, or fixed size if appropriate
    str_type <- hdf5r::H5T_STRING$new(type="c", size = Inf) # Variable length
    on.exit(str_type$close(), add = TRUE)
    h5f$create_dataset("labels", robj = labels, dtype = str_type)

    # /data group
    if (!h5f$exists("data")) data_grp <- h5f$create_group("data")
    else data_grp <- h5f[["data"]]
    
    # Create one dataset per label under /data/
    # This dataset will contain the 1D masked data for that label.
    n_vox_in_mask <- sum(mask_data_arr)
    if (n_vox_in_mask == 0) stop("Mask is empty, cannot create data datasets.")

    sanitize_label_func <- function(lbl) { gsub("[^A-Za-z0-9_.-]", "_", lbl) }

    for (lab_name in labels) {
      # Data for this label (1D vector of length n_vox_in_mask)
      label_vol_data_1d <- stats::rnorm(n_vox_in_mask)
      safe_lab_name <- sanitize_label_func(lab_name)
      
      # Path like /data/Label1, /data/Label2
      dataset_path <- file.path("data", safe_lab_name)
      data_grp$create_dataset(safe_lab_name, robj = label_vol_data_1d, 
                              dtype = hdf5r::h5types$H5T_NATIVE_FLOAT, # Consistent with write_labeled_vec default
                              chunk_dims = if(n_vox_in_mask > 0) min(1024L, n_vox_in_mask) else NULL,
                              compress_level = 0L # Minimal example, no compression needed
                              )
    }

    # Minimal NIfTI-like header information (required by read_labeled_vec to build the space)
    if (!h5f$exists("header")) header_grp <- h5f$create_group("header")
    else header_grp <- h5f[["header"]]
    
    sp <- neuroim2::NeuroSpace(vol_dims) # Basic space for one volume
    # nVols in header/dim should match length of /labels
    n_labels_for_header <- length(labels)
    header_grp$create_dataset("dim", robj = as.integer(c(4L, vol_dims, n_labels_for_header, 1L, 1L, 1L)), dtype = hdf5r::h5types$H5T_NATIVE_INT)
    header_grp$create_dataset("pixdim", robj = as.double(c(0.0, neuroim2::spacing(sp), rep(0,4))), dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    
    q_info <- tryCatch(neuroim2::matrixToQuatern(neuroim2::trans(sp)), error=function(e) NULL)
    if (is.null(q_info)) { # Default q_info if matrixToQuatern fails (e.g. singular matrix)
        q_info <- list(quaternion=c(0,0,0), qoffset=neuroim2::origin(sp), qfac=1)
    }

    header_grp$create_dataset("quatern_b", robj = q_info$quaternion[1], dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("quatern_c", robj = q_info$quaternion[2], dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("quatern_d", robj = q_info$quaternion[3], dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("qoffset_x", robj = q_info$qoffset[1], dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("qoffset_y", robj = q_info$qoffset[2], dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("qoffset_z", robj = q_info$qoffset[3], dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("qfac", robj = q_info$qfac, dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    
    affine_mat <- neuroim2::trans(sp)
    srow_x <- affine_mat[1,]
    srow_y <- affine_mat[2,]
    srow_z <- affine_mat[3,]
    header_grp$create_dataset("srow_x", robj = as.double(srow_x), dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("srow_y", robj = as.double(srow_y), dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("srow_z", robj = as.double(srow_z), dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    header_grp$create_dataset("qform_code", robj = as.integer(1), dtype = hdf5r::h5types$H5T_NATIVE_INT)
    header_grp$create_dataset("sform_code", robj = as.integer(1), dtype = hdf5r::h5types$H5T_NATIVE_INT)
    header_grp$create_dataset("sizeof_hdr", robj = as.integer(348), dtype = hdf5r::h5types$H5T_NATIVE_INT)
    header_grp$create_dataset("magic", robj = "n+1", dtype = hdf5r::H5T_STRING$new(type="c", size=4L))

  }, error = function(e) {
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) try(h5f$close_all(), silent = TRUE)
    stop(sprintf("Error creating aligned HDF5 for LabeledVolumeSet at %s: %s", out_file, e$message))
  }, finally = {
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) {
      try(h5f$close_all(), silent = TRUE)
    }
  })
  return(out_file)
}

#' Populate HDF5 structure for a single H5ClusterRun within an existing H5File
#'
#' This function is a helper for creating minimal H5ClusterExperiment examples.
#' It assumes the H5File object is open and writable.
#'
#' @param h5obj An open, writable \code{H5File} object (from package hdf5r).
#' @param scan_name Character string, the name for this scan/run.
#' @param mask_vol A \code{LogicalNeuroVol} object for the mask of this run.
#' @param cluster_map_vol A \code{ClusteredNeuroVol} object representing the cluster assignments
#'   within the mask. Its space must match \code{mask_vol}.
#' @param n_time Integer, the number of time points for the cluster data.
#' @param compress Logical, whether to compress HDF5 datasets (minimal impact for small data).
#' @keywords internal
#' @return Invisibly returns TRUE on success, or stops on error.
populate_H5ClusterRun_in_h5file <- function(h5obj,
                                                  scan_name,
                                                  mask_vol,
                                                  cluster_map_vol,
                                                  n_time,
                                                  compress = FALSE) {
  if (!inherits(h5obj, "H5File") || !h5obj$is_valid) {
    stop("'h5obj' must be a valid, open H5File object.")
  }
  if (!is(mask_vol, "LogicalNeuroVol")) stop("'mask_vol' must be a LogicalNeuroVol.")
  if (!is(cluster_map_vol, "ClusteredNeuroVol")) stop("'cluster_map_vol' must be a ClusteredNeuroVol.")
  if (!identical(neuroim2::space(mask_vol), neuroim2::space(cluster_map_vol))) {
    stop("Space of 'mask_vol' and 'cluster_map_vol' must be identical.")
  }
  if (n_time <= 0) stop("'n_time' must be positive.")

  tryCatch({
    if (!h5obj$exists("scans")) {
      h5obj$create_group("scans")
    }
    scans_grp <- h5obj[["scans"]]
    
    if (scans_grp$exists(scan_name)) {
      stop(sprintf("Scan group '%s' already exists in /scans.", scan_name))
    }
    run_grp <- scans_grp$create_group(scan_name)
    
    # Set attributes for the run group
    hdf5r::h5attr(run_grp, "class") <- "H5ClusterRun"
    hdf5r::h5attr(run_grp, "n_time") <- as.integer(n_time)
    # fmristore version attribute could be added if constructor expects/uses it.

    # Write mask for this run (can be different from experiment master mask in general)
    # For simplicity, using the mask_vol@.Data which is a 3D array
    run_grp$create_dataset("mask", robj = mask_vol@.Data, dtype = hdf5r::h5types$H5T_NATIVE_HBOOL,
                               chunk_dims = "auto", compress_level = if(compress) 4L else 0L)

    # Extract cluster information from ClusteredNeuroVol
    # cluster_map is a vector of cluster IDs for voxels *within the mask_vol*
    # cluster_ids are the unique, sorted cluster IDs present.
    # cluster_names are derived from the label_map of ClusteredNeuroVol.
    
    # Get cluster assignments only for voxels TRUE in mask_vol
    cluster_assignments_in_mask <- cluster_map_vol@.Data[mask_vol@.Data]
    unique_cluster_ids <- sort(unique(cluster_assignments_in_mask[cluster_assignments_in_mask > 0])) # Exclude 0 if it means unclustered

    if (length(unique_cluster_ids) == 0) {
      warning(sprintf("Scan '%s': No positive cluster IDs found in cluster_map_vol within the mask. Creating empty cluster datasets.", scan_name))
      # Still create the datasets for schema completeness if needed by constructor
      run_grp$create_dataset("cluster_map", robj = integer(0), dtype = hdf5r::h5types$H5T_NATIVE_INT)
      run_grp$create_dataset("cluster_ids", robj = integer(0), dtype = hdf5r::h5types$H5T_NATIVE_INT)
      run_grp$create_dataset("cluster_names", robj = character(0), dtype = hdf5r::H5T_STRING$new(type="c"))
      run_grp$create_group("clusters") # Empty group
    } else {
      # Write cluster_map (dense vector for voxels within the mask)
      run_grp$create_dataset("cluster_map", robj = as.integer(cluster_assignments_in_mask), 
                               dtype = hdf5r::h5types$H5T_NATIVE_INT)
      
      # Write cluster_ids
      run_grp$create_dataset("cluster_ids", robj = as.integer(unique_cluster_ids), 
                               dtype = hdf5r::h5types$H5T_NATIVE_INT)
      
      # Write cluster_names (try to get from ClusteredNeuroVol's label_map)
      # Match unique_cluster_ids to names. Default to "Cluster_<id>" if not found.
      cl_names <- character(length(unique_cluster_ids))
      if (!is.null(cluster_map_vol@label_map) && length(cluster_map_vol@label_map) > 0) {
        # label_map stores names as keys and IDs as values, or vice-versa depending on neuroim2
        # neuroim2::ClusteredNeuroVol: label_map is a list where names are labels and values are cluster IDs.
        # We need names for our unique_cluster_ids.
        for (i in seq_along(unique_cluster_ids)) {
          id <- unique_cluster_ids[i]
          found_name <- FALSE
          for (name_in_map in names(cluster_map_vol@label_map)) {
            if (id %in% cluster_map_vol@label_map[[name_in_map]]) { # ID can be a vector in label_map value
              cl_names[i] <- name_in_map
              found_name <- TRUE
              break
            }
          }
          if (!found_name) cl_names[i] <- paste0("Cluster_", id)
        }
      } else {
        cl_names <- paste0("Cluster_", unique_cluster_ids)
      }
      run_grp$create_dataset("cluster_names", robj = cl_names, dtype = hdf5r::H5T_STRING$new(type="c"))

      # Write cluster data (time series for each cluster)
      # /scans/<scan_name>/clusters/cluster_<id>
      clusters_data_grp <- run_grp$create_group("clusters")
      for (id in unique_cluster_ids) {
        # Number of voxels in this specific cluster for this run
        n_vox_in_this_cluster <- sum(cluster_assignments_in_mask == id)
        if (n_vox_in_this_cluster > 0) {
          # Data: n_time x n_vox_in_this_cluster
          # The H5ClusterRun constructor expects data loaded to be n_vox x n_time (from summary)
          # or reads n_time x n_vox from disk if that's how write_clustered_run_h5 does it.
          # write_clustered_run_h5 writes dataset as h5_write(..., robj = data_for_cluster) # data_for_cluster is n_vox x n_time from input
          # So, the dataset on disk should be n_vox x n_time. But cluster_ts() returns n_time x n_vox.
          # Let's assume disk storage is n_time x n_vox_in_cluster for now, common for time series.
          # If constructor expects n_vox x n_time, then this needs to be transposed before constructor is called, or disk format adjusted.
          # The H5ClusterRun constructor: data for each cluster is loaded as: dset[,]
          # Let's use n_time x n_vox_in_cluster on disk.
          cluster_ts_data <- matrix(stats::rnorm(n_time * n_vox_in_this_cluster), 
                                   nrow = n_time, ncol = n_vox_in_this_cluster)
          clusters_data_grp$create_dataset(paste("cluster_", id, sep=""), robj = cluster_ts_data, 
                                   dtype = hdf5r::h5types$H5T_NATIVE_FLOAT,
                                   chunk_dims = "auto", compress_level = if(compress) 4L else 0L)
        } else {
           # Should not happen if id is from unique_cluster_ids > 0 and cluster_assignments_in_mask is not all 0
           warning(sprintf("Scan '%s', Cluster '%s': No voxels found. Skipping data dataset.", scan_name, id))
        }
      }
    }
    
  }, error = function(e) {
    stop(sprintf("Error populating H5ClusterRun for scan '%s': %s", scan_name, e$message))
  })
  
  invisible(TRUE)
}

#' Populate HDF5 structure for a single H5ClusterRunSummary within an existing H5File
#'
#' This function is a helper for creating minimal H5ClusterExperiment examples.
#' It assumes the H5File object is open and writable.
#'
#' @param h5obj An open, writable \code{H5File} object (from package hdf5r).
#' @param scan_name Character string, the name for this scan/run.
#' @param mask_vol A \code{LogicalNeuroVol} object for the mask of this run.
#' @param cluster_map_vol A \code{ClusteredNeuroVol} object representing the cluster assignments
#'   within the mask. Its space must match \code{mask_vol}.
#' @param n_time Integer, the number of time points for the summary data.
#' @param summary_stat_names A character vector of summary statistics to generate (e.g., "mean", "median").
#'   For each stat, a dataset \code{summary_stats/<stat_name>/cluster_<id>} will be created.
#' @param compress Logical, whether to compress HDF5 datasets.
#' @keywords internal
#' @return Invisibly returns TRUE on success, or stops on error.
populate_H5ClusterRunSummary_in_h5file <- function(h5obj,
                                                     scan_name,
                                                     mask_vol,
                                                     cluster_map_vol,
                                                     n_time,
                                                     summary_stat_names = c("mean"),
                                                     compress = FALSE) {
  if (!inherits(h5obj, "H5File") || !h5obj$is_valid) {
    stop("'h5obj' must be a valid, open H5File object.")
  }
  if (!is(mask_vol, "LogicalNeuroVol")) stop("'mask_vol' must be a LogicalNeuroVol.")
  if (!is(cluster_map_vol, "ClusteredNeuroVol")) stop("'cluster_map_vol' must be a ClusteredNeuroVol.")
  if (!identical(neuroim2::space(mask_vol), neuroim2::space(cluster_map_vol))) {
    stop("Space of 'mask_vol' and 'cluster_map_vol' must be identical.")
  }
  if (n_time <= 0) stop("'n_time' must be positive.")
  if (length(summary_stat_names) == 0) stop("'summary_stat_names' cannot be empty.")

  tryCatch({
    if (!h5obj$exists("scans")) {
      h5obj$create_group("scans")
    }
    scans_grp <- h5obj[["scans"]]
    
    if (scans_grp$exists(scan_name)) {
      stop(sprintf("Scan group '%s' already exists in /scans.", scan_name))
    }
    run_grp <- scans_grp$create_group(scan_name)
    
    # Set attributes for the run group
    hdf5r::h5attr(run_grp, "class") <- "H5ClusterRunSummary"
    hdf5r::h5attr(run_grp, "n_time") <- as.integer(n_time)

    # Write mask, cluster_map, cluster_ids, cluster_names (same as H5ClusterRun part)
    run_grp$create_dataset("mask", robj = mask_vol@.Data, dtype = hdf5r::h5types$H5T_NATIVE_HBOOL,
                               chunk_dims = "auto", compress_level = if(compress) 4L else 0L)

    cluster_assignments_in_mask <- cluster_map_vol@.Data[mask_vol@.Data]
    unique_cluster_ids <- sort(unique(cluster_assignments_in_mask[cluster_assignments_in_mask > 0]))

    if (length(unique_cluster_ids) == 0) {
      warning(sprintf("Scan '%s': No positive cluster IDs. Creating empty cluster info and summary_stats.", scan_name))
      run_grp$create_dataset("cluster_map", robj = integer(0), dtype = hdf5r::h5types$H5T_NATIVE_INT)
      run_grp$create_dataset("cluster_ids", robj = integer(0), dtype = hdf5r::h5types$H5T_NATIVE_INT)
      run_grp$create_dataset("cluster_names", robj = character(0), dtype = hdf5r::H5T_STRING$new(type="c"))
      run_grp$create_group("summary_stats") # Empty group
    } else {
      run_grp$create_dataset("cluster_map", robj = as.integer(cluster_assignments_in_mask), 
                               dtype = hdf5r::h5types$H5T_NATIVE_INT)
      run_grp$create_dataset("cluster_ids", robj = as.integer(unique_cluster_ids), 
                               dtype = hdf5r::h5types$H5T_NATIVE_INT)
      
      cl_names <- character(length(unique_cluster_ids))
      if (!is.null(cluster_map_vol@label_map) && length(cluster_map_vol@label_map) > 0) {
        for (i in seq_along(unique_cluster_ids)) {
          id <- unique_cluster_ids[i]
          found_name <- FALSE
          for (name_in_map in names(cluster_map_vol@label_map)) {
            if (id %in% cluster_map_vol@label_map[[name_in_map]]) {
              cl_names[i] <- name_in_map
              found_name <- TRUE
              break
            }
          }
          if (!found_name) cl_names[i] <- paste0("Cluster_", id)
        }
      } else {
        cl_names <- paste0("Cluster_", unique_cluster_ids)
      }
      run_grp$create_dataset("cluster_names", robj = cl_names, dtype = hdf5r::H5T_STRING$new(type="c"))

      # Write summary statistics data
      # /scans/<scan_name>/summary_stats/<stat_name>/cluster_<id>
      summary_stats_grp <- run_grp$create_group("summary_stats")
      for (stat_name in summary_stat_names) {
        stat_spec_grp <- summary_stats_grp$create_group(stat_name)
        for (id in unique_cluster_ids) {
          # Summary data: n_time x 1 (e.g., mean time series for the cluster)
          summary_ts_data <- matrix(stats::rnorm(n_time * 1), nrow = n_time, ncol = 1)
          stat_spec_grp$create_dataset(paste("cluster_", id, sep=""), robj = summary_ts_data, 
                                 dtype = hdf5r::h5types$H5T_NATIVE_FLOAT,
                                 chunk_dims = "auto", compress_level = if(compress) 4L else 0L)
        }
      }
    }

  }, error = function(e) {
    stop(sprintf("Error populating H5ClusterRunSummary for scan '%s': %s", scan_name, e$message))
  })
  
  invisible(TRUE)
}

#' Create a minimal HDF5 file suitable for H5ClusterExperiment examples
#'
#' This function creates a temporary HDF5 file with a minimal but valid structure 
#' for an H5ClusterExperiment, including a master mask, master cluster definitions,
#' one H5ClusterRun, and one H5ClusterRunSummary.
#'
#' @param file_path Optional: path to HDF5 file. If \code{NULL}, a temp file is created.
#' @param master_mask_dims 3D dimensions for the master mask, e.g., c(5L, 5L, 4L).
#' @param num_master_clusters Integer, number of clusters in the master cluster map.
#' @param n_time_run1 Integer, n_time for the first (full) run.
#' @param n_time_run2 Integer, n_time for the second (summary) run.
#' @return Path to the created HDF5 file.
#' @keywords internal
#' @examples
#' # Not typically called directly by users, used in other examples via fmristore:::
#' if (requireNamespace("neuroim2", quietly = TRUE) && 
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5ClusterExperiment", where = "package:fmristore")) {
#'   
#'   # Ensure helper dependencies are available if not automatically sourced
#'   # (e.g. source the R/zzz_example_helpers.R file if running interactively outside pkg build)
#' 
#'   temp_exp_file <- NULL
#'   exp <- NULL
#'   tryCatch({
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     print(temp_exp_file)
#'     # exp <- fmristore::H5ClusterExperiment(temp_exp_file)
#'     # print(exp)
#'   }, finally = {
#'     # if (!is.null(exp)) close(exp) # Close H5ClusterExperiment handle
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) unlink(temp_exp_file)
#'   })
#' }
create_minimal_h5_for_H5ClusterExperiment <- function(
    file_path = NULL, 
    master_mask_dims = c(5L, 5L, 4L),
    num_master_clusters = 3L,
    n_time_run1 = 10L,
    n_time_run2 = 12L
  ) {

  if (!requireNamespace("neuroim2", quietly = TRUE) || !requireNamespace("hdf5r", quietly = TRUE)) {
    stop("Packages 'neuroim2' and 'hdf5r' are needed for this helper function.")
  }
  
  # 1. Create master mask and cluster map objects (in-memory)
  master_mask_vol <- create_minimal_LogicalNeuroVol(dims = master_mask_dims)
  # Ensure mask is not empty for cluster_map_vol creation
  if (sum(master_mask_vol@.Data) == 0) {
      warning("Default master_mask_vol is empty, attempting to create a new one with some TRUE voxels.")
      master_mask_vol <- create_minimal_LogicalNeuroVol(dims = master_mask_dims, true_voxels = list(c(1L,1L,1L)))
      if (sum(master_mask_vol@.Data) == 0) stop("Failed to create a non-empty master_mask_vol.")
  }
  master_cluster_map_vol <- create_minimal_ClusteredNeuroVol(mask_vol = master_mask_vol, 
                                                             num_clusters = num_master_clusters)

  if (is.null(file_path)) {
    out_file <- tempfile(fileext = ".h5exp_example.h5")
  } else {
    out_file <- file_path
  }

  h5info <- NULL
  h5f <- NULL
  tryCatch({
    # 2. Create the HDF5 file
    h5info <- open_h5(out_file, mode = "w")
    h5f <- h5info$h5

    # 3. Write master mask to /mask
    h5f$create_dataset("mask", robj = master_mask_vol@.Data, 
                       dtype = hdf5r::h5types$H5T_NATIVE_HBOOL)
    # Write space information for the master mask (as H5ClusterExperiment constructor uses it)
    # The constructor calls .read_space(h5obj, "/")
    master_space_grp <- h5f$create_group("space") # Should be at root if path is "/"
    master_sp_obj <- neuroim2::space(master_mask_vol)
    master_space_grp$create_dataset("dim", robj = as.integer(dim(master_sp_obj)), dtype = hdf5r::h5types$H5T_NATIVE_INT)
    master_space_grp$create_dataset("origin", robj = as.double(neuroim2::origin(master_sp_obj)), dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    master_space_grp$create_dataset("trans", robj = neuroim2::trans(master_sp_obj), dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)
    master_space_grp$create_dataset("spacing", robj = as.double(neuroim2::spacing(master_sp_obj)), dtype = hdf5r::h5types$H5T_NATIVE_DOUBLE)

    # 4. Write master cluster information to root
    master_cluster_assignments_in_mask <- master_cluster_map_vol@.Data[master_mask_vol@.Data]
    master_unique_ids <- sort(unique(master_cluster_assignments_in_mask[master_cluster_assignments_in_mask > 0]))

    if (length(master_unique_ids) > 0) {
        h5f$create_dataset("cluster_map", robj = as.integer(master_cluster_assignments_in_mask),
                           dtype = hdf5r::h5types$H5T_NATIVE_INT)
        h5f$create_dataset("cluster_ids", robj = as.integer(master_unique_ids),
                           dtype = hdf5r::h5types$H5T_NATIVE_INT)
        
        master_cl_names <- character(length(master_unique_ids))
        if (!is.null(master_cluster_map_vol@label_map) && length(master_cluster_map_vol@label_map) > 0) {
            for (i in seq_along(master_unique_ids)) {
                id <- master_unique_ids[i]
                found_name <- FALSE
                for (name_in_map in names(master_cluster_map_vol@label_map)) {
                    if (id %in% master_cluster_map_vol@label_map[[name_in_map]]) {
                        master_cl_names[i] <- name_in_map
                        found_name <- TRUE
                        break
                    }
                }
                if (!found_name) master_cl_names[i] <- paste0("MasterCluster_", id)
            }
        } else {
            master_cl_names <- paste0("MasterCluster_", master_unique_ids)
        }
        h5f$create_dataset("cluster_names", robj = master_cl_names, 
                           dtype = hdf5r::H5T_STRING$new(type="c"))
    } else {
        # Create empty datasets if no clusters, to satisfy constructor expectations if it reads them unconditionally
        h5f$create_dataset("cluster_map", robj = integer(0), dtype = hdf5r::h5types$H5T_NATIVE_INT)
        h5f$create_dataset("cluster_ids", robj = integer(0), dtype = hdf5r::h5types$H5T_NATIVE_INT)
        h5f$create_dataset("cluster_names", robj = character(0), dtype = hdf5r::H5T_STRING$new(type="c"))
        warning("Master cluster map is empty. Experiment will have no common clusters defined at root.")
    }

    # 5. Populate a full run
    populate_H5ClusterRun_in_h5file(h5obj = h5f, 
                                          scan_name = "Run1_Full", 
                                          mask_vol = master_mask_vol, # Using master mask for run
                                          cluster_map_vol = master_cluster_map_vol, # Using master clusters for run
                                          n_time = n_time_run1)

    # 6. Populate a summary run
    populate_H5ClusterRunSummary_in_h5file(h5obj = h5f, 
                                             scan_name = "Run2_Summary", 
                                             mask_vol = master_mask_vol, 
                                             cluster_map_vol = master_cluster_map_vol, 
                                             n_time = n_time_run2,
                                             summary_stat_names = c("mean", "sd"))

    # 7. Set root class attribute
    hdf5r::h5attr(h5f, "class") <- "H5ClusterExperiment"
    # Potentially add fmristore_version attribute if constructor checks it
    # hdf5r::h5attr(h5f, "fmristore_version") <- as.character(utils::packageVersion("fmristore"))

  }, error = function(e) {
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) try(h5f$close_all(), silent = TRUE)
    stop(sprintf("Error creating minimal HDF5 for H5ClusterExperiment at %s: %s", out_file, e$message))
  }, finally = {
    if (!is.null(h5info) && h5info$owns && !is.null(h5f) && h5f$is_valid) {
      try(h5f$close_all(), silent = TRUE)
    }
  })
  
  return(out_file)
}
</file>

<file path="tests/testthat/test_fmristore_extras.R">
# test_fmristore_extras.R

library(testthat)
library(neuroim2)
#library(fmristore)  # or whatever your package is called
library(Matrix)

test_that("H5NeuroVol handles empty or single-slice subsetting", {
  # Prepare a small 3D array
  arr <- array(rnorm(2*3*4), dim=c(2,3,4))
  spc <- NeuroSpace(dim=c(2,3,4))
  vol <- NeuroVol(arr, spc)

  # Write to HDF5
  tmpfile <- tempfile(fileext=".h5")
  #on.exit(unlink(tmpfile))
  h5vol <- as_h5(vol, tmpfile, data_type="DOUBLE", chunk_dim=c(2,3,1))

  # Single slice
  slice1 <- h5vol[1,,]
  expect_equal(dim(slice1), c(3,4))
  expect_equal(slice1, arr[1,,])

  # Subset that is empty
  empty_sub <- h5vol[ integer(0), 2, ]
  expect_equal(length(empty_sub), 0)
  expect_equal(dim(empty_sub), c(0,4))

  # Single voxel
  single_vox <- h5vol[1,2,3]
  expect_length(single_vox, 1)
  expect_equal(single_vox, arr[1,2,3])

  # Make sure file is still valid
  expect_true(h5vol@h5obj$is_valid)
})

test_that("H5NeuroVol error handling for out-of-range indices", {
  dims <- c(4,4,4)
  spc  <- NeuroSpace(dims)
  arr  <- array(runif(prod(dims)), dim=dims)
  vol  <- NeuroVol(arr, spc)

  tmp <- tempfile(fileext=".h5")
  on.exit(unlink(tmp))
  h5vol <- as_h5(vol, tmp, data_type="FLOAT")

  # Out-of-range
  expect_error(h5vol[5,1,1], regexp="Subscript")
  expect_error(h5vol[1:4,1:4,6], regexp="Subscript")
  # Negative index
  expect_error(h5vol[-1, , ], regexp="Subscript")

  # linear_access out of range
  expect_error(linear_access(h5vol, 0), regexp="out of range")
  expect_error(linear_access(h5vol, 200), regexp="out of range")
})

test_that("H5NeuroVec partial dimension subsetting and zero-size slices", {
  # 4D data: 3 x 3 x 2 x 3
  arr <- array(rnorm(3*3*2*3), dim=c(3,3,2,3))
  spc <- NeuroSpace(dim=c(3,3,2,3))
  vec <- NeuroVec(arr, spc)

  tmp <- tempfile(fileext=".h5")
  #on.exit(unlink(tmp))

  # Force no chunking to test unchunked usage
  h5vec <- to_nih5_vec(vec, file_name=tmp, chunk_dim=c(3,3,2,3), compression=0)
  expect_true(inherits(h5vec, "H5NeuroVec"))

  # Zero-slice in one dimension
  esub <- h5vec[ , , , integer(0)]
  expect_equal(dim(esub), c(3,3,2,0))

  # Single time slice
  sub1 <- h5vec[1:2, , 1, 1,drop=FALSE]
  expect_equal(dim(sub1), c(2,3,1,1))
  expect_equal(sub1, arr[1:2, , 1, 1,drop=FALSE], tolerance=1e-6)

  # i dimension only
  only_i <- h5vec[1, , , ,drop=FALSE]
  expect_equal(dim(only_i), c(1,3,2,3))

  # negative or zero index => error
  expect_error(h5vec[-1,1,1,1], "out of range")
  expect_error(h5vec[0,1,1,1],   "out of range")
})

test_that("H5NeuroVec linear access corner cases", {
  arr <- array(rnorm(2*2*2*2), dim=c(2,2,2,2))
  spc <- NeuroSpace(dim=c(2,2,2,2))
  vec <- NeuroVec(arr, spc)

  tmp <- tempfile(fileext=".h5")
  on.exit(unlink(tmp))

  h5vec <- to_nih5_vec(vec, file_name=tmp)

  # linear_access with all possible indices
  tot <- prod(dim(h5vec))
  lv  <- linear_access(h5vec, 1:tot)
  expect_equal(lv, as.vector(arr), tolerance=1e-7)

  # random sample
  idx_samp <- c(1, 2, tot-1, tot)
  lv_samp  <- linear_access(h5vec, idx_samp)
  expect_equal(lv_samp, arr[idx_samp], tolerance=1e-7)
})

test_that("LatentNeuroVec partial subsetting and out-of-mask voxels", {
  # Suppose a 2x2x2 volume with 3 timepoints => shape c(2,2,2,3)
  n_time <- 3
  n_basis <- 2
  # basis => [3 x 2]
  basis <- Matrix(matrix(rnorm(n_time * n_basis), nrow=n_time, ncol=n_basis))

  # mask => 2x2x2 but let's mask only half
  mask_arr <- array(c(TRUE, FALSE,
                      TRUE, FALSE,
                      FALSE, TRUE,
                      FALSE, TRUE), dim=c(2,2,2))
  mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(c(2,2,2)))
  # => sum(mask) = 4

  # loadings => [4 x 2]
  loadings <- matrix(rnorm(4*2), nrow=4, ncol=2)
  offset   <- rnorm(4)

  # Build LatentNeuroVec => space => 2,2,2,3
  spc <- NeuroSpace(dim=c(2,2,2,n_time))
  lat <- LatentNeuroVec(basis, loadings, spc, mask=mask_vol, offset=offset)

  # Subset => i=1:2, j=1:2, k=1:2 => entire space
  # We test that the "masked-out" voxels are zero
  full_4d <- lat[1:2, 1:2, 1:2, 1:3]
  # dimension => c(2,2,2,3)
  # For each time slice, out-of-mask is zero
  for (t in 1:3) {
    # Recompute the known partialVol = basis[t, ] %*% t(loadings) + offset
    slice <- as.numeric(basis[t,,drop=FALSE] %*% t(loadings) + offset)
    # Then reorder them into a 2x2x2 but masked voxels appear => rest 0
    arrt <- array(0, c(2,2,2))
    # mask row => which(mask_arr) => e.g. c(1,3,5,7) in linear indexing
    mv <- which(mask_arr)
    arrt[mv] <- slice
    # compare to full_4d[..,t]
    expect_equal(arrt, full_4d[,,,t], tolerance=1e-7)
  }

  idx_1d <- 1 + (2-1)*2 + (1-1)*2*2  # = 3
  s1 <- series(lat, 1, 2, 1)
  expect_length(s1, 3)

  rowid <- match(idx_1d, which(mask_arr))  # rowid in loadings; should now be 2
  manval <- numeric(3)
  for (t in seq_len(3)) {
    manval[t] <- sum(basis[t, ] * loadings[rowid, ]) + offset[rowid]
  }
  expect_equal(s1, manval, tolerance = 1e-8)


  # A voxel that is out of mask => e.g. i=2, j=1, k=1 => check mask
  idx_1d2 <- 2 + (1-1)*2 + (1-1)*2*2  # =2 => mask[2]=FALSE
  s2 <- series(lat, 2,1,1)
  expect_equal(s2, c(0,0,0))  # entire time series zero
})

test_that("LatentNeuroVec error handling", {
  # dimension mismatch in basis vs space
  expect_error({
    basis <- Matrix(rnorm(20), nrow=5)
    loadings <- Matrix(rnorm(50), nrow=10) # mismatch in #cols
    spc <- NeuroSpace(c(2,2,2,5))
    mask_arr <- array(TRUE, c(2,2,2))
    mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(c(2,2,2)))
    LatentNeuroVec(basis, loadings, spc, mask_vol)
  }, regexp="must have")

  # offset length mismatch
  expect_error({
    basis <- Matrix(rnorm(10), nrow=5, ncol=2)
    loadings <- Matrix(rnorm(2*4), nrow=4, ncol=2)
    spc <- NeuroSpace(c(2,2,1,5))
    mask_arr <- array(c(TRUE,TRUE,TRUE,TRUE), dim=c(2,2,1))
    mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(c(2,2,1)))
    LatentNeuroVec(basis, loadings, spc, mask_vol, offset=rnorm(10))
  }, "must match")
})

test_that("LabeledVolumeSet partial usage + memoise=TRUE", {
  # create a small 4D data: (4,4,2,3) => 3 volumes
  arr <- array(rnorm(4*4*2*3), dim=c(4,4,2,3))
  spc <- NeuroSpace(dim=c(4,4,2,3))
  vec <- NeuroVec(arr, spc)

  # mask => pick random subset
  mask_arr <- array(sample(c(TRUE,FALSE), 4*4*2, replace=TRUE), dim=c(4,4,2))
  mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(c(4,4,2)))

  labels <- c("volA", "volB", "volC")

  tmpfile <- tempfile(fileext=".h5")
  on.exit(unlink(tmpfile))

  # write as labeled
  write_labeled_vec(vec=vec, mask=mask_vol, labels=labels, file=tmpfile, compression=1)

  # read it back
  lvs <- read_labeled_vec(tmpfile)
  expect_s4_class(lvs, "LabeledVolumeSet")

  # check subsetting => [i,j,k,l]
  sub_4d <- lvs[1:2,2:3,1:2, 1:2]
  expect_equal(dim(sub_4d), c(2,2,2,2))
  # compare manually
  gold <- arr[1:2,2:3,1:2,1:2] * rep(mask_arr[1:2,2:3,1:2], 2)
  expect_equal(sub_4d, gold, tolerance=1e-7)

  # check linear_access
  lin_idx <- c(1,4,10,16,20,30)
  lv_vals <- linear_access(lvs, lin_idx)

  # construct "expected_full"
  # each vol => arr[,,,vol] * mask
  arr_masked <- arr
  for (v in seq_len(3)) {
    arr_masked[,,,v] <- arr_masked[,,,v] * mask_arr
  }
  gold_lin <- arr_masked[ lin_idx ]
  expect_equal(lv_vals, gold_lin, tolerance=1e-7)

  # check single volume => [[
  vC <- lvs[[3]]
  expect_s4_class(vC, "DenseNeuroVol")
  # verify it matches arr[,,,3]*mask
  expect_equal(as.array(vC@.Data), arr[,,,3]*mask_arr, tolerance=1e-7)
})
</file>

<file path="tests/testthat/test-h5classes.R">
library(neuroim2)

test_that("H5NeuroVol construction and subsetting works", {
  # Create test data
  dims <- c(10, 10, 10)
  space <- NeuroSpace(dims, spacing=c(2,2,2), origin=c(1,1,1))
  data <- array(rnorm(prod(dims)), dim=dims)
  vol <- NeuroVol(data, space)

  # Create temporary HDF5 file
  tmp <- tempfile(fileext=".h5")
  on.exit(unlink(tmp))

  # Convert to H5NeuroVol
  h5vol <- as_h5(vol, tmp)

  # Test basic properties
  expect_true(inherits(h5vol, "H5NeuroVol"))
  expect_equal(dim(h5vol), dims)
  expect_equal(spacing(space(h5vol)), spacing(space(vol)))
  expect_equal(origin(space(h5vol)), origin(space(vol)))

  # Test subsetting
  subset <- h5vol[1:5, 1:5, 1:5]
  expect_equal(dim(subset), c(5,5,5))
  expect_equal(as.array(subset), data[1:5, 1:5, 1:5], tolerance=1e-6)
})

test_that("H5NeuroVec construction and series extraction works", {
  # Create test data
  dims <- c(10, 10, 10)
  nvols <- 5
  space <- NeuroSpace(c(dims, nvols), spacing=c(2,2,2), origin=c(1,1,1))
  data <- array(rnorm(prod(dims) * nvols), dim=c(dims, nvols))
  vec <- NeuroVec(data, space)

  # Create temporary HDF5 file
  tmp <- tempfile(fileext=".h5")
  on.exit(unlink(tmp))

  # Convert to H5NeuroVec
  h5vec <- as_h5(vec, tmp)

  # Test basic properties
  expect_true(inherits(h5vec, "H5NeuroVec"))
  expect_equal(dim(h5vec), c(dims, nvols))
  expect_equal(spacing(space(h5vec)), spacing(space(vec)))
  expect_equal(origin(space(h5vec)), origin(space(vec)))
  expect_equal(trans(space(h5vec)), trans(space(vec)))

  # Test series extraction
  series1 <- series(h5vec, 5, 5, 5)
  expect_equal(length(series1), nvols)
  expect_equal(series1, data[5,5,5,], tolerance=1e-6)

  # Test subsetting
  subset <- h5vec[1:5, 1:5, 1:5, 1:3]
  expect_equal(dim(subset), c(5,5,5,3))
  expect_equal(as.array(subset), data[1:5, 1:5, 1:5, 1:3], tolerance=1e-6)
})

test_that("LatentNeuroVec single timepoint reconstruction", {
  n_basis       <- 5
  n_voxels      <- 1000
  n_timepoints  <- 100

  # basis => (n_timepoints x n_basis) => (100 x 5)
  basis <- Matrix(rnorm(n_timepoints * n_basis),
                  nrow = n_timepoints,
                  ncol = n_basis)

  # loadings => (p x k) => (1000 x 5)
  loadings <- Matrix(rnorm(n_voxels * n_basis),
                     nrow = n_voxels,
                     ncol = n_basis,
                     sparse = TRUE)

  offset <- rnorm(n_voxels)  # length=1000

  # Create a NeuroSpace with dims (10,10,10,100)
  space <- NeuroSpace(c(10,10,10,n_timepoints), spacing=c(2,2,2), origin=c(1,1,1))
  mask_vol <- LogicalNeuroVol(array(TRUE, dim=c(10,10,10)), drop_dim(space))

  latent_vec <- LatentNeuroVec(
    basis    = basis,      # (100 x 5)
    loadings = loadings,   # (1000 x 5)
    space    = space,
    mask     = mask_vol,
    offset   = offset
  )

  # Test basic properties
  expect_true(inherits(latent_vec, "LatentNeuroVec"))
  expect_equal(dim(latent_vec), c(10,10,10,n_timepoints))

  # Fix: Multiply as (1×n_basis)×(n_basis×n_voxels) => (1×n_voxels)
  # "time_point_1" is the full volume at time=1, i.e. basis[1, ] times loadings plus offset
  # Instead of basis[1,,drop=FALSE] %*% loadings:
  time_point_1 <- as.vector( basis[1,,drop=FALSE] %*% t(loadings) + offset )
  #time_point_1 <- as.vector(basis[1,,drop=FALSE] %*% loadings + offset)
  # Reconstruct time=1 from the latent vector
  reconstructed_1 <- as.vector(latent_vec[,,,1])

  # They should match
  expect_equal(time_point_1, reconstructed_1, tolerance=1e-8)

  # Test series extraction
  # e.g. single voxel's time series
  series1 <- series(latent_vec, 5, 5, 5)
  expect_equal(length(series1), n_timepoints)
})


test_that("H5NeuroVol indexing matches in-memory NeuroVol", {
  # 1) Create a random 3D volume in memory
  set.seed(42)
  arr <- array(rnorm(1000), dim = c(10, 10, 10))
  sp  <- NeuroSpace(dim=c(10, 10, 10))
  mem_vol <- NeuroVol(arr, sp)  # DenseNeuroVol

  # 2) Convert it to H5NeuroVol on the fly
  tmpfile <- tempfile(fileext = ".h5")
  h5vol   <- as_h5(mem_vol, file=tmpfile, data_type="FLOAT")

  # 3) Check basic dimension
  expect_equal(dim(h5vol), c(10,10,10))

  # 4) Test single voxel access
  expect_equal(h5vol[1,1,1], arr[1,1,1], tolerance=1e-7)
  expect_equal(h5vol[10,10,10], arr[10,10,10], tolerance=1e-7)

  # 5) Test slices
  slice_2 <- h5vol[1:5, 3:4, 2]
  expect_equal(slice_2, arr[1:5, 3:4, 2], tolerance=1e-6)

  # 6) Test linear indexing
  lin_idx <- c(1, 10, 100, 500, 999)
  h5_vals <- h5vol[lin_idx]
  mem_vals <- arr[lin_idx]
  expect_equal(h5_vals, mem_vals, tolerance=1e-6)

  # 7) Test matrix-based subsetting (arrayInd style)
  coords <- matrix(c(
    1,1,1,
    2,3,4,
    10,10,10
  ), ncol=3, byrow=TRUE)
  expect_equal(h5vol[coords], arr[coords], tolerance=1e-6)

  # 8) Random index checks
  i <- sample(1:10, 5)
  j <- sample(1:10, 4)
  k <- sample(1:10, 3)
  expect_equal(h5vol[i,j,k], arr[i,j,k], tolerance=1e-6)

  # 9) Check partial missing indices (like h5vol[i, , ])
  i2 <- 2:5
  sub_arr <- h5vol[i2,,]
  expect_equal(sub_arr, arr[i2,,], tolerance=1e-6)

  # 10) Cleanup
  hdf5r::h5close(h5vol@h5obj)
  file.remove(tmpfile)
})


test_that("H5NeuroVec indexing matches in-memory DenseNeuroVec", {

  ## 1) Construct a 4D in-memory DenseNeuroVec
  set.seed(42)
  # e.g. 10x10x5 volume with 8 timepoints => total dims c(10,10,5,8)
  arr <- array(rnorm(10*10*5*8), dim=c(10,10,5,8))
  sp  <- NeuroSpace(dim=c(10,10,5,8))
  mem_vec <- DenseNeuroVec(data=arr, space=sp)

  ## 2) Convert it to an H5NeuroVec file
  tmpfile <- tempfile(fileext=".h5")
  h5_vec <- as_h5(mem_vec, file=tmpfile, data_type="FLOAT",
                        chunk_dim=c(4,4,4,8),
                        compression=6)

  ## Basic checks
  expect_s4_class(h5_vec, "H5NeuroVec")
  expect_equal(dim(h5_vec), c(10,10,5,8))

  ## 3) Test straightforward subsetting
  # e.g. a small sub-block
  sub_arr <- mem_vec[1:5, 1:4, 2:3, 1:2]
  h5sub   <- h5_vec[1:5, 1:4, 2:3, 1:2]
  expect_equal(h5sub, sub_arr, tolerance=1e-7)

  # Another sub-block
  sub_arr2 <- mem_vec[2:3, 5:10, 1:5, 8]
  h5sub2   <- h5_vec[2:3, 5:10, 1:5, 8]
  expect_equal(h5sub2, sub_arr2, tolerance=1e-7)

  ## 4) Test partial indexing (missing one dimension => full range):
  # example: x[i, j, k] => implies full range in 4th dimension
  sub3d_mem <- mem_vec[2:3, 1:2, 4:5,]  # i.e. ignoring time => all volumes
  sub3d_h5  <- h5_vec[2:3, 1:2, 4:5,]
  expect_equal(sub3d_mem, sub3d_h5, tolerance=1e-7)

  ## 5) Test linear indexing
  # pick some scattered linear indices, compare
  lin_idx <- c(1, 50, 100, 234, 999, 1000, 2000)
  lin_mem <- mem_vec[ lin_idx ]
  lin_h5  <- h5_vec[ lin_idx ]
  expect_equal(lin_h5, lin_mem, tolerance=1e-7)

  ## 6) Test more random scattered subsetting across dimensions
  # We'll define random i, j, k, l
  i_scat <- sample(1:10, 4)
  j_scat <- sample(1:10, 5)
  k_scat <- sample(1:5,  3)
  l_scat <- sample(1:8,  2)

  scat_mem <- mem_vec[i_scat, j_scat, k_scat, l_scat]
  scat_h5  <- h5_vec[i_scat, j_scat, k_scat, l_scat]
  expect_equal(scat_h5, scat_mem, tolerance=1e-7)

  ## 7) Test random linear indexing for 4D
  lin_scat <- sample(prod(dim(mem_vec)), 20) # 20 random voxels
  lin_mem2 <- mem_vec[ lin_scat ]
  lin_h5_2 <- h5_vec[ lin_scat ]
  expect_equal(lin_h5_2, lin_mem2, tolerance=1e-7)

  ## Cleanup
  h5_vec@obj$close()
  file.remove(tmpfile)
})
</file>

<file path="R/all_class.R">
#' @import methods
#' @import Matrix
#' @importFrom hdf5r H5File
NULL

# S4 needs to know 'H5File' is an S3 class from hdf5r
setOldClass("H5File")

#' H5NeuroVol Class
#'
#' @description
#' A class representing a three-dimensional brain image backed by an HDF5 dataset.
#' \code{H5NeuroVol} objects provide efficient storage and access for large-scale
#' brain imaging data in the HDF5 format.
#'
#' @slot h5obj An instance of class \code{H5File} from the \pkg{hdf5r} package,
#'   representing the underlying HDF5 file containing the brain image data.
#'
#' @details
#' The \code{H5NeuroVol} class inherits a \code{space} slot from
#' \code{\link[neuroim2]{NeuroVol-class}}, which specifies the spatial domain
#' (dimensions, orientation, etc.). Data I/O is performed by reading/writing
#' subsets of the HDF5 dataset, allowing efficient handling of large 3D volumes.
#'
#' @section Inheritance:
#' \code{H5NeuroVol} inherits from:
#' \itemize{
#'   \item \code{\link[neuroim2]{NeuroVol-class}}: Base class for 3D brain volumes
#'   \item \code{\link[neuroim2]{ArrayLike3D-class}}: Interface for 3D array-like operations
#' }
#'
#' @seealso
#' \code{\link[neuroim2]{NeuroVol-class}} for the base 3D brain volume class.
#' \code{\link[hdf5r]{H5File}} for details on HDF5 file objects.
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5NeuroVol", where = "package:fmristore") && # Check if constructor is available
#'     !is.null(fmristore:::create_minimal_h5_for_H5NeuroVol)) { # Check helper
#'
#'   # Setup: Create a temporary HDF5 file using a helper
#'   # The helper creates a dataset named "data/elements" by default.
#'   temp_h5_path <- NULL
#'   h5_vol <- NULL
#'   tryCatch({
#'     temp_h5_path <- fmristore:::create_minimal_h5_for_H5NeuroVol(dims = c(5L, 5L, 3L))
#'     
#'     # Create an H5NeuroVol object using the constructor
#'     # The constructor defaults to dataset_name = "data/elements" if not specified
#'     # and if a NeuroSpace is not given, it reads it from /space in the HDF5 file.
#'     h5_vol <- fmristore::H5NeuroVol(file_name = temp_h5_path) 
#'     
#'     print(h5_vol)
#'     
#'     # Access a subset of the data
#'     subset_data <- h5_vol[1:2, 1:2, 1]
#'     print(subset_data)
#'     
#'   }, error = function(e) {
#'     message("H5NeuroVol example failed: ", e$message)
#'   }, finally = {
#'     # Close HDF5 handle owned by H5NeuroVol object
#'     if (!is.null(h5_vol) && inherits(h5_vol, "H5NeuroVol")) {
#'       try(close(h5_vol), silent = TRUE)
#'     }
#'     # Cleanup temporary file
#'     if (!is.null(temp_h5_path) && file.exists(temp_h5_path)) {
#'       unlink(temp_h5_path)
#'     }
#'   })
#' } else {
#'   message("Skipping H5NeuroVol example: fmristore, neuroim2, hdf5r, or helper not available.")
#' }
#'
#' @export
#' @rdname H5NeuroVol-class
#' @importClassesFrom neuroim2 ArrayLike3D
#' @importMethodsFrom neuroim2 [
#' @importMethodsFrom neuroim2 linear_access
setClass("H5NeuroVol",
         slots = c(
           h5obj = "H5File"  # underlying HDF5 file handle
         ),
         prototype = list(
           h5obj = NULL # Default to NULL, needs explicit file handle
         ),
         contains = c("NeuroVol", "ArrayLike3D"))

#' H5NeuroVec Class
#'
#' @description
#' A class representing a four-dimensional brain image backed by an HDF5 file.
#' \code{H5NeuroVec} objects provide efficient storage and access for large-scale
#' 4D neuroimaging data using the HDF5 file format.
#'
#' @slot obj An instance of class \code{H5File} from the \pkg{hdf5r} package,
#'   representing the underlying HDF5 file containing the 4D brain image data.
#'
#' @details
#' \code{H5NeuroVec} inherits a \code{space} slot from
#' \code{\link[neuroim2]{NeuroVec-class}}, defining the 4D dimensions
#' (e.g., \code{x}, \code{y}, \code{z}, \code{time}). Data are stored in an HDF5
#' dataset and accessed on demand.
#'
#' @section Inheritance:
#' \code{H5NeuroVec} inherits from:
#' \itemize{
#'   \item \code{\link[neuroim2]{NeuroVec-class}}: Base class for 4D brain images
#'   \item \code{\link[neuroim2]{ArrayLike4D-class}}: Interface for 4D array-like operations
#' }
#'
#' @seealso
#' \code{\link[neuroim2]{NeuroVec-class}} for the base 4D brain image class.
#' \code{\link{H5NeuroVol-class}} for the 3D counterpart.
#' \code{\link[hdf5r]{H5File}} for details on HDF5 file objects.
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5NeuroVec", where = "package:fmristore") && 
#'     !is.null(fmristore:::create_minimal_h5_for_H5NeuroVec)) { 
#'
#'   # Setup: Create a temporary HDF5 file using a helper
#'   temp_h5_path <- NULL
#'   h5_vec <- NULL
#'   tryCatch({
#'     temp_h5_path <- fmristore:::create_minimal_h5_for_H5NeuroVec(dims = c(5L, 5L, 3L, 4L))
#'     
#'     # Create an H5NeuroVec object using the constructor
#'     # Constructor defaults: dataset_name = "data/elements", space from HDF5 /space group
#'     h5_vec <- fmristore::H5NeuroVec(file_name = temp_h5_path) 
#'     
#'     print(h5_vec)
#'     
#'     # Access a subset of the data
#'     subset_data <- h5_vec[1:2, 1:2, 1, 1:2]
#'     print(subset_data)
#'     
#'   }, error = function(e) {
#'     message("H5NeuroVec example failed: ", e$message)
#'   }, finally = {
#'     # Close HDF5 handle owned by H5NeuroVec object
#'     if (!is.null(h5_vec) && inherits(h5_vec, "H5NeuroVec")) {
#'       try(close(h5_vec), silent = TRUE)
#'     }
#'     # Cleanup temporary file
#'     if (!is.null(temp_h5_path) && file.exists(temp_h5_path)) {
#'       unlink(temp_h5_path)
#'     }
#'   })
#' } else {
#'   message("Skipping H5NeuroVec example: fmristore, neuroim2, hdf5r, or helper not available.")
#' }
#'
#' @export
#' @rdname H5NeuroVec-class
setClass("H5NeuroVec",
         slots = c(
           obj = "H5File"  # underlying HDF5 file handle
         ),
         prototype = list(
           obj = NULL # Default to NULL
         ),
         contains = c("NeuroVec", "ArrayLike4D"))

#' H5Format Class
#'
#' @description
#' This class represents the HDF5 (Hierarchical Data Format version 5) file format.
#' It extends \code{FileFormat-class} with HDF5-specific attributes.
#'
#' @seealso \code{\link[neuroim2]{FileFormat-class}}
#'
#' @keywords internal
#' @noRd
setClass("H5Format",
         contains = c("FileFormat"))

#' H5NeuroVecSource Class
#'
#' @description
#' A class used internally to produce an \code{\linkS4class{H5NeuroVec}} from an HDF5 file.
#'
#' @slot file_name A \code{character} string specifying the name of the HDF5 file.
#'
#' @seealso \code{\link{H5NeuroVec-class}}
#'
#' @keywords internal
#' @noRd
setClass("H5NeuroVecSource",
         representation(file_name = "character"),
         prototype = list(
           file_name = character() # Default to empty character vector
         ))

#' LatentNeuroVec Class
#'
#' @description
#' A class that represents a 4-dimensional neuroimaging array using a latent space
#' decomposition. It stores the data as a set of basis functions (dictionary) and
#' a corresponding set of loadings (coefficients), enabling efficient representation
#' and manipulation of high-dimensional data.
#'
#' @slot basis A \code{Matrix} object where each column represents a basis vector
#'   in the latent space.
#' @slot loadings A \code{Matrix} object (often sparse) containing the coefficients
#'   for each basis vector across the spatial dimensions.
#' @slot offset A \code{numeric} vector representing a constant offset term for
#'   each voxel or spatial location.
#' @slot map A \code{IndexLookupVol} object representing the mapping from basis to loadings.
#' @slot label A \code{character} string representing the label for the latent vector.
#'
#' @details
#' \code{LatentNeuroVec} inherits from \code{\link[neuroim2]{NeuroVec-class}}
#' and \code{\link[neuroim2]{AbstractSparseNeuroVec-class}}. The original 4D data
#' can be reconstructed as:
#' \deqn{data[v,t] = \sum_k \bigl(basis[t,k] \times loadings[v,k]\bigr) + offset[v]}.
#' (Note: `v` indexes voxels within the mask).
#' 
#' **Important Naming Note:**
#' * In this R object: `@basis` stores temporal components (`nTime x k`), `@loadings` stores spatial components (`nVox x k`).
#' * In the HDF5 spec: `/scans/.../embedding` stores temporal (`nTime x k`), `/basis/basis_matrix` stores spatial (`k x nVox`).
#' The I/O functions handle the mapping and transposition.
#'
#' This approach is especially useful for large datasets where storing the full
#' 4D array is expensive.
#'
#' @section Inheritance:
#' \code{LatentNeuroVec} inherits from:
#' \itemize{
#'   \item \code{\link[neuroim2]{NeuroVec-class}}
#'   \item \code{\link[neuroim2]{AbstractSparseNeuroVec-class}}
#' }
#'
#' @seealso
#' \code{\link[neuroim2]{NeuroVec-class}},
#' \code{\link[neuroim2]{AbstractSparseNeuroVec-class}}.
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) && 
#'     requireNamespace("Matrix", quietly = TRUE) &&
#'     !is.null(fmristore:::create_minimal_LatentNeuroVec)) {
#'   
#'   # Create a LatentNeuroVec object using the helper
#'   # The helper creates a mask, basis, and loadings internally.
#'   # It uses new("LatentNeuroVec", ...) after creating constituent parts if not directly calling
#'   # a neuroim2::LatentNeuroVec constructor, or directly calls a constructor.
#'   # Our helper fmristore:::create_minimal_LatentNeuroVec returns a neuroim2::LatentNeuroVec.
#' 
#'   latent_vec <- NULL
#'   tryCatch({
#'     latent_vec <- fmristore:::create_minimal_LatentNeuroVec(
#'       space_dims = c(5L, 5L, 3L), 
#'       n_time = 8L, 
#'       n_comp = 2L
#'     )
#'     
#'     print(latent_vec)
#'     
#'     # Access slots (example)
#'     # print(dim(latent_vec@basis))
#'     # print(dim(latent_vec@loadings))
#'     
#'     # Example of accessing data (reconstruction for a voxel would be more complex)
#'     # This class is more about representation; direct element access is usually via methods.
#'     # For example, a method might be `series(latent_vec, vox_indices = c(1,2,3))`
#'     # For a simple demonstration, we can show its dimensions:
#'     print(dim(latent_vec)) # from NeuroVec inheritance
#'     
#'   }, error = function(e) {
#'     message("LatentNeuroVec example failed: ", e$message)
#'   })
#'   
#' } else {
#'   message("Skipping LatentNeuroVec example: neuroim2, Matrix, or helper not available.")
#' }
#'
#' @export
#' @rdname LatentNeuroVec-class
setClass("LatentNeuroVec",
         slots = c(
           basis = "Matrix",
           loadings = "Matrix",
           offset = "numeric",
           map = "IndexLookupVol",
           label = "character"
         ),
        
         contains = c("NeuroVec", "AbstractSparseNeuroVec"))

#' LatentNeuroVecSource Class
#'
#' @description
#' A class used intprotoernally to produce a \code{\linkS4class{LatentNeuroVec}} instance.
#'
#' @slot file_name A \code{character} string specifying the file name.
#'
#' @seealso \code{\link{LatentNeuroVec-class}}
#'
#' @keywords internal
#' @noRd
setClass("LatentNeuroVecSource",
         representation(file_name="character"),
         prototype = list(
           file_name = character() # Default to empty character vector
         ))


#' LabeledVolumeSet Class
#'
#' @description
#' A class representing a multi-volume dataset stored in HDF5, where each volume
#' is labeled (similar to a named 4th dimension). We store:
#' \itemize{
#'   \item a 3D mask
#'   \item a set of label strings
#'   \item for each label, data only at the mask's nonzero entries
#' }
#'
#' This extends \code{NeuroVec}, so it is logically a 4D object with dimension
#' \code{[X, Y, Z, #labels]}.
#'
#' @slot obj An \code{H5File} reference (the file handle).
#' @slot mask A \code{LogicalNeuroVol} of shape [X, Y, Z].
#' @slot labels A \code{character} vector for the volume labels.
#' @slot load_env An \code{environment} storing references for lazy loading.
#'
#' @seealso \code{\link[neuroim2]{NeuroVec-class}}
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("read_labeled_vec", where = "package:fmristore") && 
#'     !is.null(fmristore:::create_minimal_h5_for_LabeledVolumeSet)) {
#'
#'   # Setup: Create a temporary HDF5 file suitable for read_labeled_vec
#'   temp_h5_path <- NULL
#'   lvs <- NULL
#'   tryCatch({
#'     temp_h5_path <- fmristore:::create_minimal_h5_for_LabeledVolumeSet(
#'       vol_dims = c(5L, 4L, 3L),
#'       labels = c("CondA", "CondB"),
#'       num_vols_per_label = 2L
#'     )
#'     
#'     # Create a LabeledVolumeSet object using the constructor
#'     lvs <- fmristore::read_labeled_vec(file_name = temp_h5_path)
#'     
#'     print(lvs)
#'     print(labels(lvs)) # Access labels
#'     
#'     # Access data for a specific label (returns a NeuroVol or similar)
#'     # data_cond_a <- lvs[["CondA"]] 
#'     # print(dim(data_cond_a)) # Should be 3D x num_vols_per_label
#'     
#'   }, error = function(e) {
#'     message("LabeledVolumeSet example failed: ", e$message)
#'   }, finally = {
#'     # Close HDF5 handle owned by LabeledVolumeSet object
#'     if (!is.null(lvs) && inherits(lvs, "LabeledVolumeSet")) {
#'       try(close(lvs), silent = TRUE)
#'     }
#'     # Cleanup temporary file
#'     if (!is.null(temp_h5_path) && file.exists(temp_h5_path)) {
#'       unlink(temp_h5_path)
#'     }
#'   })
#' } else {
#'   message("Skipping LabeledVolumeSet example: fmristore, neuroim2, hdf5r, or helper not available.")
#' }
#'
#' @export
setClass("LabeledVolumeSet",
         slots = c(
           obj      = "H5File",          # pointer to open file
           mask     = "LogicalNeuroVol", # 3D mask
           labels   = "character",       # volume (sub-volume) names
           load_env = "environment"     # environment for lazy loading
         ),
         contains = c("NeuroVec"))  # extends NeuroVec

#' H5ClusteredArray (Virtual Base Class)
#'
#' @description
#' A **virtual** base class for representing clustered neuroimaging data stored in HDF5.
#' It holds the common elements shared across different representations (e.g., full voxel data, summary data).
#' This class is not intended to be instantiated directly.
#'
#' @slot obj An \code{H5File} object representing the open HDF5 file.
#' @slot mask A \code{LogicalNeuroVol} defining the brain mask (shared across runs).
#' @slot clusters A \code{ClusteredNeuroVol} containing cluster assignments (shared across runs).
#' @slot n_voxels An \code{integer} caching the number of voxels in the mask (sum(mask)).
#'
#' @keywords internal
#' @importFrom hdf5r H5File
#' @importFrom methods validObject
#' @importClassesFrom neuroim2 LogicalNeuroVol ClusteredNeuroVol
#' @family H5Cluster
#'
#' @examples
#' # H5ClusteredArray is a virtual class and cannot be directly instantiated.
#' # See its subclasses H5ClusterRun and H5ClusterRunSummary for examples.
#' 
#' # You can check if an object inherits from it:
#' # run_object <- H5ClusterRun(...) # Assuming run_object is created
#' # inherits(run_object, "H5ClusteredArray") # Should return TRUE
#'
#' @export
setClass("H5ClusteredArray",
         slots = c(
           obj      = "H5File",
           mask     = "LogicalNeuroVol",
           clusters = "ClusteredNeuroVol",
           n_voxels = "integer"
         ),
         prototype = list(
            obj      = NULL,
            mask     = new("LogicalNeuroVol"),
            clusters = new("ClusteredNeuroVol"),
            n_voxels = NA_integer_
         ),
         validity = function(object) {
            errors <- character()
            # Check 1: n_voxels should match sum(mask), if mask is valid
            mask_space_valid <- !is.null(object@mask) && 
                                 is(object@mask, "LogicalNeuroVol") && 
                                 validObject(object@mask@space, test=TRUE)
                                
            if (mask_space_valid) {
                expected_nvox <- sum(object@mask)
                if (!is.na(object@n_voxels) && object@n_voxels != expected_nvox) {
                   errors <- c(errors, 
                               sprintf("Slot 'n_voxels' (%d) does not match sum(mask) (%d).", 
                                       object@n_voxels, expected_nvox))
                }
                # Check 2: length of cluster vector should match n_voxels (which should match sum(mask))
                if (!is.null(object@clusters) && is(object@clusters, "ClusteredNeuroVol") && length(object@clusters@clusters) > 0) {
                   if (length(object@clusters@clusters) != expected_nvox) {
                      errors <- c(errors,
                                  sprintf("Length of clusters@clusters (%d) does not match sum(mask) (%d).",
                                          length(object@clusters@clusters), expected_nvox))
                   }
                }
            }
            # Check 3: Basic type check for H5File handle
            if (!is.null(object@obj) && !inherits(object@obj, "H5File")) {
                errors <- c(errors, "Slot 'obj' must be an H5File object or NULL.")
            }
            
            if (length(errors) == 0) TRUE else errors
         },
         contains = "VIRTUAL")

#' H5ClusterRun Class
#'
#' @description
#' Represents a single "run" or "scan" of full voxel-level clustered time-series data
#' stored in an HDF5 file. It inherits common properties (mask, clusters, file handle)
#' from `H5ClusteredArray` and adds run-specific information.
#'
#' @slot scan_name A \code{character} string identifying the scan (e.g., corresponds to a group under `/scans/`).
#' @slot n_time An \code{integer} specifying the number of time points in this run.
#' @slot compress A \code{logical} indicating if compression was intended or used (metadata).
#' Inherits slots `obj`, `mask`, `clusters`, `n_voxels` from `H5ClusteredArray`.
#'
#' @seealso \code{\link{H5ClusteredArray-class}}, \code{\link{H5ClusterRunSummary-class}}
#' @family H5Cluster
#' @export
setClass("H5ClusterRun",
         slots = c(
             scan_name = "character",
             n_time    = "integer",
             compress  = "logical" # Primarily for metadata/write path
         ),
         prototype = list(
            scan_name = character(),
            n_time = NA_integer_,
            compress = FALSE
            # Inherits prototype for obj, mask, clusters, n_voxels
         ),
         contains = "H5ClusteredArray")

#' H5ClusterRunSummary Class
#'
#' @description
#' Represents a single "run" or "scan" containing only *summary* time-series data
#' for clusters (e.g., mean signal per cluster) stored in an HDF5 file.
#' It inherits common properties from `H5ClusteredArray` but provides methods suited
#' for accessing summary data (like `as.matrix`) rather than full voxel data.
#'
#' @slot scan_name A \code{character} string identifying the scan.
#' @slot n_time An \code{integer} specifying the number of time points in this run.
#' @slot cluster_names A \code{character} vector providing names for the clusters (columns in summary matrix).
#' @slot cluster_ids An \code{integer} vector of cluster IDs corresponding to `cluster_names`.
#' @slot summary_dset A \code{character} string giving the name of the summary dataset within the run's HDF5 group (e.g., "summary_data").
#' Inherits slots `obj`, `mask`, `n_voxels` from `H5ClusteredArray`.
#' Note: The `clusters` slot inherited from `H5ClusteredArray` might be NULL or contain the map for reference, but voxel-level access methods are typically disabled/error.
#'
#' @seealso \code{\link{H5ClusteredArray-class}}, \code{\link{H5ClusterRun-class}}
#' @family H5Cluster
#' @export
setClass("H5ClusterRunSummary",
         slots = c(
            scan_name     = "character",
            n_time        = "integer",
            cluster_names = "character",
            cluster_ids   = "integer",
            summary_dset  = "character"
         ),
         prototype = list(
            scan_name     = character(),
            n_time        = NA_integer_,
            cluster_names = character(),
            cluster_ids   = integer(),
            summary_dset  = "summary_data"
         ),
         contains = "H5ClusteredArray")

#' H5ClusterExperiment Class
#'
#' @description
#' Represents a collection of clustered neuroimaging runs (scans) stored within a single HDF5 file.
#' It acts as a container for `H5ClusterRun` and/or `H5ClusterRunSummary` objects,
#' along with associated metadata.
#'
#' This class facilitates managing multiple runs that share the same mask and cluster definitions.
#'
#' @slot runs A `list` where each element is an object inheriting from `H5ClusteredArray`
#'   (typically `H5ClusterRun` or `H5ClusterRunSummary`).
#' @slot scan_metadata A `list` containing metadata for each scan in the `runs` list.
#' @slot cluster_metadata A `data.frame` containing metadata associated with the clusters.
#'
#' @seealso \code{\link{H5ClusterRun-class}}, \code{\link{H5ClusterRunSummary-class}}
#' @family H5Cluster
#' @export
setClass("H5ClusterExperiment",
         slots = c(
             runs             = "list",
             scan_metadata    = "list",
             cluster_metadata = "data.frame"
         ),
         prototype = list(
             runs             = list(),
             scan_metadata    = list(),
             cluster_metadata = data.frame()
         ),
         validity = function(object) {
            errors <- character()
            n_runs <- length(object@runs)

            # Check 1: All elements in 'runs' list must inherit from H5ClusteredArray
            if (n_runs > 0) {
                is_valid_run <- vapply(object@runs, inherits, logical(1),
                                       what = "H5ClusteredArray")
                if (!all(is_valid_run)) {
                    invalid_indices <- which(!is_valid_run)
                    errors <- c(errors,
                                paste0("Elements at indices ",
                                       paste(invalid_indices, collapse=", "),
                                       " in the 'runs' list do not inherit from H5ClusteredArray."))
                    # Stop further checks if basic type is wrong
                    return(errors)
                }
            }

            # Check 2: Ensure scan_metadata has same length as runs if not empty
            if (length(object@scan_metadata) > 0 && length(object@scan_metadata) != n_runs) {
                 errors <- c(errors,
                           sprintf("Length of 'scan_metadata' (%d) does not match length of 'runs' list (%d).",
                                   length(object@scan_metadata), n_runs))
            }

            # Check 3: If multiple runs, verify they share the same H5File, mask, and clusters
            if (n_runs > 1) {
                first_run <- object@runs[[1]]
                # Check H5 File (using filename as a robust check)
                first_filename <- tryCatch(first_run@obj$get_filename(), error=function(e) NA_character_)
                if(is.na(first_filename)) {
                    errors <- c(errors, "Could not get HDF5 filename from the first run object.")
                }

                for (i in 2:n_runs) {
                    current_run <- object@runs[[i]]
                    # Check H5 File consistency
                    current_filename <- tryCatch(current_run@obj$get_filename(), error=function(e) NA_character_)
                    if (is.na(current_filename) || !identical(first_filename, current_filename)) {
                        errors <- c(errors,
                                   sprintf("Run %d uses a different HDF5 file ('%s') than Run 1 ('%s').",
                                           i, current_filename, first_filename))
                    }
                    # Check mask consistency (using identical to check for same object in memory)
                    if (!identical(first_run@mask, current_run@mask)) {
                        errors <- c(errors, sprintf("Run %d has a different mask object than Run 1.", i))
                    }
                    # Check clusters consistency (using identical)
                    # Need to handle NULL case for H5ClusterRunSummary
                    if (!identical(first_run@clusters, current_run@clusters)) {
                         # Allow comparison if both are NULL (summary runs might have NULL clusters)
                         if (!(is.null(first_run@clusters) && is.null(current_run@clusters))) {
                            errors <- c(errors, sprintf("Run %d has a different clusters object than Run 1.", i))
                         }
                    }
                }
            }

            if (length(errors) == 0) TRUE else errors
         }
         # Does not contain H5ClusteredArray
)

#' @seealso \code{\link{H5ClusterRun-class}}, \code{\link{H5ClusterRunSummary-class}}
#' @family H5Cluster
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5ClusterExperiment", where = "package:fmristore") &&
#'     !is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'
#'   temp_exp_file <- NULL
#'   exp <- NULL
#'   tryCatch({
#'     # 1. Create an HDF5 file for an experiment containing runs
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment()
#'     
#'     # 2. Load the experiment using the constructor
#'     exp <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # 3. Show the experiment object
#'     print(exp)
#'     print(names(runs(exp))) # Show the names of the runs it loaded
#'     
#'   }, error = function(e) {
#'     message("H5ClusterExperiment example failed: ", e$message)
#'   }, finally = {
#'     # Close H5ClusterExperiment handle (closes underlying file)
#'     if (!is.null(exp)) try(close(exp), silent = TRUE)
#'     # Cleanup temporary file
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping H5ClusterExperiment example: dependencies or helper not available.")
#' }
#'
#' @export
setClass("H5ClusterExperiment",
         slots = c(
             runs             = "list",
             scan_metadata    = "list",
             cluster_metadata = "data.frame"
         ),
         prototype = list(
             runs             = list(),
             scan_metadata    = list(),
             cluster_metadata = data.frame()
         ),
         validity = function(object) {
            errors <- character()
            n_runs <- length(object@runs)

            # Check 1: All elements in 'runs' list must inherit from H5ClusteredArray
            if (n_runs > 0) {
                is_valid_run <- vapply(object@runs, inherits, logical(1),
                                       what = "H5ClusteredArray")
                if (!all(is_valid_run)) {
                    invalid_indices <- which(!is_valid_run)
                    errors <- c(errors,
                                paste0("Elements at indices ",
                                       paste(invalid_indices, collapse=", "),
                                       " in the 'runs' list do not inherit from H5ClusteredArray."))
                    # Stop further checks if basic type is wrong
                    return(errors)
                }
            }

            # Check 2: Ensure scan_metadata has same length as runs if not empty
            if (length(object@scan_metadata) > 0 && length(object@scan_metadata) != n_runs) {
                 errors <- c(errors,
                           sprintf("Length of 'scan_metadata' (%d) does not match length of 'runs' list (%d).",
                                   length(object@scan_metadata), n_runs))
            }

            # Check 3: If multiple runs, verify they share the same H5File, mask, and clusters
            if (n_runs > 1) {
                first_run <- object@runs[[1]]
                # Check H5 File (using filename as a robust check)
                first_filename <- tryCatch(first_run@obj$get_filename(), error=function(e) NA_character_)
                if(is.na(first_filename)) {
                    errors <- c(errors, "Could not get HDF5 filename from the first run object.")
                }

                for (i in 2:n_runs) {
                    current_run <- object@runs[[i]]
                    # Check H5 File consistency
                    current_filename <- tryCatch(current_run@obj$get_filename(), error=function(e) NA_character_)
                    if (is.na(current_filename) || !identical(first_filename, current_filename)) {
                        errors <- c(errors,
                                   sprintf("Run %d uses a different HDF5 file ('%s') than Run 1 ('%s').",
                                           i, current_filename, first_filename))
                    }
                    # Check mask consistency (using identical to check for same object in memory)
                    if (!identical(first_run@mask, current_run@mask)) {
                        errors <- c(errors, sprintf("Run %d has a different mask object than Run 1.", i))
                    }
                    # Check clusters consistency (using identical)
                    # Need to handle NULL case for H5ClusterRunSummary
                    if (!identical(first_run@clusters, current_run@clusters)) {
                         # Allow comparison if both are NULL (summary runs might have NULL clusters)
                         if (!(is.null(first_run@clusters) && is.null(current_run@clusters))) {
                            errors <- c(errors, sprintf("Run %d has a different clusters object than Run 1.", i))
                         }
                    }
                }
            }

            if (length(errors) == 0) TRUE else errors
         }
         # Does not contain H5ClusteredArray
)
</file>

<file path="tests/testthat/test_latent_vec.R">
library(testthat)
library(hdf5r)
library(neuroim2)
library(Matrix)
library(fmristore)

# Helper function to create dummy data for LatentNeuroVec testing
create_dummy_latent_data <- function(dims = c(5, 5, 3, 10), # x,y,z,t
                                       k = 4) { # Number of components
  
  sp <- NeuroSpace(dims)
  nTime <- dims[4]
  nVox_total <- prod(dims[1:3])
  
  # Create a simple mask (e.g., roughly half the voxels)
  mask_arr <- array(FALSE, dim = dims[1:3])
  mask_arr[1:floor(dims[1]/2), , ] <- TRUE
  mask_vol <- LogicalNeuroVol(mask_arr, drop_dim(sp))
  nVox_mask <- sum(mask_vol)
  
  # Create basis (nTime x k)
  basis_mat <- Matrix(rnorm(nTime * k), nrow = nTime, ncol = k)
  
  # Create loadings (nVox_mask x k)
  loadings_mat <- Matrix(rnorm(nVox_mask * k), nrow = nVox_mask, ncol = k, sparse=TRUE)
  # Ensure sparsity
  loadings_mat[sample(length(loadings_mat), length(loadings_mat)*0.7)] <- 0
  
  # Create offset (nVox_mask)
  offset_vec <- rnorm(nVox_mask, mean=5, sd=1)
  
  # Create the LatentNeuroVec object
  lvec <- LatentNeuroVec(basis = basis_mat, 
                         loadings = loadings_mat, 
                         space = sp, 
                         mask = mask_vol, 
                         offset = offset_vec,
                         label = "test_scan_label")
                         
  return(lvec)
}

# Helper function to create basic LatentNeuroVec components
create_test_latent_components <- function(X=10, Y=10, Z=5, T=20, K=5) {
  dims_4d <- c(X, Y, Z, T)
  dims_3d <- dims_4d[1:3]
  sp <- NeuroSpace(dims_4d)
  
  mask_arr <- array(FALSE, dims_3d)
  mask_arr[2:(X-1), 2:(Y-1), 2:(Z-1)] <- TRUE # Create a smaller inner mask
  mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(dims_3d))
  nVox <- sum(mask_vol)
  
  basis <- Matrix(rnorm(T * K), nrow=T, ncol=K)
  # Create loadings that match mask cardinality
  loadings <- Matrix(rnorm(nVox * K), nrow=nVox, ncol=K)
  offset <- rnorm(nVox)
  offset_empty <- numeric(0)
  
  list(
    space=sp, 
    mask=mask_vol, 
    basis=basis, 
    loadings=loadings, 
    offset=offset, 
    offset_empty=offset_empty,
    nVox=nVox,
    K=K,
    T=T
  )
}


test_that("LatentNeuroVec HDF5 round-trip works and validates", {
  
  # 1. Create original object
  lvec_orig <- create_dummy_latent_data()
  
  # 2. Define temp file path
  temp_h5 <- tempfile(fileext = ".lv.h5")
  on.exit(unlink(temp_h5), add = TRUE)
  
  # 3. Write the object - capture warnings to debug
  warnings <- capture_warnings(write_vec(lvec_orig, temp_h5))
  if (length(warnings) > 0) {
    print(paste("Warnings during write:", paste(warnings, collapse="\n")))
  }
  expect_true(file.exists(temp_h5), info = "HDF5 file should exist after write_vec")
  
  # 4. Validate the written file
  validation_result <- FALSE
  expect_silent(validation_result <- validate_latent_file(temp_h5))
  expect_true(validation_result, info = "validate_latent_file should return TRUE for a correctly written file.")
  
  # 5. Read the object back
  lvec_source <- LatentNeuroVecSource(temp_h5)
  lvec_loaded <- NULL
  # Load using the specific scan name we expect the writer to use
  expected_scan_name <- lvec_orig@label 
  expect_no_error(lvec_loaded <- load_data(lvec_source, scan_name = expected_scan_name))
  expect_s4_class(lvec_loaded, "LatentNeuroVec")
  
  # 6. Compare original and loaded objects
  # Using tolerance due to potential float precision differences in HDF5/Matrix
  tolerance <- 1e-7 
  
  # Compare slots - use all.equal for matrices/numerics with tolerance
  expect_true(all.equal(as.matrix(lvec_loaded@basis), as.matrix(lvec_orig@basis), tolerance = tolerance),
              info = "Basis matrices do not match.")
  expect_true(all.equal(as.matrix(lvec_loaded@loadings), as.matrix(lvec_orig@loadings), tolerance = tolerance),
              info = "Loadings matrices do not match.")
  expect_equal(lvec_loaded@offset, lvec_orig@offset, tolerance = tolerance, 
               info = "Offset vectors do not match.")
               
  # Compare NeuroSpace (might need tolerance or specific attribute checks)
  expect_equal(space(lvec_loaded), space(lvec_orig), 
               info = "NeuroSpace objects do not match.")
               
  # Compare Mask (should be exact for logical)
  expect_equal(lvec_loaded@mask, lvec_orig@mask, 
               info = "Mask volumes do not match.")
  
  # Compare Map indices
  expect_equal(lvec_loaded@map@indices, lvec_orig@map@indices, 
               info = "Map indices do not match.")
               
  # Compare label
  expect_equal(lvec_loaded@label, lvec_orig@label, 
               info = "Labels do not match.")

  # Optional: Test data reconstruction equivalence
  # This is a stricter test
  # expect_equal(as.array(lvec_loaded), as.array(lvec_orig), tolerance=tolerance,
  #             info = "Reconstructed 4D arrays do not match.")

})

# Add test for dense round-trip
test_that("LatentNeuroVec HDF5 dense round-trip works and validates", {
  
  # 1. Create original object with dense loadings
  comps_dense <- create_test_latent_components(X=6, Y=6, Z=4, T=8, K=3) # Smaller dims for speed
  # Ensure loadings are dense matrix, not sparse Matrix
  lvec_orig_dense <- LatentNeuroVec(basis = comps_dense$basis, 
                                    loadings = as.matrix(comps_dense$loadings), # Force dense matrix
                                    space = comps_dense$space, 
                                    mask = comps_dense$mask, 
                                    offset = comps_dense$offset,
                                    label = "dense_test_label")
                                    
  # Calculate density to ensure the writer *should* choose dense
   density_check <- Matrix::nnzero(lvec_orig_dense@loadings) / length(lvec_orig_dense@loadings)
   # Ensure density is >= 30% for this test case
   expect_gte(density_check, 0.3)
  
  # 2. Define temp file path
  temp_h5_dense <- tempfile(fileext = ".dense.lv.h5")
  on.exit(unlink(temp_h5_dense), add = TRUE)
  
  # 3. Write the object
  warnings_dense <- capture_warnings(write_vec(lvec_orig_dense, temp_h5_dense))
  if (length(warnings_dense) > 0) {
    print(paste("Warnings during DENSE write:", paste(warnings_dense, collapse="\n"))) # Distinguish warning source
  }
  expect_true(file.exists(temp_h5_dense), info = "HDF5 file (dense) should exist after write_vec")
  
  # 4. INSPECT HDF5 structure: Verify DENSE basis was written
  h5_inspect <- NULL
  tryCatch({
      h5_inspect <- hdf5r::H5File$new(temp_h5_dense, mode = "r")
      expect_true(h5_inspect$exists("basis/basis_matrix"), 
                  info = "Dense basis dataset '/basis/basis_matrix' should exist.")
      expect_false(h5_inspect$exists("basis/basis_matrix_sparse"), 
                   info = "Sparse basis group '/basis/basis_matrix_sparse' should NOT exist.")
  }, finally = {
      if (!is.null(h5_inspect) && h5_inspect$is_valid) h5_inspect$close_all()
  })

  # 5. Validate the written file
  validation_result_dense <- FALSE
  expect_silent(validation_result_dense <- validate_latent_file(temp_h5_dense))
  expect_true(validation_result_dense, info = "validate_latent_file should return TRUE for a DENSELY written file.")
  
  # 6. Read the object back
  lvec_source_dense <- LatentNeuroVecSource(temp_h5_dense)
  lvec_loaded_dense <- NULL
  expected_scan_name_dense <- lvec_orig_dense@label 
  expect_no_error(lvec_loaded_dense <- load_data(lvec_source_dense, scan_name = expected_scan_name_dense))
  expect_s4_class(lvec_loaded_dense, "LatentNeuroVec")
  
  # 7. Compare original and loaded objects
  tolerance <- 1e-7 
  expect_true(all.equal(as.matrix(lvec_loaded_dense@basis), as.matrix(lvec_orig_dense@basis), tolerance = tolerance),
              info = "Basis matrices do not match (dense test).")
  # Comparison for loadings might be tricky if sparse comes back - force both to dense
  expect_true(all.equal(as.matrix(lvec_loaded_dense@loadings), as.matrix(lvec_orig_dense@loadings), tolerance = tolerance),
              info = "Loadings matrices do not match (dense test).")
  expect_equal(lvec_loaded_dense@offset, lvec_orig_dense@offset, tolerance = tolerance, 
               info = "Offset vectors do not match (dense test).")
  expect_equal(space(lvec_loaded_dense), space(lvec_orig_dense), 
               info = "NeuroSpace objects do not match (dense test).")
  expect_equal(lvec_loaded_dense@mask, lvec_orig_dense@mask, 
               info = "Mask volumes do not match (dense test).")
  expect_equal(lvec_loaded_dense@map@indices, lvec_orig_dense@map@indices, 
               info = "Map indices do not match (dense test).")
  expect_equal(lvec_loaded_dense@label, lvec_orig_dense@label, 
               info = "Labels do not match (dense test).")

})

# TODO: Add tests for validator function (checking expected failures)
# TODO: Add tests for error conditions in load_data (e.g., wrong scan_name)
# TODO: Add tests for edge cases (e.g., k=1 component)

# --- Constructor Tests --- 

test_that("LatentNeuroVec constructor validates dimensions correctly", {
  comps <- create_test_latent_components()
  
  # Valid construction
  expect_no_error(
    LatentNeuroVec(basis=comps$basis, loadings=comps$loadings, 
                   space=comps$space, mask=comps$mask, offset=comps$offset)
  )
  expect_no_error(
    LatentNeuroVec(basis=comps$basis, loadings=comps$loadings, 
                   space=comps$space, mask=comps$mask, offset=comps$offset_empty) # Test empty offset
  )
  expect_no_error(
    LatentNeuroVec(basis=comps$basis, loadings=comps$loadings, 
                   space=comps$space, mask=comps$mask, offset=NULL) # Test NULL offset
  )
  
  # Invalid: Basis time mismatch
  basis_bad_time <- Matrix(rnorm((comps$T + 1) * comps$K), nrow=comps$T + 1, ncol=comps$K)
  expect_error(LatentNeuroVec(basis=basis_bad_time, loadings=comps$loadings, 
                              space=comps$space, mask=comps$mask), 
               regexp="'basis' must have \\d+ rows \\(the 4th dimension of space\\)")
               
  # Invalid: Loadings voxels mismatch (mask cardinality)
  loadings_bad_vox <- Matrix(rnorm((comps$nVox + 1) * comps$K), nrow=comps$nVox + 1, ncol=comps$K)
  expect_error(LatentNeuroVec(basis=comps$basis, loadings=loadings_bad_vox, 
                              space=comps$space, mask=comps$mask), 
               regexp="'loadings' must have \\d+ rows \\(i\\.e\\. #non-zero in mask\\)")
               
  # Invalid: Component (K) mismatch
  basis_bad_k <- Matrix(rnorm(comps$T * (comps$K + 1)), nrow=comps$T, ncol=comps$K + 1)
  expect_error(LatentNeuroVec(basis=basis_bad_k, loadings=comps$loadings, 
                              space=comps$space, mask=comps$mask), 
               regexp="must have the same number of columns")
               
  # Invalid: Offset length mismatch
  offset_bad_len <- rnorm(comps$nVox + 1)
  expect_error(LatentNeuroVec(basis=comps$basis, loadings=comps$loadings,
                              space=comps$space, mask=comps$mask, offset=offset_bad_len),
               regexp="'offset' length must match number of rows in 'loadings'")

  # Invalid: Non-finite values
  basis_bad_na <- comps$basis
  basis_bad_na[1,1] <- NA_real_
  expect_error(LatentNeuroVec(basis=basis_bad_na, loadings=comps$loadings,
                              space=comps$space, mask=comps$mask, offset=comps$offset),
               regexp="basis.*finite")

  loadings_bad_inf <- comps$loadings
  loadings_bad_inf[1,1] <- Inf
  expect_error(LatentNeuroVec(basis=comps$basis, loadings=loadings_bad_inf,
                              space=comps$space, mask=comps$mask, offset=comps$offset),
               regexp="loadings.*finite")

  offset_bad_na2 <- comps$offset
  offset_bad_na2[1] <- NA_real_
  expect_error(LatentNeuroVec(basis=comps$basis, loadings=comps$loadings,
                              space=comps$space, mask=comps$mask, offset=offset_bad_na2),
               regexp="offset.*finite")
               
  # Invalid: Mask space mismatch
  # TODO LogicalNeuroVol constructor is not working as expected
  # It silently corrects the dimensions of the mask to match the space
  #mask_bad_space <- LogicalNeuroVol(as.logical(comps$mask), NeuroSpace(dim(comps$mask)+1))
  #expect_error(LatentNeuroVec(basis=comps$basis, loadings=comps$loadings, 
  #                            space=comps$space, mask=mask_bad_space), 
  #             regexp="Space of provided mask does not match")
               
})

test_that("LatentNeuroVec constructor handles sparse matrix coercion with warnings", {
  comps <- create_test_latent_components(T=10, K=2) # Smaller example
  
  # Dense basis matrix (low density - no warning expected)
  basis_dense_low <- matrix(rnorm(10 * 2), nrow=10, ncol=2)
  basis_dense_low[sample(20, 15)] <- 0 # Make it sparse-ish
  
  # Dense loadings matrix (high density - warning expected)
  loadings_dense_high <- matrix(rnorm(comps$nVox * 2), nrow=comps$nVox, ncol=2)
  
  expect_warning(
    LatentNeuroVec(basis=basis_dense_low, loadings=loadings_dense_high, 
                   space=comps$space, mask=comps$mask),
    regexp = "Coercing dense 'loadings' matrix.*density is high"
  )
  
  # Check no warning for basis (low density)
  # Need to capture warnings specifically
  warnings_basis <- capture_warnings(
      LatentNeuroVec(basis=basis_dense_low, loadings=comps$loadings, # Use sparse loadings here
                     space=comps$space, mask=comps$mask)
  )
  expect_length(warnings_basis, 0)

})



make_small_lvec <- function() {
  # 4 x 4 x 3 volume, 5 time points, 2 components
  dims <- c(4, 4, 3, 5)
  sp   <- NeuroSpace(dims)
  mask <- array(FALSE, dim=dims[1:3])
  mask[2:3, 2:4, ] <- TRUE                 # interior mask (18 voxels)
  mask_vol <- LogicalNeuroVol(mask, drop_dim(sp))
  
  k  <- 2
  nt <- dims[4]
  nv <- sum(mask)
  
  basis    <- Matrix(matrix(seq_len(nt*k), nt, k))        # deterministic numbers
  loadings <- Matrix(matrix(seq_len(nv*k), nv, k), sparse=TRUE)
  offset   <- seq_len(nv)
  
  LatentNeuroVec(basis, loadings, sp, mask_vol, offset)
}

# --- 1. subsetting [] ------------------------------------------------
test_that("[] returns correct reconstructed block", {
  lvec <- make_small_lvec()
  
  # target block: voxels (x=2..3, y=2, z=1) over time 2 & 4  -> dims 2 x 1 x 1 x 2
  cut <- lvec[2:3, 2, 1, c(2,4), drop = FALSE]
  expect_equal(dim(cut), c(2,1,1,2))
  
  ## ground-truth reconstruction (no offset simplification)
  B  <- as.matrix(basis(lvec))[c(2,4), ]            # 2 x k
  # Get rows 1 and 2 from loadings, corresponding to 3D indices 6 and 7
  L  <- as.matrix(loadings(lvec))[ c(1, 2), ]      
  off<- offset(lvec)[ c(1, 2) ]
  # Calculate Basis * t(Loadings) and add offset per voxel (column-wise)
  expected_raw <- sweep(tcrossprod(B, L), 2, off, "+") # Result is 2x2 (time x voxel)
  # Manually construct the target array [x=2, y=1, z=1, t=2]
  expected <- array(0, dim = c(2,1,1,2)) 
  expected[1,1,1,1] <- expected_raw[1,1] # x=2, t=2 <- time=2, voxel=1
  expected[2,1,1,1] <- expected_raw[1,2] # x=3, t=2 <- time=2, voxel=2
  expected[1,1,1,2] <- expected_raw[2,1] # x=2, t=4 <- time=4, voxel=1
  expected[2,1,1,2] <- expected_raw[2,2] # x=3, t=4 <- time=4, voxel=2
  
  expect_equal(cut, expected, tolerance = 1e-12)
})

# --- 2. series(), linear & outside-mask ---------------------------------
test_that("series() matches manual calculation and zeros outside mask", {
  lvec <- make_small_lvec()
  nt   <- dim(lvec)[4]
  
  # voxel (x=2,y=2,z=1) is inside the mask; linear index = 2 + (2-1)*4 + (1-1)*4*4 = 6
  ts1 <- series(lvec, 6L)
  B   <- as.matrix(basis(lvec))          # nt x k
  L   <- as.numeric(loadings(lvec)[1, ]) # first in-mask voxel
  off <- offset(lvec)[1]
  expect_equal(ts1, drop(B %*% L + off), tolerance = 1e-12)
  
  # voxel (x=1,y=1,z=1) is **outside** the mask -> should be all zeros
  lin_out <- 1L
  expect_equal(series(lvec, lin_out), rep(0, nt))
  
  # random mixed set of 10 linear indices (inside & outside); compare to full array
  set.seed(42)
  inds <- sample.int(prod(dim(lvec)[1:3]), 10)
  # Manual calculation by flattening the 4D array
  full_arr <- as.array(lvec) # dims [x,y,z,t]
  nvox <- prod(dim(lvec)[1:3])
  # Reshape: flatten spatial dims into rows, keep time in columns
  # as.array is in column-major order: [x,y,z,t] flatten gives first fixed x,y,z varying fastest.
  # matrix(data, ncol=nt) splits into nvox rows.
  full_mat <- matrix(full_arr, ncol = nt)
  # Transpose to [time x nvox]
  full_mat_t <- t(full_mat)
  # Subset the columns corresponding to the requested linear indices
  manual <- full_mat_t[, inds]
  
  expect_equal(series(lvec, inds), manual, tolerance = 1e-12)
})

# ------------------------------------------------------------------
# test_latent_neurovec_extract.R
# ------------------------------------------------------------------
context("[[ extractor returns correct SparseNeuroVol")

make_tiny_lvec <- function() {
  # 3 x 3 x 2 volume, 4 time points, 2 components
  sp   <- NeuroSpace(c(3,3,2,4))
  mask <- array(TRUE, dim = c(3,3,2))
  mask[1,1,] <- FALSE                      # punch out two voxels
  mask_vol <- LogicalNeuroVol(mask, drop_dim(sp))
  
  k   <- 2
  nt  <- 4
  nv  <- sum(mask)
  
  basis    <- Matrix(matrix(seq_len(nt*k), nt,  k))  # deterministic
  loadings <- Matrix(matrix(seq_len(nv*k),  nv, k))  # dense
  offset   <- rep(5, nv)                             # easy to check
  
  LatentNeuroVec(basis, loadings, sp, mask_vol, offset)
}

test_that("[[ ... returns numerically and structurally correct volume", {
  lvec <- make_tiny_lvec()
  tsel <- 3L                                # third time-point
  
  vol  <- lvec[[tsel]]
  expect_s4_class(vol, "SparseNeuroVol")
  
  # ------------- numeric equivalence ---------------------------------------
  # ground-truth 3-D array via algebra
  B_row  <- as.numeric(basis(lvec)[tsel, ])          # 1 x k
  L_mat  <- loadings(lvec)                           # p x k
  voxval <- drop(B_row %*% t(L_mat)) + offset(lvec)  # length p (only mask voxels)
  
  expected <- array(0, dim = dim(lvec)[1:3])
  expected[ which(mask(lvec)@.Data) ] <- voxval
  
  expect_equivalent(as.array(vol@data), expected, tolerance = 1e-12)
  
  # ------------- structural checks -----------------------------------------
  expect_equal(sum(vol != 0), sum(mask(lvec)), info = "all mask voxels stored once")
  expect_equal(space(vol), drop_dim(space(lvec)), info = "NeuroSpace preserved")
  # indices slot should match mask linear indices

})


# ------------------------------------------------------------------
# test_latent_neurovec_matricized_access.R
# ------------------------------------------------------------------
context("LatentNeuroVec :: matricized_access() fast-paths")

## Helper that lets us toggle dense / sparse ------------------------
make_lvec_for_mat_access <- function(sparse = FALSE) {
  # 3 x 3 x 2 volume  ––  4 time points  ––  3 components
  sp  <- NeuroSpace(c(3, 3, 2, 4))
  msk <- LogicalNeuroVol(array(TRUE, dim = c(3,3,2)), drop_dim(sp))
  
  k   <- 3
  nt  <- dim(sp)[4]
  nv  <- sum(msk)
  
  B <- matrix(seq_len(nt * k),  nrow = nt, ncol = k)        # deterministic
  L <- matrix(seq_len(nv * k)/10, nrow = nv, ncol = k)      # deterministic
  off <- rep(5, nv)
  
  if (sparse) {
    B <- Matrix::Matrix(B, sparse = TRUE)
    L <- Matrix::Matrix(L, sparse = TRUE)
  }
  
  LatentNeuroVec(basis = B, loadings = L, space = sp,
                 mask  = msk, offset = off)
}

## ---------- 1.  integer path (full time-series) -------------------
test_that("matricized_access(integer) gives the expected nTime × nVoxel block", {
  lvec <- make_lvec_for_mat_access()
  
  vox <- c(1L, 4L, 7L)                         # choose three voxels
  res <- matricized_access(lvec, vox)
  
  expect_equal(dim(res), c(dim(lvec)[4], length(vox)))
  
  # manual reference: B %*% t(L[vox,]) + offset
  manual <- tcrossprod(as.matrix(basis(lvec)),
                       as.matrix(loadings(lvec)[vox, , drop = FALSE]))
  manual <- sweep(manual, 2, offset(lvec)[vox], "+")
  
  expect_equal(res, manual, tolerance = 1e-12)
})

## ---------- 2a.  matrix path – dense branch -----------------------
test_that("matricized_access(matrix) (dense) returns correct dot-products", {
  lvec <- make_lvec_for_mat_access()
  
  pair_idx <- rbind(
    c(1L, 1L),   # (time 1, voxel 1)
    c(3L, 2L),   # (time 3, voxel 2)
    c(4L, 1L)    # (time 4, voxel 1) – duplicates on purpose
  )
  res <- matricized_access(lvec, pair_idx)
  
  # manual scalar per row
  manual <- apply(pair_idx, 1L, function(rc) {
    t <- rc[1]; v <- rc[2]
    sum(basis(lvec)[t, ] * loadings(lvec)[v, ]) + offset(lvec)[v]
  })
  
  expect_equal(res, manual, tolerance = 1e-12)
})

## ---------- 2b.  matrix path – sparse branch ----------------------
test_that("matricized_access(matrix) works when both B and L are dgCMatrix", {
  lvec <- make_lvec_for_mat_access(sparse = TRUE)   # forces dgCMatrix
  
  pair_idx <- rbind(
    c(2L, 3L),
    c(4L, 6L),
    c(1L, 5L)
  )
  res <- matricized_access(lvec, pair_idx)
  
  manual <- apply(pair_idx, 1L, function(rc) {
    t <- rc[1]; v <- rc[2]
    sum(as.numeric(basis(lvec)[t, ]) * as.numeric(loadings(lvec)[v, ])) +
      offset(lvec)[v]
  })
  
  expect_equal(res, manual, tolerance = 1e-12)
})

# --- matricized_access tests ---

test_that("matricized_access provides correct and efficient access", {
  # Create a test object with different sparse/dense configurations
  n_time <- 100    # Time points
  n_vox <- 200     # Voxels in mask
  k_small <- 2     # Small component count
  k_large <- 20    # Larger component count (for performance comparison)
  
  # Create spaces
  dims_4d <- c(10, 10, 2, n_time)  # 10×10×2 spatial, 100 time
  sp <- NeuroSpace(dims_4d)
  
  # Create mask (all TRUE for simplicity)
  mask_arr <- array(TRUE, dim = dims_4d[1:3])
  mask_vol <- LogicalNeuroVol(mask_arr, drop_dim(sp))
  
  # 1. Test with small component count (k=2)
  set.seed(123)
  # Create dense and sparse versions
  basis_dense <- matrix(rnorm(n_time * k_small), n_time, k_small)
  basis_sparse <- Matrix::Matrix(basis_dense)
  loadings_dense <- matrix(rnorm(n_vox * k_small), n_vox, k_small)
  loadings_sparse <- Matrix::Matrix(loadings_dense)
  offset <- rnorm(n_vox)
  
  # Create LatentNeuroVec objects with different matrix types
  lvec_dense_dense <- LatentNeuroVec(basis_dense, loadings_dense, sp, mask_vol, offset)
  lvec_sparse_sparse <- LatentNeuroVec(basis_sparse, loadings_sparse, sp, mask_vol, offset)
  lvec_dense_sparse <- LatentNeuroVec(basis_dense, loadings_sparse, sp, mask_vol, offset)
  
  # Create test indices: 20 random time/voxel pairs
  set.seed(456)
  n_pairs <- 20
  idx_time <- sample(n_time, n_pairs, replace=TRUE)
  idx_vox <- sample(n_vox, n_pairs, replace=TRUE)
  idx_matrix <- cbind(idx_time, idx_vox)
  
  # --- 1. Test correctness ---
  # Reference calculation - manual matrix operations
  reference_result <- numeric(n_pairs)
  for (i in 1:n_pairs) {
    t <- idx_time[i]
    v <- idx_vox[i]
    # Manual calculation for this time/voxel pair
    reference_result[i] <- sum(basis_dense[t,] * loadings_dense[v,]) + offset[v]
  }
  
  # Direct call to matricized_access
  direct_result <- neuroim2::matricized_access(lvec_dense_dense, idx_matrix)
  
  # Compare results
  expect_equal(direct_result, reference_result, tolerance=1e-12,
              info="matricized_access should return correct values")
  
  # Verify that all three matrix type combinations give same results
  expect_equal(
    neuroim2::matricized_access(lvec_dense_dense, idx_matrix),
    neuroim2::matricized_access(lvec_sparse_sparse, idx_matrix),
    tolerance=1e-12,
    info="matricized_access with different Matrix formats should match"
  )
  
  expect_equal(
    neuroim2::matricized_access(lvec_dense_dense, idx_matrix),
    neuroim2::matricized_access(lvec_dense_sparse, idx_matrix),
    tolerance=1e-12,
    info="matricized_access with mixed Matrix formats should match"
  )
  
  # --- 2. Context: Where is it called? ---
  # We can trace this through series() which uses it internally
  linear_indices <- sample(prod(dims_4d[1:3]), 5)  # 5 random voxels
  series_values <- series(lvec_dense_dense, linear_indices)
  
  # Check that series returns the expected shape: n_time × n_voxels
  expect_equal(dim(series_values), c(n_time, 5),
              info="series() should use matricized_access for efficient lookup")
  
  # --- 3. Compare optimized matrix operations for larger k ---
  if (requireNamespace("microbenchmark", quietly = TRUE)) {
    # Only run this portion if microbenchmark is available
    
    # Create new test objects with k_large components
    basis_dense_large <- matrix(rnorm(n_time * k_large), n_time, k_large)
    basis_sparse_large <- Matrix::Matrix(basis_dense_large)
    loadings_dense_large <- matrix(rnorm(n_vox * k_large), n_vox, k_large)
    loadings_sparse_large <- Matrix::Matrix(loadings_dense_large)
    
    # Create test objects
    lvec_dense_dense_large <- LatentNeuroVec(basis_dense_large, loadings_dense_large, sp, mask_vol, offset)
    lvec_sparse_sparse_large <- LatentNeuroVec(basis_sparse_large, loadings_sparse_large, sp, mask_vol, offset)
    
    # Extract matrices for benchmarking
    b1 <- basis_dense_large[idx_time[1:5],, drop=FALSE]
    b2 <- loadings_dense_large[idx_vox[1:5],, drop=FALSE]
    
    # Compare different matrix multiplication approaches
    bm <- microbenchmark::microbenchmark(
      rowSums_elementwise = rowSums(b1 * b2),
      rowSums_crossprod = rowSums(Matrix::crossprod(b2, b1)),
      times = 50
    )
    
    # Calculate median time ratio (should be > 1 for large k, showing crossprod is faster)
    median_times <- tapply(bm$time, bm$expr, median)
    ratio <- median_times["rowSums_elementwise"] / median_times["rowSums_crossprod"]
    
    # For larger k, crossprod should be faster (ratio > 1)
    message(paste0("For k=", k_large, ", crossprod efficiency ratio=", round(ratio, 2), 
                   " (>1 means crossprod is faster)"))
    expect_gte(ratio, 0.5)
    
    # Additional info printed but not tested (as results are hardware dependent)
    message("Performance ratio (elementwise/crossprod): ", round(ratio, 2), 
           " (>1 means crossprod is faster)")
  }
})

# --- Tests for validate_latent_file ---

# Helper function to create a basic, valid HDF5 structure for latent vec
# This is a simplified version, focusing on structure for validation tests.
create_minimal_latent_h5 <- function(file_path, X=5,Y=5,Z=3,Tval=10,Kval=4) {
  h5f <- NULL
  tryCatch({
    h5f <- H5File$new(file_path, mode = "w")
    
    # /header group and datasets
    hdr_grp <- h5f$create_group("header")
    hdr_grp$create_dataset("dim", robj = as.integer(c(4, X, Y, Z, Tval, 1, 1, 1)), dtype = h5types$H5T_NATIVE_INT32)
    # Add other minimal required header elements if validate_latent_file checks them before structure
    hdr_grp$create_dataset("pixdim", robj = as.double(c(0,1,1,1,1,0,0,0)), dtype = h5types$H5T_NATIVE_DOUBLE) # Example
    
    # /mask dataset
    mask_data <- array(1L, dim = c(X,Y,Z)) # All 1s for simplicity
    h5f$create_dataset("mask", robj = mask_data, dtype = h5types$H5T_NATIVE_INT32)
    nVox_mask <- sum(mask_data)

    # /basis group and dataset (dense for simplicity)
    basis_grp <- h5f$create_group("basis")
    basis_mat_data <- matrix(runif(Kval * nVox_mask), nrow = Kval, ncol = nVox_mask)
    basis_grp$create_dataset("basis_matrix", robj = basis_mat_data, dtype = h5types$H5T_NATIVE_DOUBLE)

    # /scans group and a scan with embedding
    scans_grp <- h5f$create_group("scans")
    scan1_grp <- scans_grp$create_group("scan1")
    embedding_data <- matrix(runif(Tval * Kval), nrow = Tval, ncol = Kval)
    scan1_grp$create_dataset("embedding", robj = embedding_data, dtype = h5types$H5T_NATIVE_DOUBLE)
    
    # Optional: /offset
    # h5f$create_dataset("offset", robj = rnorm(nVox_mask), dtype = h5types$H5T_NATIVE_DOUBLE)

  }, finally = {
    if (!is.null(h5f) && h5f$is_valid) {
      h5f$close_all()
    }
  })
  return(invisible(NULL))
}

test_that("validate_latent_file detects missing /header group", {
  temp_h5_malformed <- tempfile(fileext = ".malformed_header.lv.h5")
  on.exit(unlink(temp_h5_malformed), add = TRUE)

  # 1. Create a base valid structure
  create_minimal_latent_h5(temp_h5_malformed)

  # 2. Introduce malformation: delete /header group
  h5f_modify <- NULL
  tryCatch({
    h5f_modify <- H5File$new(temp_h5_malformed, mode = "a") # Open in append mode to modify
    if (h5f_modify$exists("header")) {
      h5f_modify$link_delete("header")
    } else {
      skip("Could not delete /header for test, it was not found after creation.")
    }
  }, finally = {
    if (!is.null(h5f_modify) && h5f_modify$is_valid) {
      h5f_modify$close_all()
    }
  })
  
  # 3. Validate and check for specific warning and FALSE return
  # validate_latent_file itself uses warning() for failures and returns FALSE
  # It might also throw an error before returning if file access fails catastrophically
  
  # We expect a warning from validate_latent_file, not an error stopping testthat
  # And the function should return FALSE
  # The error "Mandatory group '/header' not found." comes from a stop() call within validate_latent_file
  # So we should expect an error, not a warning.
  expect_error(
    validate_latent_file(temp_h5_malformed),
    regexp = "Mandatory group '/header' not found"
  )

  # To check the return value, we'd have to wrap validate_latent_file in a tryCatch
  # or modify validate_latent_file to not stop() on validation failures but only warn and return FALSE.
  # Given the current implementation of validate_latent_file stopping on this error,
  # testing the error is the direct approach.
  # If we wanted to test the FALSE return, validate_latent_file would need refactoring.
  # For now, let's assume that if it stops with the expected error, the 'is_valid' would be FALSE.
})

test_that("validate_latent_file detects missing /mask dataset", {
  temp_h5_malformed <- tempfile(fileext = ".malformed_mask.lv.h5")
  on.exit(unlink(temp_h5_malformed), add = TRUE)

  create_minimal_latent_h5(temp_h5_malformed)

  h5f_modify <- NULL
  tryCatch({
    h5f_modify <- H5File$new(temp_h5_malformed, mode = "a")
    if (h5f_modify$exists("mask")) {
      h5f_modify$link_delete("mask")
    } else {
      skip("Could not delete /mask for test.")
    }
  }, finally = {
    if (!is.null(h5f_modify) && h5f_modify$is_valid) h5f_modify$close_all()
  })
  
  expect_error(
    validate_latent_file(temp_h5_malformed),
    regexp = "Mandatory dataset '/mask' not found"
  )
})

test_that("validate_latent_file detects /header/dim not starting with 4", {
  temp_h5_malformed <- tempfile(fileext = ".malformed_hdr_dim.lv.h5")
  on.exit(unlink(temp_h5_malformed), add = TRUE)

  # Create the file with a bad /header/dim
  h5f <- NULL
  X=5; Y=5; Z=3; Tval=10; Kval=4
  tryCatch({
    h5f <- H5File$new(temp_h5_malformed, mode = "w")
    hdr_grp <- h5f$create_group("header")
    # Malformed dim: starts with 3 instead of 4
    hdr_grp$create_dataset("dim", robj = as.integer(c(3, X, Y, Z, Tval, 1, 1, 1)), dtype = h5types$H5T_NATIVE_INT32)
    hdr_grp$create_dataset("pixdim", robj = as.double(c(0,1,1,1,1,0,0,0)), dtype = h5types$H5T_NATIVE_DOUBLE)
    
    mask_data <- array(1L, dim = c(X,Y,Z))
    h5f$create_dataset("mask", robj = mask_data, dtype = h5types$H5T_NATIVE_INT32)
    nVox_mask <- sum(mask_data)

    basis_grp <- h5f$create_group("basis")
    basis_mat_data <- matrix(runif(Kval * nVox_mask), nrow = Kval, ncol = nVox_mask)
    basis_grp$create_dataset("basis_matrix", robj = basis_mat_data, dtype = h5types$H5T_NATIVE_DOUBLE)

    scans_grp <- h5f$create_group("scans")
    scan1_grp <- scans_grp$create_group("scan1")
    embedding_data <- matrix(runif(Tval * Kval), nrow = Tval, ncol = Kval)
    scan1_grp$create_dataset("embedding", robj = embedding_data, dtype = h5types$H5T_NATIVE_DOUBLE)
  }, finally = {
    if (!is.null(h5f) && h5f$is_valid) h5f$close_all()
  })

  # validate_latent_file should warn and return FALSE for this content error
  # It uses stop() for missing mandatory groups/datasets, but warning() for content checks.
  # The warning comes from the list of checks: valid_checks$hdr_dim0
  expect_warning(
    validation_result <- validate_latent_file(temp_h5_malformed),
    regexp = "'/header/dim' should start with 4, but starts with 3"
  )
  expect_false(validation_result)
})

# TODO: Add tests for validator function (checking expected failures) for other conditions.
</file>

<file path="tests/testthat/test-cluster_experiment.R">
# tests/testthat/test-cluster_experiment.R

library(testthat)
library(hdf5r)
library(neuroim2)
library(fmristore) # Make sure package functions are available



test_that("writer + reader round-trip", {
  tf <- tempfile(fileext = ".h5")
  on.exit(unlink(tf), add = TRUE)

  msp <- NeuroSpace(c(4,4,2), c(1,1,1))
  # Create dummy mask and clusters
  mask <- LogicalNeuroVol(array(c(rep(FALSE, 16*1), rep(TRUE, 16*1)), dim(msp)), msp)
  expect_equal(sum(mask), 16)
  set.seed(1)
  clus <- ClusteredNeuroVol(mask, sample(1:3, sum(mask), TRUE))

  # Generate some dummy data
  # Need nVoxels = sum(mask) = 16
  # cluster 1: ? voxels, cluster 2: ? voxels, cluster 3: ? voxels
  cluster_counts <- table(clus@clusters)
  n_vox_c1 <- cluster_counts[["1"]] %||% 0
  n_vox_c2 <- cluster_counts[["2"]] %||% 0
  n_vox_c3 <- cluster_counts[["3"]] %||% 0
  total_n_vox <- n_vox_c1 + n_vox_c2 + n_vox_c3
  expect_equal(total_n_vox, sum(mask)) # Sanity check

  n_time1 <- 10
  n_time2 <- 12
  
  # Create full run data list (matrices [nVoxInClus, nTime])
  full_data_list <- list()
  if (n_vox_c1 > 0) full_data_list$cluster_1 <- matrix(rnorm(n_vox_c1 * n_time1), n_vox_c1, n_time1)
  if (n_vox_c2 > 0) full_data_list$cluster_2 <- matrix(rnorm(n_vox_c2 * n_time1), n_vox_c2, n_time1)
  if (n_vox_c3 > 0) full_data_list$cluster_3 <- matrix(rnorm(n_vox_c3 * n_time1), n_vox_c3, n_time1)
  
  # Create summary run data matrix [nTime, nClusters]
  summ_mat  <- matrix(rnorm(n_time2 * 3), n_time2, 3)
  
  # Create cluster metadata data.frame
  clus_meta_df <- data.frame(cluster_id=1:3, 
                             desc=paste("Cluster", 1:3), 
                             size=c(n_vox_c1, n_vox_c2, n_vox_c3))

  # Create the runs_data list for the writer
  runs <- list(
    list(scan_name = "run1",
         type = "full",
         data = full_data_list,
         metadata = list(TR = 2.0, Task = "RestingState", SubjectID = "Sub01")),
    list(scan_name = "run2",
         type = "summary",
         data = summ_mat,
         metadata = list(TR = 2.5, Task = "FingerTapping", SubjectID = "Sub01"))
  )
  
  # Write the file
  write_clustered_experiment_h5(tf, mask, clus, runs, 
         cluster_metadata = clus_meta_df, overwrite = TRUE, verbose = FALSE)

  # --- Test Reading --- 
  # Can we open it?
  exp <- H5ClusterExperiment(tf, keep_handle_open = TRUE)
  on.exit(try(exp$h5file$close_all(), silent=TRUE), add = TRUE) # Ensure cleanup
  
  expect_s4_class(exp, "H5ClusterExperiment")
  expect_equal(n_scans(exp), 2)
  expect_equal(scan_names(exp), c("run1", "run2"))
  
  # Check run types
  expect_s4_class(exp@runs[[1]], "H5ClusterRun")
  expect_s4_class(exp@runs[[2]], "H5ClusterRunSummary")
  
  # Check loaded scan metadata
  expect_equal(exp@scan_metadata$run1$TR, 2.0)
  expect_equal(exp@scan_metadata$run1$Task, "RestingState")
  expect_equal(exp@scan_metadata$run1$SubjectID, "Sub01")
  expect_equal(exp@scan_metadata$run2$TR, 2.5)
  expect_equal(exp@scan_metadata$run2$Task, "FingerTapping")
  expect_equal(exp@scan_metadata$run2$SubjectID, "Sub01")
  
  # Check loaded cluster metadata
  expect_s3_class(exp@cluster_metadata, "data.frame")
  # Order might change on read/write depending on dataset listing order, so test carefully
  expect_setequal(names(exp@cluster_metadata), names(clus_meta_df)) 
  expect_equal(nrow(exp@cluster_metadata), nrow(clus_meta_df))
  # Check content after sorting by cluster_id to ensure alignment
  clus_meta_read_sorted <- exp@cluster_metadata[order(exp@cluster_metadata$cluster_id),]
  clus_meta_orig_sorted <- clus_meta_df[order(clus_meta_df$cluster_id),]
  expect_equal(clus_meta_read_sorted$cluster_id, clus_meta_orig_sorted$cluster_id)
  expect_equal(clus_meta_read_sorted$desc, clus_meta_orig_sorted$desc)
  expect_equal(clus_meta_read_sorted$size, clus_meta_orig_sorted$size)
  
  # Check shared components
  expect_true(identical(space(mask(exp)), space(mask)))
  expect_true(identical(mask(exp)@.Data, mask@.Data)) # Check data matches
  expect_true(identical(space(clusters(exp)), space(clus)))
  expect_true(identical(clusters(exp)@clusters, clus@clusters)) # Check data matches

  # Check handles and linkage within loaded object
  expect_true(identical(mask(exp), mask(exp@runs[[1]])))
  expect_true(identical(mask(exp), mask(exp@runs[[2]])))
  expect_true(identical(clusters(exp), clusters(exp@runs[[1]])))
  # Summary run might have NULL clusters slot if not explicitly loaded, check object identity if non-NULL
  if (!is.null(clusters(exp@runs[[2]]))) { 
      expect_true(identical(clusters(exp), clusters(exp@runs[[2]])))
  }
  expect_true(identical(h5file(exp), h5file(exp@runs[[1]])))
  expect_true(identical(h5file(exp), h5file(exp@runs[[2]])))
  
  # --- Test Data Concatenation --- 
  # Voxel concatenation (series_concat) - should match the input full data
  vox_indices <- 1:total_n_vox # Get all voxels in mask order
  
  # Reconstruct the expected full matrix [nTime, nVoxelsInMask]
  expected_full_t_vox <- matrix(NA_real_, nrow = n_time1, ncol = total_n_vox)
  mask_indices_global <- which(as.array(mask))
  
  for(cid_str in names(cluster_counts)) {
      cid <- as.integer(cid_str)
      current_run_data <- full_data_list[[paste0("cluster_", cid)]]
      if (!is.null(current_run_data)) {
          # Find which mask indices belong to this cluster
          vox_in_cluster_mask_indices <- which(clus@clusters == cid)
          # These indices correspond to columns in the final concatenated matrix
          expect_equal(nrow(current_run_data), length(vox_in_cluster_mask_indices)) # sanity check
          expected_full_t_vox[, vox_in_cluster_mask_indices] <- t(current_run_data)
      }
  }
  
  # Call series_concat for the first run (which is the full run)
  concatenated_voxels <- series_concat(exp, vox_indices, run_indices = 1)
  expect_equal(nrow(concatenated_voxels), n_time1)
  expect_equal(ncol(concatenated_voxels), total_n_vox)
  expect_equal(concatenated_voxels, expected_full_t_vox, tolerance = 1e-6)

  # Matrix concatenation (matrix_concat) - should match the input summary data
  # Call matrix_concat for the second run (which is the summary run)
  concatenated_summary <- matrix_concat(exp, run_indices = 2)
  expect_equal(nrow(concatenated_summary), n_time2)
  expect_equal(ncol(concatenated_summary), 3)
  expect_equivalent(concatenated_summary, summ_mat,  tolerance = 1e-5)
  
  # Test trying to concat wrong types
  expect_error(series_concat(exp, vox_indices, run_indices = 2), ".*not an H5ClusterRun object.*")
  expect_error(matrix_concat(exp, run_indices = 1), ".*not an H5ClusterRunSummary object.*")
})

context("H5ClusterExperiment Constructor Validation")

test_that("writer errors if mask or clusters are NULL", {
  # Create minimal valid mask/clusters
  mask_val <- LogicalNeuroVol(array(TRUE, dim=c(2,2,2)))
  clus_val <- ClusteredNeuroVol(1:8, space(mask_val))
  runs_val <- list(list(scan_name="s1", type="summary", data=matrix(1:10, 5, 2)))
  tf <- tempfile(fileext = ".h5")
  on.exit(unlink(tf), add=TRUE)
  
  expect_error(write_clustered_experiment_h5(tf, mask=NULL, clusters=clus_val, runs_data=runs_val), ".*must be.*LogicalNeuroVol.*")
  expect_error(write_clustered_experiment_h5(tf, mask=mask_val, clusters=NULL, runs_data=runs_val), ".*must be.*ClusteredNeuroVol.*")
})

test_that("reader default path (mask=NULL, clusters=NULL) works", {
  tf <- tempfile(fileext = ".h5")
  on.exit(unlink(tf), add = TRUE)
  mask <- LogicalNeuroVol(array(c(FALSE, TRUE, TRUE, FALSE), dim = c(2,2,1)))
  clus <- ClusteredNeuroVol(1:2, space(mask))
  runs <- list(list(scan_name = "run1", type = "summary", data = matrix(1:10, 5, 2)))
  write_clustered_experiment_h5(tf, mask, clus, runs, overwrite = TRUE, verbose = FALSE)
  
  # Should load successfully with mask=NULL, clusters=NULL
  exp <- H5ClusterExperiment(tf)
  expect_s4_class(exp, "H5ClusterExperiment")
  expect_s4_class(mask(exp), "LogicalNeuroVol")
  expect_s4_class(clusters(exp), "ClusteredNeuroVol")
  expect_equal(sum(mask(exp)), sum(mask)) # Check if loaded mask is correct
  # Check space consistency implicitly via identical check below
  expect_true(identical(space(mask(exp)), space(mask)))
})

test_that("reader validation works for provided mask and clusters", {
  tf <- tempfile(fileext = ".h5")
  on.exit(unlink(tf), add = TRUE)
  mask_orig <- LogicalNeuroVol(array(c(rep(FALSE,4), rep(TRUE,4)), dim = c(2,2,2)))
  clus_orig <- ClusteredNeuroVol(sample(1:2, sum(mask_orig), TRUE), space(mask_orig))
  runs <- list(list(scan_name = "run1", type = "summary", data = matrix(1:10, 5, 2)))
  # Write cluster metadata as well
  clus_meta_df <- data.frame(cluster_id=1:2, desc=c("One", "Two"))
  write_clustered_experiment_h5(tf, mask_orig, clus_orig, runs, cluster_metadata=clus_meta_df, overwrite = TRUE, verbose = FALSE)

  # 1. Provide valid mask and clusters - should succeed
  expect_message(exp_valid <- H5ClusterExperiment(tf, mask = mask_orig, clusters = clus_orig), ".*validation successful.*", all=TRUE)
  expect_s4_class(exp_valid, "H5ClusterExperiment")
  expect_true(identical(mask(exp_valid), mask_orig)) # Checks space and data
  expect_true(identical(clusters(exp_valid), clus_orig)) # Checks space and data

  # 2. Provide mask with different space - should fail
  mask_bad_space <- LogicalNeuroVol(mask_orig@.Data, NeuroSpace(dim=c(2,2,2), spacing=c(2,2,2)))
  expect_error(H5ClusterExperiment(tf, mask = mask_bad_space), ".*NeuroSpace does not match.*")

  # 3. Provide mask with different voxel pattern - should fail
  mask_bad_pattern <- mask_orig
  mask_bad_pattern[1,1,1] <- TRUE # Original was FALSE
  expect_error(H5ClusterExperiment(tf, mask = mask_bad_pattern), ".*pattern of TRUE voxels does not match.*")
  
  # 4. Provide mask with wrong sum (caught by length check against cluster_map)
  mask_bad_sum <- LogicalNeuroVol(array(TRUE, dim=c(2,2,2))) # sum=8, but cluster_map length=4
  expect_error(H5ClusterExperiment(tf, mask = mask_bad_sum), ".*TRUE voxel count.*does not match length.*cluster_map.*") 
  
  # 5. Provide clusters with different space - should fail
  clus_bad_space <- ClusteredNeuroVol(clus_orig@clusters, space(mask_bad_space)) # Use mask with wrong space
  # Need to pass the original mask here, error happens when comparing clusters space to mask space
  expect_error(H5ClusterExperiment(tf, mask = mask_orig, clusters = clus_bad_space), ".*clusters object.*NeuroSpace does not match the mask.*")
  
  # 6. Provide clusters with wrong length - should fail
  clus_bad_len <- ClusteredNeuroVol(c(clus_orig@clusters, 99), space(mask_orig)) # Add extra element
  expect_error(H5ClusterExperiment(tf, mask = mask_orig, clusters = clus_bad_len), ".*clusters vector length.*does not match.*number of TRUE voxels.*")

})

test_that("summary_preference defaults based on /scans attribute", {
  tf1 <- tempfile(fileext = ".h5")
  tf2 <- tempfile(fileext = ".h5")
  on.exit({unlink(tf1); unlink(tf2)}, add = TRUE)

  mask <- fmristore:::create_minimal_LogicalNeuroVol(dims = c(2,2,2))
  clus <- fmristore:::create_minimal_ClusteredNeuroVol(mask_vol = mask, num_clusters = 2)

  runs_sum <- list(list(scan_name = "s1", type = "summary", data = matrix(1:4, 2, 2)))
  write_clustered_experiment_h5(tf1, mask, clus, runs_sum, overwrite = TRUE, verbose = FALSE)

  exp_sum <- H5ClusterExperiment(tf1)
  expect_s4_class(exp_sum@runs[[1]], "H5ClusterRunSummary")

  runs_full <- list(list(scan_name = "f1", type = "full", data = list(cluster_1 = matrix(1:4, 2, 2))))
  write_clustered_experiment_h5(tf2, mask, clus, runs_full, overwrite = TRUE, verbose = FALSE)

  exp_full <- H5ClusterExperiment(tf2)
  expect_s4_class(exp_full@runs[[1]], "H5ClusterRun")
})

test_that("warning for mismatched summary_only attribute", {
  tf <- tempfile(fileext = ".h5")
  on.exit(unlink(tf), add = TRUE)

  mask <- fmristore:::create_minimal_LogicalNeuroVol(dims = c(2,2,2))
  clus <- fmristore:::create_minimal_ClusteredNeuroVol(mask_vol = mask, num_clusters = 2)

  runs_full <- list(list(scan_name = "f1", type = "full", data = list(cluster_1 = matrix(1:4, 2, 2))))
  write_clustered_experiment_h5(tf, mask, clus, runs_full, overwrite = TRUE, verbose = FALSE)

  h5f <- hdf5r::H5File$new(tf, mode = "r+")
  h5attr(h5f[["scans"]], "summary_only") <- TRUE
  h5f$close_all()

  expect_warning(exp_warn <- H5ClusterExperiment(tf), "/scans@summary_only")
  expect_s4_class(exp_warn@runs[[1]], "H5ClusterRun")
})

})

test_that("dataset cluster_meta is read as data.frame", {
  tf <- tempfile(fileext = ".h5")
  on.exit(unlink(tf), add = TRUE)

  msp <- NeuroSpace(c(2,2,1), c(1,1,1))
  mask <- LogicalNeuroVol(array(TRUE, dim = c(2,2,1)), msp)
  clus <- ClusteredNeuroVol(mask, c(1L,2L,1L,2L))

  full_data_list <- list(
    cluster_1 = matrix(rnorm(2*3), 2, 3),
    cluster_2 = matrix(rnorm(2*3), 2, 3)
  )
  runs <- list(list(scan_name = "run1", type = "full", data = full_data_list))
  meta_df <- data.frame(cluster_id=c(1L,2L), desc=c("A","B"))

  write_clustered_experiment_h5(tf, mask, clus, runs,
                                cluster_metadata = meta_df,
                                overwrite = TRUE, verbose = FALSE)

  h5f <- H5File$new(tf, mode="r+")
  meta_grp <- h5f[["/clusters/cluster_meta"]]
  meta_list <- lapply(names(meta_grp), function(nm) meta_grp[[nm]][])
  names(meta_list) <- names(meta_grp)
  df <- as.data.frame(meta_list)
  h5f$link_delete("/clusters/cluster_meta")
  h5f$create_dataset("/clusters/cluster_meta", df)
  h5f$close_all()

  exp <- H5ClusterExperiment(tf)
  expect_s3_class(exp@cluster_metadata, "data.frame")
  expect_setequal(names(exp@cluster_metadata), names(meta_df))
  expect_equal(nrow(exp@cluster_metadata), nrow(meta_df))
})
</file>

<file path="tests/testthat/test-cluster_run_full.R">
# tests/testthat/test-cluster_run_full.R

library(testthat)
library(hdf5r)
library(neuroim2)
# Assuming fmristore classes/methods are loaded

# Helper to setup the test HDF5 file using create_dummy...
# Allows controlling how n_time is stored for different test cases
setup_test_file_full <- function(write_n_time_attr = FALSE, 
                                   write_n_time_meta = FALSE,
                                   n_time_val = 12, # Match default in create_dummy...
                                   ...) { # Pass extra args to create_dummy...
  
  auto_loc <- "none"
  if (write_n_time_attr && write_n_time_meta) {
    stop("Cannot set both write_n_time_attr and write_n_time_meta to TRUE")
  }
  if (write_n_time_attr) auto_loc <- "attribute"
  if (write_n_time_meta) auto_loc <- "metadata"
  
  tmp_filepath <- tempfile(fileext = ".h5")
  
  create_dummy_clustered_full_h5(filepath = tmp_filepath, 
                                 n_time = n_time_val, 
                                 auto_n_time_location = auto_loc,
                                 ...)
}

# Helper to clean up the created test file
cleanup_test_file <- function(setup_info) {
  if (!is.null(setup_info) && !is.null(setup_info$filepath) && file.exists(setup_info$filepath)) {
    unlink(setup_info$filepath)
  }
}

# Helper function to create a dummy HDF5 file with FULL clustered data
create_dummy_clustered_full_h5 <- function(filepath,
                                           dims = c(4, 4, 3), # x,y,z
                                           n_time = 12,
                                           scan_name = "scan_full1",
                                           cluster_ids = 1:3,
                                           auto_n_time_location = "none", # "none", "attribute", "metadata"
                                           invalid_cluster_shape = FALSE) {

  n_clusters <- length(cluster_ids)
  n_vox_total <- prod(dims)

  # Create basic mask (e.g., center region)
  mask_arr <- array(FALSE, dim = dims)
  mask_arr[2:3, 2:3, 1:2] <- TRUE # Smaller mask within dims
  mask_vol <- LogicalNeuroVol(mask_arr, NeuroSpace(dims))
  n_vox_mask <- sum(mask_vol)

  # Create cluster map for voxels within the mask
  # Assign voxels to clusters sequentially
  clus_map_vals <- rep(cluster_ids, length.out = n_vox_mask)
  clusters_vol <- ClusteredNeuroVol(mask_vol, clusters = clus_map_vals)

  # Create voxel coordinates (relative to mask)
  voxel_coords_mask <- which(mask_arr, arr.ind = TRUE) # Get 3D indices of TRUE values
  colnames(voxel_coords_mask) <- c("x", "y", "z")

  # Create predictable full data (store temporarily)
  # We need data structured as list[[cluster_id]][nVoxInClus, nTime]
  cluster_data_list <- list()

  # Create HDF5 file
  file <- H5File$new(filepath, mode = "w")
  on.exit(if(file$is_valid) file$close_all(), add = TRUE) # Ensure closure

  # Write Mask, Cluster Map, Voxel Coords
  file[["mask"]] <- as.integer(mask_arr)
  file[["cluster_map"]] <- clus_map_vals
  file[["voxel_coords"]] <- voxel_coords_mask

  # Write Global Cluster Info
  clus_grp <- file$create_group("clusters")
  clus_grp[["cluster_ids"]] <- cluster_ids
  # Simple meta for now
  clus_meta_df <- data.frame(cluster_id = cluster_ids, description = paste("Global Cluster", cluster_ids))
  # hdf5r needs help with data.frames -> convert to list or compound type if necessary
  # For simplicity, let's skip writing complex meta for now, just IDs.

  # Create Scan Group
  scans_grp <- file$create_group("scans")
  scan_grp <- scans_grp$create_group(scan_name)

  # Write n_time if requested
  if (auto_n_time_location == "attribute") {
      h5attr(scan_grp, "n_time") <- n_time
  } else if (auto_n_time_location == "metadata") {
      meta_grp <- scan_grp$create_group("metadata")
      meta_grp[["n_time"]] <- n_time
  }

  # Write Full Cluster Data under the scan
  scan_clus_grp <- scan_grp$create_group("clusters")
  expected_reconstruction <- array(0, dim = c(dims, n_time)) # For potential checks

  for (cid in cluster_ids) {
    # Find which voxels *within the mask* belong to this cluster
    mask_indices_in_cluster <- which(clusters_vol@clusters == cid)
    n_vox_in_cluster <- length(mask_indices_in_cluster)

    if (n_vox_in_cluster > 0) {
      # Generate data [nVoxInCluster, nTime]
      # Formula: cluster_id * 1000 + (voxel_offset_within_cluster * 100) + timepoint
      cluster_mat <- matrix(0, nrow = n_vox_in_cluster, ncol = n_time)
      for (vox_idx in 1:n_vox_in_cluster) {
        cluster_mat[vox_idx, ] <- cid * 1000 + (vox_idx * 100) + 1:n_time
      }

      # Write dataset
      if (invalid_cluster_shape && cid == cluster_ids[1]) {
        # Add an extra singleton dimension to make the dataset 3D
        scan_clus_grp[[paste0("cluster_", cid)]] <- array(cluster_mat, dim = c(dim(cluster_mat), 1))
      } else {
        scan_clus_grp[[paste0("cluster_", cid)]] <- cluster_mat
      }
      cluster_data_list[[as.character(cid)]] <- cluster_mat

      # Fill expected full array (for testing subsets later)
      # Get the 3D coordinates corresponding to these mask indices
      coords_this_cluster <- voxel_coords_mask[mask_indices_in_cluster, , drop=FALSE]
      for(vox_idx in 1:n_vox_in_cluster) {
          coord <- coords_this_cluster[vox_idx, ]
          expected_reconstruction[coord[1], coord[2], coord[3], ] <- cluster_mat[vox_idx, ]
      }
    } else {
      # Handle case where a cluster ID might have 0 voxels (though unlikely with rep())
       scan_clus_grp[[paste0("cluster_", cid)]] <- matrix(numeric(0), nrow=0, ncol=n_time)
       cluster_data_list[[as.character(cid)]] <- matrix(numeric(0), nrow=0, ncol=n_time)
    }
  }

  # Explicitly close before returning path
  file$close_all()

  return(list(filepath = filepath,
              mask = mask_vol,
              clusters = clusters_vol,
              voxel_coords = voxel_coords_mask, # Coords of masked voxels
              dims = dims,
              n_time = n_time,
              scan_name = scan_name,
              cluster_ids = cluster_ids,
              cluster_data_list = cluster_data_list, # Data as stored per cluster
              expected_reconstruction = expected_reconstruction # Full 4D array (potentially large)
             ))
}


test_that("make_run_full constructs object correctly from file path", {
  setup_info <- setup_test_file_full()
  on.exit(cleanup_test_file(setup_info))

  # Use new constructor
  run_full <- H5ClusterRun(file = setup_info$filepath,
                                 scan_name = setup_info$scan_name,
                                 mask = setup_info$mask,
                                 clusters = setup_info$clusters,
                                 n_time = setup_info$n_time) 
  
  expect_s4_class(run_full, "H5ClusterRun")
  expect_equal(run_full@scan_name, setup_info$scan_name)
  expect_equal(run_full@n_time, setup_info$n_time)
  expect_equal(run_full@n_voxels, sum(setup_info$mask))
  expect_true(inherits(run_full@obj, "H5File"))
  expect_true(run_full@obj$is_valid)
  
  # Important: Close the handle managed by the object
  expect_silent(h5file(run_full)$close_all())
})

# Test constructing from an already open H5File handle
test_that("make_run_full constructs object correctly from open H5File handle", {
  setup_info <- setup_test_file_full()
  on.exit(cleanup_test_file(setup_info))
  
  h5f <- H5File$new(setup_info$filepath, mode = 'r')
  # Note: If make_run_full takes ownership, h5f might be closed by its finalizer
  # Here, we assume make_run_full *uses* the handle but doesn't close it if passed open
  on.exit(if (h5f$is_valid) h5f$close_all(), add = TRUE)

  # Use new constructor with the file path instead of the H5File object
  # We need to get the filepath from the H5File object
  filepath <- h5f$get_filename()
  
  # Create the object with the new constructor
  run_full_open <- H5ClusterRun(file = filepath,
                                     scan_name = setup_info$scan_name,
                                     mask = setup_info$mask,
                                     clusters = setup_info$clusters,
                                     n_time = setup_info$n_time)
  
  # Set the object's H5File handle to our existing one to avoid opening a new connection
  # This assumes the constructor has already opened and stored a handle
  if (run_full_open@obj$is_valid) {
    run_full_open@obj$close_all()  # Close the automatically created handle
  }
  run_full_open@obj <- h5f  # Use our existing handle
  
  expect_s4_class(run_full_open, "H5ClusterRun")
  expect_true(run_full_open@obj$is_valid)
  expect_equal(h5file(run_full_open)$get_filename(), h5f$get_filename())
  # Don't close run_full_open@obj here, as it shares the handle with h5f which is closed by on.exit
})

test_that("make_run_full reads n_time from HDF5 attributes if NULL", {
  setup_attr <- setup_test_file_full(write_n_time_attr = TRUE)
  on.exit(cleanup_test_file(setup_attr))

  # Use new constructor
  run_attr <- H5ClusterRun(file = setup_attr$filepath,
                                 scan_name = setup_attr$scan_name,
                                 mask = setup_attr$mask,
                                 clusters = setup_attr$clusters,
                                 n_time = NULL) # Explicitly pass NULL
                                 
  expect_equal(run_attr@n_time, setup_attr$n_time)
  expect_silent(h5file(run_attr)$close_all())
})

test_that("make_run_full reads n_time from HDF5 metadata dataset if NULL", {
  setup_meta <- setup_test_file_full(write_n_time_meta = TRUE)
  on.exit(cleanup_test_file(setup_meta))

  # Use new constructor
  run_meta <- H5ClusterRun(file = setup_meta$filepath,
                                 scan_name = setup_meta$scan_name,
                                 mask = setup_meta$mask,
                                 clusters = setup_meta$clusters,
                                 n_time = NULL)
                                 
  expect_equal(run_meta@n_time, setup_meta$n_time)
  expect_silent(h5file(run_meta)$close_all())
})

test_that("make_run_full infers n_time from first cluster dataset if NULL and other sources missing", {
  setup_none <- setup_test_file_full(write_n_time_attr = FALSE, write_n_time_meta = FALSE)
  on.exit(cleanup_test_file(setup_none))

  # Use new constructor
  expect_message( # Expect message about inference
      run_infer <- H5ClusterRun(file = setup_none$filepath,
                                    scan_name = setup_none$scan_name,
                                    mask = setup_none$mask,
                                    clusters = setup_none$clusters,
                                    n_time = NULL),
      "Inferred n_time"
  )
  expect_equal(run_infer@n_time, setup_none$n_time)
  expect_silent(h5file(run_infer)$close_all())
})


test_that("make_run_full throws errors for invalid inputs", {
  setup_info <- setup_test_file_full()
  on.exit(cleanup_test_file(setup_info))

  # Create bad inputs
  bad_mask <- setup_info$mask
  neuroim2::dim(bad_mask) <- c(1,1,1) # Mismatched dimensions
  bad_clusters <- setup_info$clusters
  bad_clusters@clusters <- bad_clusters@clusters[-1] # Mismatched length

  # Use new constructor for error checks
  expect_error(H5ClusterRun("nonexistent.h5", "s1", setup_info$mask, setup_info$clusters, 10), "HDF5 file does not exist") # Error from open_h5
  # Original tests for bad mask/cluster types are now implicitly tested by `is()` checks inside the constructor
  # expect_error(H5ClusterRun(setup_info$filepath, setup_info$scan_name, 1, setup_info$clusters, setup_info$n_time), "must be a LogicalNeuroVol")
  # expect_error(H5ClusterRun(setup_info$filepath, setup_info$scan_name, setup_info$mask, 1, setup_info$n_time), "must be a ClusteredNeuroVol")
  # Test dimension mismatch (uses check_same_dims)
  expect_error(H5ClusterRun(file=setup_info$filepath, scan_name=setup_info$scan_name, mask=bad_mask, clusters=setup_info$clusters, n_time=setup_info$n_time), "Dimensions of 'mask' and 'clusters' must match")
  # Test cluster length mismatch
  expect_error(H5ClusterRun(file=setup_info$filepath, scan_name=setup_info$scan_name, mask=setup_info$mask, clusters=bad_clusters, n_time=setup_info$n_time), "Mismatch: clusters@clusters length")
  # Test error if n_time cannot be determined
  setup_uninferrable <- setup_test_file_full(clear_cluster_data = TRUE)
  on.exit(cleanup_test_file(setup_uninferrable), add = TRUE)
  expect_error(H5ClusterRun(file = setup_uninferrable$filepath,
                                  scan_name = setup_uninferrable$scan_name,
                                  mask = setup_uninferrable$mask,
                                  clusters = setup_uninferrable$clusters,
                                  n_time = NULL), 
               "Could not determine 'n_time'")
})

test_that("make_run_full stops if n_time determined is invalid", {
  setup_info <- setup_test_file_full(write_n_time_attr = TRUE, n_time_val = -5) # Write invalid n_time
  on.exit(cleanup_test_file(setup_info))
  
  # Use new constructor
  expect_error(H5ClusterRun(file = setup_info$filepath,
                                  scan_name = setup_info$scan_name,
                                  mask = setup_info$mask,
                                  clusters = setup_info$clusters,
                                  n_time = NULL), 
               "must be a single positive integer")
})

test_that("make_run_full errors when cluster dataset has wrong dimensions", {
  setup_bad <- setup_test_file_full(invalid_cluster_shape = TRUE)
  on.exit(cleanup_test_file(setup_bad))

  expect_error(
    H5ClusterRun(file = setup_bad$filepath,
                       scan_name = setup_bad$scan_name,
                       mask = setup_bad$mask,
                       clusters = setup_bad$clusters,
                       n_time = NULL),
    "malformed per specification"
  )
})

# ------------------------------------------------------------------------------
# Data access tests

test_that("series() retrieves correct voxel time series", {
  setup_info <- setup_test_file_full()
  on.exit(cleanup_test_file(setup_info))

  run_full <- H5ClusterRun(file = setup_info$filepath,
                                 scan_name = setup_info$scan_name,
                                 mask = setup_info$mask,
                                 clusters = setup_info$clusters,
                                 n_time = setup_info$n_time)

  mask_idx1 <- 1L
  coord1 <- setup_info$voxel_coords[mask_idx1, ]
  expected_ts <- as.numeric(setup_info$expected_reconstruction[
    coord1[1], coord1[2], coord1[3], ]
  )

  ts_mask <- as.numeric(series(run_full, mask_idx1))
  expect_equal(ts_mask, expected_ts)

  ts_coord <- as.numeric(series(run_full, coord1[1], coord1[2], coord1[3]))
  expect_equal(ts_coord, expected_ts)

  mask_indices <- 1:2
  coords_mat <- setup_info$voxel_coords[mask_indices, ]
  expected_mat <- vapply(mask_indices, function(ii) {
    cidx <- setup_info$voxel_coords[ii, ]
    setup_info$expected_reconstruction[cidx[1], cidx[2], cidx[3], ]
  }, numeric(setup_info$n_time))

  ts_multi <- series(run_full, coords_mat)
  expect_equal(ts_multi, expected_mat)

  h5file(run_full)$close_all()
})

test_that("linear_access reconstructs voxel values", {
  setup_info <- setup_test_file_full()
  on.exit(cleanup_test_file(setup_info))

  run_full <- H5ClusterRun(file = setup_info$filepath,
                                 scan_name = setup_info$scan_name,
                                 mask = setup_info$mask,
                                 clusters = setup_info$clusters,
                                 n_time = setup_info$n_time)

  dims4 <- dim(run_full)
  coord_in <- setup_info$voxel_coords[1, ]
  t_in <- 1L
  idx_in <- coord_in[1] + (coord_in[2] - 1) * dims4[1] +
            (coord_in[3] - 1) * dims4[1] * dims4[2] +
            (t_in - 1) * dims4[1] * dims4[2] * dims4[3]

  val_in <- linear_access(run_full, idx_in)
  expected_in <- setup_info$expected_reconstruction[
    coord_in[1], coord_in[2], coord_in[3], t_in]
  expect_equal(val_in, expected_in)

  coord_out <- c(1L, 1L, 3L)
  t_out <- 1L
  idx_out <- coord_out[1] + (coord_out[2] - 1) * dims4[1] +
             (coord_out[3] - 1) * dims4[1] * dims4[2] +
             (t_out - 1) * dims4[1] * dims4[2] * dims4[3]

  val_out <- linear_access(run_full, idx_out)
  expect_equal(val_out, 0)

  h5file(run_full)$close_all()
})
</file>

<file path="DESCRIPTION">
Package: fmristore
Type: Package
Title: Efficient Storage of fMRI Data
Version: 0.1.0
Authors@R: 
    person(given = "Bradley",
           family = "Buchsbaum",
           role = c("aut", "cre"),
           email = "brad.buchsbaum@gmail.com",
           comment = c(ORCID = "0000-0002-5891-3515"))
Description: Provides efficient storage of fMRI data in HDF5 format.
License: GPL (>= 3)
Encoding: UTF-8
LazyData: true
RoxygenNote: 7.3.2.9000
Imports: 
    assertthat,
    cli,
    crayon,
    hdf5r,
    lifecycle,
    Matrix,
    methods,
    neuroim2,
    purrr,
    withr
Suggests: 
    knitr,
    rmarkdown,
    testthat (>= 3.0.0)
VignetteBuilder: knitr
Config/testthat/edition: 3
Collate: 
    'all_class.R'
    'all_generic.R'
    'assertions.R'
    'cluster_array.R'
    'cluster_experiment.R'
    'constructors.R'
    'h5_utils.R'
    'h5neurovec.R'
    'h5neurovol.R'
    'io_h5_generic.R'
    'io_h5_helpers.R'
    'io_write_h5.R'
    'labeled_vec.R'
    'latent_vec.R'
    'zzz_example_helpers.R'
</file>

<file path="R/cluster_experiment.R">
#' @include all_class.R cluster_array.R
#' @importFrom methods is
NULL

# Contains methods and helper functions for H5ClusterExperiment objects

# Helper function for stricter validation of a user-provided mask against HDF5 content
# Internal to H5ClusterExperiment constructor logic
#
# @param user_mask The mask object provided by the user (already passed basic checks by ensure_mask).
# @param h5_handle An open H5File handle to the experiment file.
# @param expected_cmap_len The expected number of voxels based on /cluster_map length.
# @param master_space The NeuroSpace of the experiment (for dimension information).
# @return Invisible NULL if validation passes, otherwise stops with an error.
# @keywords internal
.validate_user_provided_mask_h5 <- function(user_mask, h5_handle, expected_cmap_len, master_space) {
  if (!is(user_mask, "LogicalNeuroVol")) {
    stop("Internal Error: .validate_user_provided_mask_h5 expects a LogicalNeuroVol for user_mask.")
  }
  if (!is(h5_handle, "H5File") || !h5_handle$is_valid) {
    stop("Internal Error: .validate_user_provided_mask_h5 requires a valid H5File handle.")
  }
  if (!is(master_space, "NeuroSpace")) {
    stop("Internal Error: .validate_user_provided_mask_h5 requires a NeuroSpace object for master_space.")
  }

  message("[H5ClusterExperiment] Performing stricter HDF5-based validation for user-provided mask...")
  
  # 1. Check consistency with cluster map length (number of active voxels)
  if (sum(user_mask@.Data) != expected_cmap_len) {
    stop(sprintf("Provided mask's TRUE voxel count (%d) does not match length of HDF5 /cluster_map (%d). User mask override rejected.",
                 sum(user_mask@.Data), expected_cmap_len))
  }

  # 2. Check voxel pattern consistency against /voxel_coords or /mask in the HDF5 file
  file_voxel_indices <- NULL
  vox_coords_dset <- NULL
  mask_dset_val <- NULL
  
  # Local on.exit for HDF5 handles opened within this helper
  on.exit({
    if (!is.null(vox_coords_dset) && inherits(vox_coords_dset, "H5D") && vox_coords_dset$is_valid) close_h5_safely(vox_coords_dset)
    if (!is.null(mask_dset_val) && inherits(mask_dset_val, "H5D") && mask_dset_val$is_valid) close_h5_safely(mask_dset_val)
  }, add = TRUE) # Add to existing on.exit handlers if any in calling scope (though this is top-level helper)

  tryCatch({
    if (h5_handle$exists("/voxel_coords")) {
      vox_coords_dset <- h5_handle[["/voxel_coords"]]
      coords_matrix <- vox_coords_dset$read()
      # Ensure master_space@dim is valid and has 3 dimensions for this calculation
      if (length(master_space@dim) < 3) stop("Master space dimensions are less than 3.")
      file_voxel_indices <- coords_matrix[,1] + (coords_matrix[,2]-1)*master_space@dim[1] + (coords_matrix[,3]-1)*master_space@dim[1]*master_space@dim[2]
    } else if (h5_handle$exists("/mask")) {
      warning("Dataset '/voxel_coords' not found, validating user mask against HDF5 '/mask' data. Consider adding /voxel_coords for efficiency.")
      mask_dset_val <- h5_handle[["/mask"]]
      file_mask_data <- mask_dset_val$read()
      file_voxel_indices <- which(as.logical(file_mask_data))
    } else {
      stop("Cannot validate user-provided mask pattern: neither '/voxel_coords' nor '/mask' found in HDF5 file.")
    }
  }, error = function(e) {
    stop(sprintf("Error reading HDF5 '/voxel_coords' or '/mask' for user mask validation: %s", e$message))
  })

  provided_voxel_indices <- which(user_mask@.Data)
  if (!identical(sort(provided_voxel_indices), sort(file_voxel_indices))) {
    # Comparing sorted indices because order might not be guaranteed identical even if sets are same
    stop("User-provided mask's pattern of TRUE voxels does not match the pattern derived from the HDF5 file ('/voxel_coords' or '/mask'). User mask override rejected.")
  }
  
  message("[H5ClusterExperiment] User-provided mask pattern validation successful.")
  invisible(NULL)
}

#' @describeIn series_concat Concatenate voxel time series for H5ClusterExperiment
#' @export
#' @family H5Cluster
setMethod("series_concat",
  signature(experiment = "H5ClusterExperiment", mask_idx = "numeric"),
  function(experiment, mask_idx, run_indices = NULL) {

  if (length(experiment@runs) == 0) {
    warning("[series_concat] Experiment contains no runs. Returning empty matrix.")
    return(matrix(numeric(0), nrow = 0, ncol = length(mask_idx %||% 0)))
  }
  # Use the assertion helper
  assert_non_empty_numeric(mask_idx, arg = "mask_idx", fn = "series_concat")
  
  # Validate run_indices if provided
  if (is.null(run_indices)) {
      run_indices <- seq_along(experiment@runs)
  } else {
      if (!is.numeric(run_indices) || any(run_indices < 1) || any(run_indices > length(experiment@runs))) {
          stop(sprintf("[series_concat] Invalid 'run_indices'. Must be between 1 and %d.", length(experiment@runs)))
      }
      run_indices <- as.integer(run_indices)
  }
  if (length(run_indices) == 0) {
      warning("[series_concat] No runs selected by 'run_indices'. Returning empty matrix.")
      return(matrix(numeric(0), nrow = 0, ncol = length(mask_idx)))
  }

  all_series <- list()
  total_time <- 0
  first_run_voxels <- NULL # To check consistency

  for (idx in run_indices) {
    current_run <- experiment@runs[[idx]]

    if (!is(current_run, "H5ClusterRun")) {
      stop(sprintf("[series_concat] Run %d (scan: '%s') is not an H5ClusterRun object. Voxel-level series cannot be extracted.",
                   idx, current_run@scan_name))
    }

    tryCatch({
      # series() returns [nTime, nVoxels] matrix
      run_series <- series(current_run, i = mask_idx)

      # Basic consistency check on number of columns returned
      if (is.null(first_run_voxels)) {
          first_run_voxels <- ncol(run_series)
          if (first_run_voxels != length(mask_idx)) {
             warning(sprintf("[series_concat] Run %d returned %d voxels, but %d mask indices were requested. Mismatch possible if some indices were outside mask.", idx, first_run_voxels, length(mask_idx)))
          }
      } else if (ncol(run_series) != first_run_voxels) {
          stop(sprintf("[series_concat] Inconsistent number of voxels returned between runs (Run 1: %d, Run %d: %d). Concatenation aborted.", first_run_voxels, idx, ncol(run_series)))
      }

      all_series[[length(all_series) + 1]] <- run_series
      total_time <- total_time + nrow(run_series)

    }, error = function(e) {
      stop(sprintf("[series_concat] Failed to extract series for run %d (scan: '%s'). Error: %s",
                   idx, current_run@scan_name, e$message))
    })
  }

  if (length(all_series) == 0) {
    return(matrix(numeric(0), nrow = 0, ncol = length(mask_idx))) # Should match first_run_voxels if non-null?
  }

  # Use do.call for efficiency
  result_matrix <- do.call(rbind, all_series)
  
  # Final dimension check
  if (nrow(result_matrix) != total_time || (!is.null(first_run_voxels) && ncol(result_matrix) != first_run_voxels)) {
       warning("[series_concat] Final matrix dimensions seem inconsistent with expectations after rbind. Check results carefully.")
  }

  return(result_matrix)
})


#' @describeIn matrix_concat Concatenate summary matrices for H5ClusterExperiment
#' @export
#' @family H5Cluster
setMethod("matrix_concat",
  signature(experiment = "H5ClusterExperiment"),
  function(experiment, run_indices = NULL) {

  if (length(experiment@runs) == 0) {
    warning("[matrix_concat] Experiment contains no runs. Returning empty matrix.")
    return(matrix(numeric(0), nrow = 0, ncol = 0))
  }
  
  # Validate run_indices if provided
  if (is.null(run_indices)) {
      run_indices <- seq_along(experiment@runs)
  } else {
      if (!is.numeric(run_indices) || any(run_indices < 1) || any(run_indices > length(experiment@runs))) {
          stop(sprintf("[matrix_concat] Invalid 'run_indices'. Must be between 1 and %d.", length(experiment@runs)))
      }
      run_indices <- as.integer(run_indices)
  }
  if (length(run_indices) == 0) {
      warning("[matrix_concat] No runs selected by 'run_indices'. Returning empty matrix.")
      return(matrix(numeric(0), nrow = 0, ncol = 0))
  }

  all_matrices <- list()
  total_time <- 0
  n_clusters <- NULL # Check for consistency

  for (idx in run_indices) {
    current_run <- experiment@runs[[idx]]

    if (!is(current_run, "H5ClusterRunSummary")) {
      stop(sprintf("[matrix_concat] Run %d (scan: '%s') is not an H5ClusterRunSummary object. Summary matrix cannot be extracted.",
                   idx, current_run@scan_name))
    }

    tryCatch({
      run_matrix <- as.matrix(current_run)

      # Check consistency of number of clusters (columns)
      if (is.null(n_clusters)) {
        n_clusters <- ncol(run_matrix)
      } else if (ncol(run_matrix) != n_clusters) {
        stop(sprintf("[matrix_concat] Inconsistent number of clusters found. Run %d has %d columns, expected %d. Concatenation aborted.",
                     idx, ncol(run_matrix), n_clusters))
      }

      all_matrices[[length(all_matrices) + 1]] <- run_matrix
      total_time <- total_time + nrow(run_matrix)

    }, error = function(e) {
      stop(sprintf("[matrix_concat] Failed to extract matrix for run %d (scan: '%s'). Error: %s",
                   idx, current_run@scan_name, e$message))
    })
  }

  if (length(all_matrices) == 0) {
    return(matrix(numeric(0), nrow = 0, ncol = (n_clusters %||% 0)))
  }

  result_matrix <- do.call(rbind, all_matrices)
  
  # Final dimension check
  if (nrow(result_matrix) != total_time || (!is.null(n_clusters) && ncol(result_matrix) != n_clusters)) {
       warning("[matrix_concat] Final matrix dimensions seem inconsistent with expectations after rbind. Check results carefully.")
  }

  return(result_matrix)
})


#' Constructor for H5ClusterExperiment Objects
#'
#' @description
#' Creates a new `H5ClusterExperiment` object, representing a collection of
#' clustered neuroimaging runs sharing a common HDF5 file, mask, and cluster map.
#'
#' This function handles opening the HDF5 file (if a path is provided),
#' identifying available scans, and creating the appropriate run objects
#' (`H5ClusterRun` or `H5ClusterRunSummary`) for each scan based on
#' the available data within the HDF5 file structure (following the
#' ClusteredTimeSeriesSpec).
#'
#' @param file Either a character string path to the HDF5 file or an
#'   open \code{H5File} object.
#' @param scan_names (Optional) A character vector specifying which scans under `/scans/`
#'   to include in the experiment. If `NULL` (default), the constructor attempts
#'   to discover all available scan groups under `/scans/`.
#' @param mask (Optional) A `LogicalNeuroVol` object for the brain mask. If `NULL`,
#'   the constructor attempts to load it from `/mask` in the HDF5 file.
#' @param clusters (Optional) A `ClusteredNeuroVol` object for cluster assignments.
#'   If `NULL`, the constructor attempts to load it from `/cluster_map` and
#'   potentially `/voxel_coords` in the HDF5 file.
#' @param scan_metadata (Optional) A list to override or supplement metadata read
#'   from the HDF5 file. If provided, its length should match the number of scans.
#' @param cluster_metadata (Optional) A data.frame to override or supplement
#'   cluster metadata read from `/clusters/cluster_meta` in the HDF5 file.
#' @param summary_preference (Optional) Character string controlling which run type to load.
#'   If \code{NULL} (default), the constructor reads the \code{/scans@summary_only}
#'   attribute to choose a default: \code{"require"} if \code{summary_only=TRUE},
#'   \code{"ignore"} if \code{summary_only=FALSE}, otherwise \code{"prefer"}.
#'   Explicit values can be "require" (only load summary runs, error if missing),
#'   "prefer" (load summary if available, else full), or "ignore" (load full runs only).
#'   This influences whether \code{make_run_summary} or \code{make_run_full} is called.
#'   *Note: This parameter requires careful implementation based on HDF5 content checks.*
#' @param keep_handle_open (Logical) Only relevant if \code{file_source} is a path.
#'   If \code{TRUE} (default), the HDF5 file handle is kept open within the returned
#'   object. If \code{FALSE}, the handle is closed after reading metadata.
#'   *Note:* For most operations, the handle needs to remain open.
#'
#' @return A new \code{H5ClusterExperiment} object.
#' @importFrom hdf5r H5File list.groups H5A H5D h5attr h5attr_names
#' @importFrom methods new is
#' @export
#' @family H5Cluster
H5ClusterExperiment <- function(file,
                                  scan_names = NULL,
                                  mask = NULL,
                                  clusters = NULL,
                                  scan_metadata = NULL,
                                  cluster_metadata = NULL,
                                  summary_preference = NULL,
                                  keep_handle_open = TRUE # TODO: Implement finalizer logic if TRUE
                                  ) {

  # Rename file_source to file internally for clarity
  file_source <- file
  rm(file) # Remove the old name from the scope

  h5obj <- NULL
  opened_here <- FALSE
  # Track objects opened within this function scope for cleanup
  opened_groups <- list() 
  opened_datasets <- list() 
  on.exit({
    # Close datasets first
    lapply(opened_datasets, function(ds) if (!is.null(ds) && is(ds, "H5D") && ds$is_valid) close_h5_safely(ds))
    # Then close groups
    lapply(opened_groups, function(grp) if (!is.null(grp) && is(grp, "H5Group") && grp$is_valid) close_h5_safely(grp))
    # Finally, close file if opened here and not keeping open
    if (opened_here && !keep_handle_open && !is.null(h5obj) && h5obj$is_valid) {
         close_h5_safely(h5obj)
    }
    # TODO: Add finalizer registration if keep_handle_open is TRUE
  }, add = TRUE)

  # --- 1. Handle File Source and Read Header/Create Space ---
  fh <- open_h5(file_source, mode = "r")
  h5obj <- fh$h5
  # Defer closing the file only if we opened it and the user doesn't want to keep it open.
  # Note: keep_handle_open is TRUE by default, so defer usually won't close it here.
  # The H5ClusterExperiment object's finalizer handles closing if keep_handle_open=TRUE.
  defer({
      if (fh$owns && !keep_handle_open && h5obj$is_valid) {
          message("[H5ClusterExperiment] Closing HDF5 file handle opened by constructor.")
          try(h5obj$close_all(), silent = TRUE)
      }
  }, envir = parent.frame()) # Defer in the context of the calling function H5ClusterExperiment

  # --- Read Header Info and Create Master NeuroSpace ---
  master_space <- NULL
  hdr_grp <- NULL
  tryCatch({
      hdr_grp_path <- "/header"
      if (!h5obj$exists(hdr_grp_path)) stop("Required '/header' group not found in HDF5 file.")
      hdr_grp <- h5obj[[hdr_grp_path]]; opened_groups[["header"]] <- hdr_grp
      
      .rd_hdr <- function(nm) {
         d <- NULL; val <- NULL
         tryCatch({
             if (hdr_grp$exists(nm)) {
                 d <- hdr_grp[[nm]]; val <- d[]
             }
         }, finally = { if (!is.null(d) && d$is_valid) d$close() })
         return(val)
      }
      
      dims   <- .rd_hdr("dim")
      pixdim <- .rd_hdr("pixdim")
      qb     <- .rd_hdr("quatern_b"); qc <- .rd_hdr("quatern_c"); qd <- .rd_hdr("quatern_d")
      qx     <- .rd_hdr("qoffset_x"); qy <- .rd_hdr("qoffset_y"); qz <- .rd_hdr("qoffset_z")
      qfac   <- .rd_hdr("qfac")

      if (is.null(dims) || length(dims) < 4 || dims[1] != 4) {
         stop("Invalid or missing 'dim' in /header. Must start with 4 and have at least X,Y,Z dims.")
      }
      XYZ_dims <- dims[2:4]
      
      if (!is.null(pixdim) && length(pixdim) >= 4) {
        spacing_dims <- pixdim[2:4]
      } else {
        warning("Missing or incomplete 'pixdim' in header. Using default spacing (1,1,1).")
        spacing_dims <- c(1,1,1)
      }
      
      qfac_val <- qfac %||% 1.0
      if (!all(sapply(list(qb, qc, qd, qx, qy, qz), function(x) !is.null(x) && is.numeric(x)))) {
         warning("Missing or non-numeric quaternion parameters in header. Using default transform.")
         transform_mat <- diag(4)
         transform_mat[1,1] <- spacing_dims[1]; transform_mat[2,2] <- spacing_dims[2]; transform_mat[3,3] <- spacing_dims[3]
         origin_vec <- c(qx %||% 0, qy %||% 0, qz %||% 0)
         transform_mat[1:3, 4] <- origin_vec # NeuroSpace uses corner origin convention
      } else {
         transform_mat <- tryCatch(
             neuroim2::quaternToMatrix(
                 quat     = c(qb,qc,qd),
                 origin   = c(qx,qy,qz),
                 stepSize = spacing_dims,
                 qfac     = qfac_val
             ),
             error = function(e) {
                 warning("Error calling quaternToMatrix: ", e$message, ". Using default transform.")
                 mat_fallback <- diag(4)
                 mat_fallback[1,1] <- spacing_dims[1]; mat_fallback[2,2] <- spacing_dims[2]; mat_fallback[3,3] <- spacing_dims[3]
                 mat_fallback
             }
         )
      }
      master_space <- NeuroSpace(dim = XYZ_dims, spacing = spacing_dims, trans = transform_mat)
      
  }, error = function(e) {
      stop(sprintf("[H5ClusterExperiment] Failed to read header and create NeuroSpace: %s", e$message))
  })
  # Header group is automatically closed by on.exit if added to opened_groups

  # --- Auto-load/Validate mask and clusters, using the master_space --- 
  # Cache cluster map length for validation
  cmap_len <- NULL
  cmap_dset <- NULL
  tryCatch({
      cmap_path <- "/cluster_map"
      if(!h5obj$exists(cmap_path)) stop("Required dataset '/cluster_map' not found.")
      cmap_dset <- h5obj[[cmap_path]]; opened_datasets[["cmap"]] <- cmap_dset
      # Get length without reading the whole vector if possible (use H5S V1 API)
      cmap_space <- cmap_dset$get_space()
      cmap_len <- cmap_space$get_simple_extent_dims()$dims
  }, error = function(e) {
      stop(sprintf("Failed to get length of /cluster_map: %s", e$message))
  }) 
  # cmap_dset is closed via on.exit
  if (is.null(cmap_len) || length(cmap_len) != 1) {
     stop("/cluster_map must be a 1D dataset.")
  }

  # --- Validate/Load MASK --- 
  initial_mask_arg <- mask # Capture the original mask argument from the function call
  
  # ensure_mask will validate a provided mask or load one from HDF5 if initial_mask_arg is NULL.
  # The 'mask' variable is updated with the result.
  mask <- ensure_mask(initial_mask_arg, h5obj, master_space, path = "/mask")
  
  # If the user originally provided a mask (initial_mask_arg was not NULL),
  # then perform stricter HDF5-based validation on the 'mask' object
  # (which is now the user-provided mask, assuming it passed ensure_mask's checks).
  if (!is.null(initial_mask_arg)) {
    .validate_user_provided_mask_h5(user_mask = mask, 
                                    h5_handle = h5obj, 
                                    expected_cmap_len = cmap_len, 
                                    master_space = master_space)
  }
  # 'mask' now holds the final, validated mask for subsequent use.
  
  # --- Validate/Load CLUSTERS --- 
  if (is.null(clusters)) {
      message("Clusters argument is NULL, attempting to read from HDF5 file (/cluster_map).")
      # The helper `read_h5_clusters_to_ClusteredNeuroVol` already performs length check against sum(mask)
      # and uses space(mask). Mask is now the final, validated mask object.
      clusters <- read_h5_clusters_to_ClusteredNeuroVol(h5obj, mask, "/cluster_map") 
  } else {
      # CLUSTERS PROVIDED - Perform validation
      message("Clusters argument provided. Validating against mask...")
      # 1. Space check
      if (!identical(space(clusters), space(mask))) {
          stop("Provided clusters object's NeuroSpace does not match the mask's NeuroSpace.")
      }
      # 2. Length check
      if (length(clusters@clusters) != sum(mask)) {
         stop(sprintf("Provided clusters vector length (%d) does not match the number of TRUE voxels in the mask (%d).",
                      length(clusters@clusters), sum(mask)))
      }
      message("Provided clusters validation successful.")
      # Use the validated provided clusters
  }
  # --- End Auto-load/Validation ---

  # --- 2. Determine Scan Names ---
  scans_group_path <- "/scans"
  if (!h5obj$exists(scans_group_path)) {
    stop(sprintf("[H5ClusterExperiment] Scans group not found at '%s'.", scans_group_path))
  }
  scans_group <- h5obj[[scans_group_path]]; opened_groups[["scans"]] <- scans_group

  # Read summary_only attribute if present
  summary_only_attr <- NULL
  if ("summary_only" %in% h5attr_names(scans_group)) {
    summary_only_attr <- tryCatch(h5attr(scans_group, "summary_only"),
                                  error = function(e) NULL)
  }

  if (is.null(summary_preference)) {
    if (isTRUE(summary_only_attr)) {
      summary_preference <- "require"
    } else if (identical(summary_only_attr, FALSE)) {
      summary_preference <- "ignore"
    } else {
      summary_preference <- "prefer"
    }
  }
  
  # Use names() to list only direct members of /scans
  available_scans <- tryCatch(names(scans_group), error=function(e) character(0))
  
  if (is.null(scan_names)) {
    if (length(available_scans) == 0) {
      warning("[H5ClusterExperiment] No scan groups found under '/scans'.")
      scan_names <- character(0)
    } else {
      scan_names <- available_scans
    }
  } else {
    if (!is.character(scan_names) || length(scan_names) == 0) {
        stop("[H5ClusterExperiment] Provided 'scan_names' must be a non-empty character vector.")
    }
    missing_scans <- setdiff(scan_names, available_scans)
    if (length(missing_scans) > 0) {
      stop(sprintf("[H5ClusterExperiment] Specified scan names not found under '/scans': %s",
                   paste(missing_scans, collapse=", ")))
    }
    # Keep only requested scans in the specified order
  }

  # --- 5. Create Run Objects and Load Scan Metadata ---
  runs_list <- vector("list", length(scan_names))
  names(runs_list) <- scan_names
  final_scan_metadata <- vector("list", length(scan_names))
  names(final_scan_metadata) <- scan_names

  found_full <- FALSE
  found_summary <- FALSE

  for (sname in scan_names) {
    scan_path <- file.path(scans_group_path, sname)
    has_full_data = h5obj$exists(file.path(scan_path, "clusters"))
    has_summary_data = h5obj$exists(file.path(scan_path, "clusters_summary")) # Check group first
    summary_dset_name <- "summary_data" # Default, could be made configurable
    summary_dset_exists <- FALSE
    summary_group_path <- file.path(scan_path, "clusters_summary") # Define path here
    
    if (has_summary_data) {
         summary_group <- NULL
         tryCatch({
             # Only need to check existence, H5ClusterRunSummary will open group/dataset
             # summary_group <- h5obj[[summary_group_path]]; opened_groups[[paste0(sname,"_summary")]] <- summary_group
             summary_group <- h5obj[[summary_group_path]] # Open just to check dataset existence
             summary_dset_exists <- summary_group$exists(summary_dset_name)
         }, error = function(e) {
             warning(sprintf("Error checking summary dataset for scan %s: %s", sname, e$message))
         }, finally = {
             # Close the group if we opened it just for the check
             if (!is.null(summary_group) && inherits(summary_group, "H5Group") && summary_group$is_valid) {
                 close_h5_safely(summary_group)
             }
         })
    }

    found_full <- found_full || has_full_data
    found_summary <- found_summary || summary_dset_exists
    
    create_summary <- FALSE
    if (summary_preference == "require") {
        if (!summary_dset_exists) stop(sprintf("Summary data required but not found for scan '%s' at %s", sname, file.path(summary_group_path, summary_dset_name)))
        create_summary <- TRUE
    } else if (summary_preference == "prefer") {
        create_summary <- summary_dset_exists
    } else { # ignore
        create_summary <- FALSE
    }

    # --- Load Scan-Specific Metadata ---
    current_scan_meta <- list()
    meta_path <- file.path(scan_path, "metadata")
    meta_grp <- NULL
    if (h5obj$exists(meta_path)) {
        tryCatch({
            meta_grp <- h5obj[[meta_path]]; opened_groups[[paste0(sname,"_meta")]] <- meta_grp
            meta_items <- list.datasets(meta_grp) # Use list.datasets
            for (item_name in meta_items) {
                 dset <- NULL
                 tryCatch({
                     dset <- meta_grp[[item_name]]
                     # Handle scalar vs array appropriately
                     val <- if(length(dset$dims) == 0 || is.null(dset$dims)) dset[1] else dset[]
                     current_scan_meta[[item_name]] <- val
                 }, error = function(e_read) {
                     warning(sprintf("Failed to read metadata item '%s' for scan '%s': %s", item_name, sname, e_read$message))
                 }, finally = {
                      if (!is.null(dset) && dset$is_valid) try(dset$close(), silent=TRUE)
                 })
            }
        }, error = function(e_grp) {
             warning(sprintf("Failed to access metadata group for scan '%s': %s", sname, e_grp$message))
        })
        # Meta group closed via on.exit
    }
    final_scan_metadata[[sname]] <- current_scan_meta
    # --- End Scan Metadata Loading ---

    # The new constructors handle cluster name/ID loading internally if needed.
    # scan_specific_cluster_names <- character() 
    # scan_specific_cluster_ids <- integer()

    if (create_summary && summary_dset_exists) {
        if (!has_summary_data) stop("Internal logic error: create_summary is TRUE but has_summary_data is FALSE") # Should not happen
        tryCatch({
            # Use the canonical constructor
            runs_list[[sname]] <- H5ClusterRunSummary(
              file = h5obj, # MODIFIED: Pass H5File handle
              scan_name = sname,
              mask = mask,       # Pass validated mask
              clusters = clusters, # Pass validated clusters (can be NULL)
              # cluster_names = character(), # Let constructor handle defaults/loading
              # cluster_ids = integer(),     # Let constructor handle defaults/loading
              summary_dset = summary_dset_name
            )
        }, error=function(e) stop(sprintf("Failed to create H5ClusterRunSummary for scan '%s': %s", sname, e$message)))

    } else {

        if (!has_full_data) stop(sprintf("Full voxel data required but not found for scan '%s' under group %s", sname, file.path(scan_path, "clusters")))
        
        # Get n_time from metadata if available, else pass NULL (constructor will try to find it)
        scan_specific_n_time <- current_scan_meta$n_time %||% NULL 

        tryCatch({
             # Use the canonical constructor
             runs_list[[sname]] <- H5ClusterRun(
               file = h5obj, # MODIFIED: Pass H5File handle
               scan_name = sname,
               mask = mask,       # Pass validated mask
               clusters = clusters, # Pass validated clusters
               n_time = scan_specific_n_time # Pass potential n_time from metadata
               # compress = ? # Let constructor handle reading compress attr if needed
             )
         }, error=function(e) stop(sprintf("Failed to create H5ClusterRun for scan '%s': %s", sname, e$message)))
    }
  }

  if (!is.null(summary_only_attr)) {
    if (isTRUE(summary_only_attr) && found_full) {
      warning("[H5ClusterExperiment] '/scans@summary_only' is TRUE but full data was found in at least one scan.")
    }
  }

  # --- 6. Load Global Cluster Metadata ---
  if (is.null(cluster_metadata)) {
      clusters_group_path <- "/clusters"
      cluster_meta_group_path <- file.path(clusters_group_path, "cluster_meta")
      global_meta_df <- data.frame()
      if (tryCatch(h5obj$exists(cluster_meta_group_path), error=function(e) FALSE)) {
          meta_obj <- NULL
          tryCatch({
              meta_obj <- h5obj[[cluster_meta_group_path]]; opened_groups[["global_cluster_meta"]] <- meta_obj

              if (is(meta_obj, "H5Group")) {
                  meta_datasets <- list.datasets(meta_obj)
                  if (length(meta_datasets) > 0) {
                      meta_list <- list()
                      for (dname in meta_datasets) {
                          dset <- NULL
                          tryCatch({
                              dset <- meta_obj[[dname]]
                              meta_list[[dname]] <- if(length(dset$dims) == 0 || is.null(dset$dims)) dset[1] else dset[]
                          }, error = function(e_read) {
                              warning(sprintf("Failed to read global cluster metadata dataset '%s': %s", dname, e_read$message))
                          }, finally = {
                              if (!is.null(dset) && dset$is_valid) try(dset$close(), silent=TRUE)
                          })
                      }
                      lens <- vapply(meta_list, length, integer(1))
                      if (length(unique(lens)) == 1) {
                          global_meta_df <- as.data.frame(meta_list)
                      } else {
                          warning("Global cluster metadata datasets have inconsistent lengths. Returning as a list.")
                          global_meta_df <- meta_list
                      }
                  }
              } else if (is(meta_obj, "H5D")) {
                  meta_data <- NULL
                  tryCatch({
                      meta_data <- meta_obj$read()
                  }, error=function(e_read) {
                      warning(sprintf("Failed to read global cluster metadata dataset '%s': %s", cluster_meta_group_path, e_read$message))
                  })
                  if (!is.null(meta_data)) {
                      if (is.data.frame(meta_data)) global_meta_df <- meta_data else global_meta_df <- as.data.frame(meta_data)
                  }
                  if (!is.null(meta_obj) && meta_obj$is_valid) try(meta_obj$close(), silent=TRUE)
                  opened_groups[["global_cluster_meta"]] <- NULL
              } else {
                  warning(sprintf("Object '%s' is neither an H5Group nor H5D (class '%s').", cluster_meta_group_path, class(meta_obj)[1]))
                  if (!is.null(meta_obj) && inherits(meta_obj, "H5RefClass") && meta_obj$is_valid) {
                     try(meta_obj$close(), silent=TRUE)
                     opened_groups[["global_cluster_meta"]] <- NULL
                  }
              }
          }, error = function(e_grp) {
               warning(sprintf("Error reading global cluster metadata from '%s': %s", cluster_meta_group_path, e_grp$message))
          })
      }
      cluster_metadata <- global_meta_df
  } else {
      if (!is.data.frame(cluster_metadata)) stop("[H5ClusterExperiment] Provided 'cluster_metadata' must be a data.frame.")
      # User provided metadata overrides loaded
  }
  
  # --- 7. Override scan metadata if provided ---
  if (!is.null(scan_metadata)) {
       if (!is.list(scan_metadata) || length(scan_metadata) != length(runs_list)) {
            stop(sprintf("[H5ClusterExperiment] Provided 'scan_metadata' must be a list of length %d.", length(runs_list)))
       }
       # Merge/replace carefully. For now, just replace.
       # Ensure names match if replacing
       if (!identical(sort(names(scan_metadata)), sort(names(final_scan_metadata)))) {
           warning("Names in provided 'scan_metadata' do not match the loaded scan names. Using provided names.")
       }
       final_scan_metadata <- scan_metadata 
       names(final_scan_metadata) <- names(scan_metadata) # Use names from provided metadata
  }

  # --- 8. Create Experiment Object ---
  exp_obj <- new("H5ClusterExperiment",
                 runs = runs_list,
                 scan_metadata = final_scan_metadata,
                 cluster_metadata = cluster_metadata)

  # --- Add Finalizer if needed (Checklist Item 4.3) ---
  # Removed finalizer logic for simplicity. User manages handle if keep_handle_open=TRUE.
  # if (opened_here && keep_handle_open) {
  #   message("Registering finalizer to close HDF5 handle when experiment object is garbage collected.")
  #   reg.finalizer(exp_obj, function(e) {
  #     # Access the handle from the object's structure (assuming it's consistent)
  #     # This relies on the handle being stored accessible within the object, e.g., exp_obj@runs[[1]]@obj
  #     h5_handle_to_close <- NULL
  #     if (length(e@runs) > 0 && inherits(e@runs[[1]], "H5ClusteredArray")) {
  #        h5_handle_to_close <- e@runs[[1]]@obj
  #     }
  #     if (!is.null(h5_handle_to_close) && is_h5file(h5_handle_to_close) && h5_handle_to_close$is_valid) {
  #        try(h5_handle_to_close$close_all(), silent = TRUE)
  #     }
  #   }, onexit = TRUE)
  # } else if (opened_here && !keep_handle_open) {
  #     # Close handle immediately if opened here and not keeping open
  #     try(h5obj$close_all(), silent = TRUE)
  # } # else: handle was passed in open, user manages its lifecycle
  # --- End Finalizer ---

  # If file was opened here and we are NOT keeping it open, close it now.
  if (opened_here && !keep_handle_open) {
      try(h5obj$close_all(), silent = TRUE)
  }

  return(exp_obj)
}


#' Accessor Methods for H5ClusterExperiment

#' Access Slots/Properties using `$`
#'
#' Provides convenient access to shared properties like `mask`, `clusters`,
#' and the underlying HDF5 file object (`obj`) by retrieving them from the
#' first run object stored within the experiment.
#' Also provides access to the experiment's own slots (`runs`, `scan_metadata`, `cluster_metadata`).
#'
#' @param x An `H5ClusterExperiment` object.
#' @param name The name of the property or slot to access (`mask`, `clusters`, `obj`, `runs`, `scan_metadata`, `cluster_metadata`).
#'
#' @return The requested object or value.
#' @export
#' @family H5Cluster
setMethod("$", "H5ClusterExperiment", function(x, name) {
    # Check for experiment's own slots first
    if (name %in% slotNames(x)) {
        return(slot(x, name))
    }
    
    # Check for convenient accessors from the first run
    if (name %in% c("mask", "clusters", "obj", "h5file", "n_voxels")) {
        if (length(x@runs) == 0) {
            stop(sprintf("Cannot access property '%s': Experiment contains no runs.", name))
        }
        first_run <- x@runs[[1]]
        if (name == "h5file") name <- "obj" # Map generic to slot name
        if (!hasMethod("slot", signature = class(first_run)) || !(name %in% slotNames(first_run))){
             stop(sprintf("Cannot access property '%s': Slot not found in the first run object of class '%s'.", name, class(first_run)[1]))
        }
        return(slot(first_run, name))
    } 
    
    # Default behavior or error for unknown names
    # stop(sprintf("'%s' is not a valid slot or accessible property for H5ClusterExperiment", name))
    # Returning NULL might be safer than erroring? Or follow default $ behavior if possible.
    return(NULL) 
})

#' Get the HDF5 file object via generic
#' @param x H5ClusterExperiment object
#' @return The HDF5 file object from the first run.
#' @rdname h5file-methods
#' @export
#' @family H5Cluster
setMethod("h5file", "H5ClusterExperiment", function(x) {
    if (length(x@runs) == 0) {
        stop("Cannot get HDF5 file object: Experiment contains no runs.")
    }
    # Assumes all runs share the same handle (enforced by validity check)
    x@runs[[1]]@obj 
})

#' Get the mask object via generic
#' @param x H5ClusterExperiment object
#' @return The mask object from the first run.
#' @rdname mask-methods
#' @export
#' @family H5Cluster
setMethod("mask", "H5ClusterExperiment", function(x) {
     if (length(x@runs) == 0) {
        stop("Cannot get mask object: Experiment contains no runs.")
    }
    x@runs[[1]]@mask
})

#' Get the clusters object via generic
#' @param x H5ClusterExperiment object
#' @return The clusters object from the first run.
#' @rdname clusters-methods
#' @export
#' @family H5Cluster
setMethod("clusters", "H5ClusterExperiment", function(x) {
     if (length(x@runs) == 0) {
        stop("Cannot get clusters object: Experiment contains no runs.")
    }
    x@runs[[1]]@clusters
})

#' Get scan names
#' @param x H5ClusterExperiment object
#' @return Character vector of scan names.
#' @rdname scan_names-methods
#' @export
#' @family H5Cluster
setMethod("scan_names", "H5ClusterExperiment", function(x) {
    names(x@runs) %||% character(0)
})

#' Get number of scans
#' @param x H5ClusterExperiment object
#' @return Integer number of scans.
#' @rdname n_scans-methods
#' @export
#' @family H5Cluster
setMethod("n_scans", "H5ClusterExperiment", function(x) {
    length(x@runs)
})

#' Get scan metadata
#' @param x H5ClusterExperiment object
#' @return List of scan metadata.
#' @rdname scan_metadata-methods
#' @export
#' @family H5Cluster
setMethod("scan_metadata", "H5ClusterExperiment", function(x) {
    x@scan_metadata
})

#' Get cluster metadata
#' @param x H5ClusterExperiment object
#' @return Data frame of cluster metadata.
#' @rdname cluster_metadata-methods
#' @export
#' @family H5Cluster
setMethod("cluster_metadata", "H5ClusterExperiment", function(x) {
    x@cluster_metadata
})

#' @rdname show-methods
#' @importFrom methods show
setMethod("show", "H5ClusterExperiment", function(object) {
  cat("\nH5ClusterExperiment\n")
  cat("  # runs        :", length(object@runs), "\n")
  # Use %||% which should be defined in io_h5_helpers.R now
  num_clusters <- length(object@cluster_metadata$cluster_id) %||% 
                  (if(length(object@runs)>0) length(object@runs[[1]]@cluster_ids) else NA)
  cat("  # clusters    :", num_clusters %||% "Unknown", "\n")
  if (length(object@runs) > 0) {
     cat("  mask dims     :", paste(dim(object@runs[[1]]@mask), collapse="x"), "\n")
     h5_handle <- object@runs[[1]]@obj
     file_path <- if(is_h5file(h5_handle) && h5_handle$is_valid) h5_handle$get_filename() else "Invalid/Closed Handle"
     cat("  HDF5 file     :", file_path, "\n\n")
  } else {
     cat("  mask dims     : Unknown (no runs loaded)\n")
     cat("  HDF5 file     : Unknown (no runs loaded)\n\n")
  }
})

# TODO: Add show method for H5ClusterExperiment
</file>

<file path="R/constructors.R">
#' Constructor for H5ClusterRun Objects
#' 
#' @description
#' Creates a new \code{H5ClusterRun} object, representing a single run of full
#' voxel-level clustered data from an HDF5 file.
#' 
#' This function handles file opening/closing and reads necessary metadata like
#' \code{n_time} from the HDF5 file if not provided explicitly.
#' 
#' @param file Character string path to the HDF5 file.
#' @param scan_name The character string name of the scan (e.g., "run1").
#' @param mask A \code{LogicalNeuroVol} object for the brain mask.
#' @param clusters A \code{ClusteredNeuroVol} object for cluster assignments.
#' @param n_time (Optional) The number of time points. If \code{NULL} (default), the function
#'   will attempt to read it from the HDF5 file attributes or metadata dataset.
#' @param compress (Optional) Logical indicating compression status (metadata).
#' 
#' @return A new \code{H5ClusterRun} object with an open file handle managed by the object.
#' @importFrom methods new is
#' @importFrom hdf5r h5attr
#' @importFrom withr defer
#' @export
H5ClusterRun <- function(file, scan_name, 
                               mask, clusters,
                               n_time = NULL, compress = FALSE) {
  
  # --- 1. Argument Validation (Basic) --- 
  if (!((is.character(file) && length(file) == 1 && nzchar(file)) || inherits(file, "H5File"))) {
    stop("[H5ClusterRun] 'file' must be a non-empty character string path or an H5File object.")
  }
  if (!is.character(scan_name) || length(scan_name) != 1 || !nzchar(scan_name)) {
    stop("[H5ClusterRun] 'scan_name' must be a non-empty character string.")
  }
  if (!is(mask, "LogicalNeuroVol")) {
    stop("[H5ClusterRun] 'mask' must be a LogicalNeuroVol object.")
  }
  if (!is(clusters, "ClusteredNeuroVol")) {
     stop("[H5ClusterRun] 'clusters' must be a ClusteredNeuroVol object.")
  }
  # Dimension consistency check (using helper)
  check_same_dims(mask, clusters, dims_to_compare = 1:3, 
                  msg = "[H5ClusterRun] Dimensions of 'mask' and 'clusters' must match.")
  
  n_vox <- sum(mask)
  if (!identical(length(clusters@clusters), as.integer(n_vox))) {
    stop(sprintf(
      "[H5ClusterRun] Mismatch: clusters@clusters length (%d) != sum(mask) (%d).",
      length(clusters@clusters), n_vox
    ))
  }

  # --- 2. Open HDF5 file --- 
  # Use open_h5 with mode 'r' by default. It handles file existence check.
  # Note: open_h5 now returns a list(h5=handle, owns=TRUE/FALSE)
  fh <- open_h5(file, mode = "r") 
  h5obj <- fh$h5
  # We *don't* defer closure here. The returned object will own the handle.
  
  determined_n_time <- n_time # Store original or NULL
  
  # --- 3. Determine n_time (if NULL) --- 
  # This logic is copied/adapted from make_run_full
  if (is.null(determined_n_time)) {
    scan_group_path <- paste0("/scans/", scan_name)
    scan_group <- NULL
    ds <- NULL
    
    scan_group_exists <- tryCatch(h5obj$exists(scan_group_path), error = function(e) FALSE)
    
    if (scan_group_exists) {
      tryCatch({
        scan_group <- h5obj[[scan_group_path]]
        on.exit(if(!is.null(scan_group) && inherits(scan_group, "H5Group") && scan_group$is_valid) try(scan_group$close()), add = TRUE)
        
        if ("n_time" %in% hdf5r::h5attr_names(scan_group)) { 
          determined_n_time <- hdf5r::h5attr(scan_group, "n_time")
        } else if (tryCatch(scan_group$exists("metadata/n_time"), error = function(e) FALSE)) {
             md_time_ds <- NULL
             tryCatch({
                  md_time_ds <- scan_group[["metadata/n_time"]]
                  determined_n_time <- md_time_ds$read()
             }, finally = {
                  if(!is.null(md_time_ds) && md_time_ds$is_valid) try(md_time_ds$close())
             })
        } else if (tryCatch(scan_group$exists("clusters"), error = function(e) FALSE)) {
          first_cid <- clusters@clusters[1] # Assumes at least one cluster
          if (!is.null(first_cid) && !is.na(first_cid)) {
            dset_path_cid1 <- sprintf("/scans/%s/clusters/cluster_%d", scan_name, as.integer(first_cid))
            if (tryCatch(h5obj$exists(dset_path_cid1), error = function(e) FALSE)) {
               dset_cid1 <- NULL
               tryCatch({
                    dset_cid1 <- h5obj[[dset_path_cid1]]
                    dims <- dset_cid1$dims
                    if (length(dims) == 2) {
                        determined_n_time <- dims[2]
                        message(sprintf("[H5ClusterRun] Inferred n_time = %d from dataset '%s'.", determined_n_time, dset_path_cid1))
                    }
               }, finally = {
                    if(!is.null(dset_cid1) && dset_cid1$is_valid) try(dset_cid1$close())
               })
            }
          }
        }
      }, error = function(e) {
        # Close scan_group if opened before error
        if (!is.null(scan_group) && scan_group$is_valid) try(scan_group$close())
        warning(sprintf("[H5ClusterRun] Error reading n_time metadata for scan '%s': %s. Proceeding without inferred n_time.", scan_name, e$message))
      })
    }
    
    if (is.null(determined_n_time)) {
        # Close the file handle we opened if we couldn't determine n_time
        if (fh$owns) try(h5obj$close_all(), silent = TRUE)
        stop(sprintf("[H5ClusterRun] Could not determine 'n_time' for scan '%s'. Provide it explicitly or ensure it exists in HDF5 attributes/metadata.", scan_name))
    }
  }
  
  # --- 4. Final n_time validation ---
  if (!is.numeric(determined_n_time) || length(determined_n_time) != 1 || determined_n_time <= 0 || floor(determined_n_time) != determined_n_time) {
    # Close the file handle before stopping
    if (fh$owns) try(h5obj$close_all(), silent = TRUE)
    stop(sprintf("[H5ClusterRun] Determined 'n_time' (%s) must be a single positive integer.", as.character(determined_n_time)))
  }
  final_n_time <- as.integer(determined_n_time)
  
  # --- 5. Create the object --- 
  # The object takes ownership of the h5obj handle. 
  # No defer() needed here because we are returning the object containing the handle.
  # The object's finalizer (if defined) or manual closing should handle it later.
  new_obj <- tryCatch({
       new("H5ClusterRun",
           obj       = h5obj, # Pass the open handle
           scan_name = scan_name,
           mask      = mask,
           clusters  = clusters,
           n_voxels  = as.integer(n_vox),
           n_time    = final_n_time,
           compress  = as.logical(compress)
       )
   }, error = function(e) {
       # If new() fails, close the handle we opened.
       if (fh$owns) try(h5obj$close_all(), silent = TRUE)
       stop(sprintf("[H5ClusterRun] Failed to create object: %s", e$message))
   })
  
  # Return the created object, which now manages the H5 handle.
  return(new_obj)
} 

#' Constructor for H5ClusterRunSummary Objects
#' 
#' @description
#' Creates a new \code{H5ClusterRunSummary} object, representing a single run of
#' summary cluster time-series data from an HDF5 file.
#' 
#' This function handles file opening/closing, validates the summary dataset,
#' determines \code{n_time} from the dataset dimensions, and reconciles cluster names/IDs.
#'
#' @param file Character string path to the HDF5 file.
#' @param scan_name The character string name of the scan (e.g., "run1").
#' @param mask A \code{LogicalNeuroVol} object for the brain mask (used for reference/consistency).
#' @param clusters (Optional) A \code{ClusteredNeuroVol} object for cluster assignments. If \code{NULL},
#'   cluster names/IDs must be provided or derivable from the dataset.
#' @param cluster_names (Optional) Character vector of names for the clusters.
#' @param cluster_ids (Optional) Integer vector of IDs for the clusters.
#' @param summary_dset (Optional) The name of the dataset within the run's summary group
#'   (default: "summary_data").
#'
#' @return A new \code{H5ClusterRunSummary} object with an open file handle managed by the object.
#' @importFrom methods new is
#' @importFrom hdf5r H5D
#' @export
H5ClusterRunSummary <- function(file, scan_name,
                                mask, clusters = NULL,
                                cluster_names = character(), cluster_ids = integer(),
                                summary_dset = "summary_data") {

  # --- 1. Argument Validation (Basic) --- 
  if (!((is.character(file) && length(file) == 1 && nzchar(file)) || inherits(file, "H5File"))) {
    stop("[H5ClusterRunSummary] 'file' must be a non-empty character string path or an H5File object.")
  }
  if (!is.character(scan_name) || length(scan_name) != 1 || !nzchar(scan_name)) {
    stop("[H5ClusterRunSummary] 'scan_name' must be a non-empty character string.")
  }
  if (!is(mask, "LogicalNeuroVol")) {
    stop("[H5ClusterRunSummary] 'mask' must be a LogicalNeuroVol object.")
  }
  n_vox <- sum(mask)
  if (!is.null(clusters)) {
      if (!is(clusters, "ClusteredNeuroVol")) {
        stop("[H5ClusterRunSummary] 'clusters' must be a ClusteredNeuroVol object.")
      }
      check_same_dims(mask, clusters, dims_to_compare = 1:3,
                      msg = "[H5ClusterRunSummary] Dimensions of 'mask' and provided 'clusters' must match.")
      if (!identical(length(clusters@clusters), as.integer(n_vox))) {
        stop(sprintf(
          "[H5ClusterRunSummary] Mismatch: provided clusters@clusters length (%d) != sum(mask) (%d).",
          length(clusters@clusters), n_vox
        ))
      }
  }
  if (!is.character(summary_dset) || length(summary_dset) != 1 || !nzchar(summary_dset)) {
      stop("[H5ClusterRunSummary] 'summary_dset' must be a non-empty character string.")
  }

  # --- 2. Open HDF5 file --- 
  fh <- open_h5(file, mode = "r") 
  h5obj <- fh$h5
  # The returned object will own the handle. No defer needed here.
  
  # --- 3. Validate HDF5 structure before reading data --- 
  scan_group_path <- file.path("/scans", scan_name)
  summary_group_path <- file.path(scan_group_path, "clusters_summary")
  dset_path <- file.path(summary_group_path, summary_dset)
  
  tryCatch({
      assert_h5_path(h5obj, scan_group_path, "scan group")
      assert_h5_path(h5obj, summary_group_path, "summary group")
      assert_h5_path(h5obj, dset_path, "summary dataset")
  }, error = function(e) {
      if (fh$owns) try(h5obj$close_all(), silent = TRUE)
      stop(paste0("[H5ClusterRunSummary] ", e$message))
  })
  
  # --- 4. Read dataset dimensions and reconcile cluster info --- 
  final_n_time <- NA_integer_
  final_n_clusters <- NA_integer_
  final_cluster_names <- cluster_names
  final_cluster_ids <- cluster_ids

  dataset_dims <- NULL
  dataset_attrs <- NULL

  info_res <- tryCatch({
    with_h5_dataset(h5obj, dset_path, function(ds) {
      list(
        dims = ds$dims,
        names = if (ds$attr_exists("cluster_names")) ds$attr_read("cluster_names") else character(),
        ids   = if (ds$attr_exists("cluster_ids")) ds$attr_read("cluster_ids") else integer()
      )
    })
  }, error = function(e) {
    if (fh$owns) try(h5obj$close_all(), silent = TRUE)
    stop(sprintf("[H5ClusterRunSummary] Error processing summary dataset '%s': %s", dset_path, e$message))
  })

  dataset_dims <- info_res$dims
  if (length(dataset_dims) != 2) {
    if (fh$owns) try(h5obj$close_all(), silent = TRUE)
    stop(sprintf("Summary dataset '%s' does not have 2 dimensions (found %d).", dset_path, length(dataset_dims)))
  }
  final_n_time <- dataset_dims[1]
  final_n_clusters <- dataset_dims[2]

  if (length(final_cluster_names) == 0 && length(final_cluster_ids) == 0) {
    final_cluster_names <- info_res$names
    final_cluster_ids   <- info_res$ids

    if (length(final_cluster_names) > 0 && length(final_cluster_names) != final_n_clusters) {
      stop(sprintf("Read cluster_names length (%d) mismatch with dataset columns (%d).", length(final_cluster_names), final_n_clusters))
    }
    if (length(final_cluster_ids) > 0 && length(final_cluster_ids) != final_n_clusters) {
      stop(sprintf("Read cluster_ids length (%d) mismatch with dataset columns (%d).", length(final_cluster_ids), final_n_clusters))
    }

    if (length(final_cluster_names) == 0 && final_n_clusters > 0) {
      final_cluster_names <- paste0("Cluster", seq_len(final_n_clusters))
    }
    if (length(final_cluster_ids) == 0 && final_n_clusters > 0) {
      if (!is.null(clusters) && length(unique(clusters@clusters)) == final_n_clusters) {
        final_cluster_ids <- sort(unique(clusters@clusters))
      } else {
        final_cluster_ids <- seq_len(final_n_clusters)
      }
    }
  }

  if (length(final_cluster_names) != final_n_clusters) {
    stop(sprintf("Final cluster_names length (%d) does not match dataset columns (%d).", length(final_cluster_names), final_n_clusters))
  }
  if (length(final_cluster_ids) != final_n_clusters) {
    stop(sprintf("Final cluster_ids length (%d) does not match dataset columns (%d).", length(final_cluster_ids), final_n_clusters))
  }
  if (!is.integer(final_cluster_ids)) {
    final_cluster_ids <- as.integer(final_cluster_ids)
  }

  }, error = function(e) {
      if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) close_h5_safely(ds)
      if (fh$owns) try(h5obj$close_all(), silent = TRUE)
      stop(sprintf("[H5ClusterRunSummary] Error processing summary dataset '%s': %s", dset_path, e$message))
  }, finally = {
      if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) close_h5_safely(ds)
  })

  # --- 5. Create the object ---
  new_obj <- tryCatch({
    new("H5ClusterRunSummary",
        obj = h5obj, # Pass the open handle
        scan_name = scan_name,
        mask = mask,
        n_voxels = as.integer(n_vox), # Add n_voxels, inherited from H5ClusteredArray
        clusters = clusters, # Pass through, can be NULL
        n_time = as.integer(final_n_time),
        cluster_names = final_cluster_names,
        cluster_ids = final_cluster_ids,
        summary_dset = summary_dset
    )
  }, error = function(e){
    if (fh$owns) try(h5obj$close_all(), silent = TRUE)
    stop(sprintf("[H5ClusterRunSummary] Failed to create H5ClusterRunSummary object: %s", e$message))
  })
  
  return(new_obj)
}
</file>

<file path="R/cluster_array.R">
#' @include all_class.R
#' @import hdf5r
#' @import neuroim2
#' @importFrom withr defer
#' @importFrom methods new is
#' @importFrom lifecycle deprecate_warn
#' @importFrom neuroim2 series
NULL




#' @rdname mask-methods
#' @export
setMethod("mask", "H5ClusteredArray", function(x) x@mask)

#' @rdname clusters-methods
#' @export
setMethod("clusters", "H5ClusteredArray", function(x) x@clusters)


#' @rdname h5file-methods
#' @export
setMethod("h5file", "H5ClusteredArray", function(x) x@obj)


#' Get Cluster Time Series by Mask Index (Internal Helper)
#'
#' @description
#' Internal helper function to retrieve time series data for specific voxels
#' (specified by mask-relative indices) from their corresponding cluster datasets
#' in an HDF5 file.
#'
#' This function is designed to work with objects inheriting from `H5ClusteredArray`.
#' It uses the `.dataset_path` generic method (which must be implemented by concrete subclasses)
#' to locate the correct HDF5 dataset for each cluster.
#'
#' @param x An object inheriting from `H5ClusteredArray`.
#' @param mask_indices An integer vector of voxel indices relative to the mask (1 to `x@n_voxels`).
#' @param time_indices An integer vector specifying which time points to retrieve. If `NULL`, all time points are retrieved.
#' @param n_time The total number of time points for the specific run/scan being accessed.
#'
#' @return A numeric matrix of shape `[length(mask_indices), length(time_indices)]`
#'         containing the requested time series data.
#' @keywords internal
#' @noRd
.get_cluster_timeseries_by_mask_index <- function(x, mask_indices, time_indices = NULL, n_time) {
    # 1. Validate spatial indices 'mask_indices' (relative to mask)
    nVoxMask <- x@n_voxels # Use stored voxel count from H5ClusteredArray
    if (any(mask_indices < 1 | mask_indices > nVoxMask)) {
      stop(sprintf("[.get_cluster_timeseries] Indices in 'mask_indices' are out of the valid mask range [1..%d]", nVoxMask))
    }
    mask_indices <- as.integer(mask_indices)
    n_request <- length(mask_indices)
    full_time_length <- as.integer(n_time) # Use provided n_time

    # 2. Validate optional time_indices
    if (!is.null(time_indices)) {
      if (!is.numeric(time_indices) || any(time_indices < 1 | time_indices > full_time_length)) {
        stop(sprintf("[.get_cluster_timeseries] 'time_indices' are out of the valid time range [1..%d]", full_time_length))
      }
      time_indices <- as.integer(time_indices)
      n_time_request <- length(time_indices)
    } else {
      time_indices <- seq_len(full_time_length) # Default to all time points
      n_time_request <- full_time_length
    }

    if (n_request == 0 || n_time_request == 0) {
        return(matrix(numeric(0), nrow = n_request, ncol = n_time_request))
    }

    # 3. Prepare result matrix
    result_mat <- matrix(NA_real_, nrow = n_request, ncol = n_time_request)

    # 4. Get cluster assignments for *all* requested indices ONCE
    # Accessing clusters slot from H5ClusteredArray
    clus_ids_req <- tryCatch(x@clusters@clusters[mask_indices], error = function(e) {
        stop(sprintf("[.get_cluster_timeseries] Failed to get cluster assignments for provided indices. Error: %s", e$message))
    })

    # 5. Group requested indices by their cluster ID
    index_groups <- split(seq_len(n_request), clus_ids_req)
    needed_cluster_ids <- names(index_groups)
    # Removed base_path calculation

    # 6. Loop through needed clusters and read data
    for (cid_str in needed_cluster_ids) {
        cid <- as.integer(cid_str) # Cluster ID should be integer
        row_indices_in_result <- index_groups[[cid_str]]
        mask_indices_this_cluster_req <- mask_indices[row_indices_in_result]

        # === Use .dataset_path generic ===
        dset_path <- tryCatch(.dataset_path(x, cid),
                              error = function(e) {
                                  stop(sprintf("Failed to get dataset path for cluster %d. Subclass implementation error? Original error: %s", cid, e$message))
                              })
        # ==================================

        tryCatch({
            all_mask_indices_in_this_cluster <- which(x@clusters@clusters == cid)
            row_offsets_in_dataset <- match(mask_indices_this_cluster_req, all_mask_indices_in_this_cluster)

            if(any(is.na(row_offsets_in_dataset))) {
                stop(sprintf("Internal inconsistency: some requested voxels mapped to cluster %d but not found in its index list.", cid))
            }
            row_offsets_in_dataset <- as.integer(row_offsets_in_dataset)

            cluster_data_subset <- h5_read_subset(
                x@obj,
                dset_path,
                list(row_offsets_in_dataset, time_indices)
            )
            # Accessing obj slot from H5ClusteredArray
            assert_h5_path(x@obj, dset_path,
                           sprintf("Dataset for cluster %d", cid))
            ds <- x@obj[[dset_path]]
            # Simplified on.exit handling: rely on hdf5r GC or explicit closure later
            # on.exit(if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) ds$close(), add = TRUE, after = FALSE)

            cluster_data_subset <- ds[row_offsets_in_dataset, time_indices, drop = FALSE]

            # Close dataset handle immediately after reading
            if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) {
                 close_h5_safely(ds)
            }

            expected_rows <- length(row_indices_in_result)
            if (!is.matrix(cluster_data_subset) ||
                 nrow(cluster_data_subset) != expected_rows ||
                 ncol(cluster_data_subset) != n_time_request) {
                 stop(sprintf("Read data dimensions [%s] mismatch expected [%d,%d] for cluster %d",
                              paste(dim(cluster_data_subset), collapse=","), expected_rows, n_time_request, cid))
            }

            result_mat[row_indices_in_result, ] <- cluster_data_subset

        }, error = function(e) {
            if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) {
                close_h5_safely(ds)
            }
            stop(sprintf("[.get_cluster_timeseries] Failed processing cluster %d at path '%s'. Original error: %s", cid, dset_path, e$message))
        })
    } # End loop over clusters

    # 7. Final check for NAs
    if (any(is.na(result_mat))) {
        warning("[.get_cluster_timeseries] Result matrix contains NAs. Data reading may have failed for some clusters or voxels within clusters.")
    }

    return(result_mat)
}




#' @keywords internal
#' @noRd
setMethod(".dataset_path", "H5ClusteredArray",
          function(x, cid, ...) {
            stop(sprintf("Internal Error: .dataset_path method not implemented for class '%s'. Subclass must provide a method.", class(x)[1]))
          })

#' Get HDF5 Dataset Path for H5ClusterRun
#'
#' @description
#' Implementation of the `.dataset_path` generic for `H5ClusterRun` objects.
#' Constructs the path to a specific cluster dataset within the run's group in the HDF5 file,
#' typically `/scans/<scan_name>/clusters/cluster_<cid>`.
#'
#' @param x An `H5ClusterRun` object.
#' @param cid The cluster ID (integer).
#' @param ... Additional arguments (not used).
#'
#' @return A character string representing the HDF5 dataset path.
#' @keywords internal
#' @noRd
setMethod(".dataset_path", "H5ClusterRun",
          function(x, cid) {
            # Basic validation
            if (!is.numeric(cid) || length(cid) != 1 || floor(cid) != cid || cid <= 0) {
                stop("['.dataset_path', H5ClusterRun] Cluster ID 'cid' must be a single positive integer.")
            }
            if (!is.character(x@scan_name) || length(x@scan_name) != 1 || nchar(x@scan_name) == 0) {
                 stop("['.dataset_path', H5ClusterRun] Invalid 'scan_name' slot.")
            }
            
            # Construct the standard path
            sprintf("/scans/%s/clusters/cluster_%d", x@scan_name, as.integer(cid))
          })



#' @keywords internal
#' @noRd
.subset_h5crunfull <- function(x, i, j, k, l, drop = TRUE) {

  # Inherits obj, mask, clusters, n_voxels from H5ClusteredArray
  # Uses scan_name, n_time from H5ClusterRun
  dims   <- dim(x@mask)
  nvox   <- x@n_voxels
  nt     <- x@n_time # Use n_time from H5ClusterRun

  # ---------- 1.  default / normalise inputs -----------------------
  if (missing(j) || is.null(j)) j <- NULL
  if (missing(k) || is.null(k)) k <- NULL
  if (missing(l) || is.null(l)) l <- seq_len(nt)

  # ---------- 2.  fast path : mask-based indexing ------------------
  if (is.null(j) && is.null(k)) {
    if (is.numeric(i) && all(i >= 1) && all(i <= nvox)) {
      # treat i as mask indices
      # Note: pass n_time explicitly to the helper
      dat <- .get_cluster_timeseries_by_mask_index(x, mask_indices = as.integer(i),
                                                 time_indices = l, n_time = nt)
      if (drop) return(drop(dat))
      return(dat)
    }
    # Fall through: interpret i as x-coordinate
  }

  # ---------- 3.  coordinate path ---------------------------------
  if (missing(i) || is.null(i)) i <- seq_len(dims[1])
  if (is.null(j)) j <- seq_len(dims[2])
  if (is.null(k)) k <- seq_len(dims[3])

  if (!is.numeric(i) || !is.numeric(j) || !is.numeric(k) || !is.numeric(l)) {
      stop("[',H5ClusterRun'] Subscripts must be numeric or NULL.")
  }

  if (any(i < 1L | i > dims[1L]) ||
      any(j < 1L | j > dims[2L]) ||
      any(k < 1L | k > dims[3L]))
      stop("[',H5ClusterRun'] Spatial subscript out of bounds")

  if (any(l < 1L | l > nt))
      stop("[',H5ClusterRun'] Time subscript out of bounds")

  out <- array(0, dim = c(length(i), length(j), length(k), length(l)))

  grid <- as.matrix(expand.grid(i = i, j = j, k = k, KEEP.OUT.ATTRS = FALSE))
  lin  <- grid[,1] + (grid[,2]-1L)*dims[1] + (grid[,3]-1L)*dims[1]*dims[2]

  mask_array <- as.logical(as.array(x@mask))
  inside <- mask_array[lin]
  if (!any(inside)) return(if (drop) drop(out) else out)

  mask_lin <- which(mask_array)
  v_idx    <- match(lin[inside], mask_lin)

  # Note: pass n_time explicitly to the helper
  dat  <- .get_cluster_timeseries_by_mask_index(x, mask_indices = v_idx,
                                                time_indices = l, n_time = nt)

  idx <- grid[inside,, drop = FALSE]
  out_i <- match(idx[,1], i)
  out_j <- match(idx[,2], j)
  out_k <- match(idx[,3], k)

  for (t in seq_along(l))
    out[cbind(as.integer(out_i), as.integer(out_j), as.integer(out_k), t)] <- dat[, t]

  if (drop) drop(out) else out
}

#' Subset an H5ClusterRun Object
#'
#' @description
#' Extracts data from an \code{H5ClusterRun} object using array-like indexing.
#' Handles both coordinate-based and mask-index-based subsetting.
#'
#' @param x An \code{H5ClusterRun} object.
#' @param i Row index (x-coordinate or mask index).
#' @param j Column index (y-coordinate).
#' @param k Slice index (z-coordinate).
#' @param l Time index.
#' @param ... Not used.
#' @param drop Logical. If \code{TRUE}, the result is coerced to the lowest possible dimension.
#'
#' @return An array or vector containing the subset of data.
#' @export
#' @family H5Cluster
setMethod("[",
  signature(x = "H5ClusterRun", i = "ANY", j = "ANY", drop = "ANY"),
  function(x, i, j, k, l, ..., drop = TRUE)
      .subset_h5crunfull(x, i, j, k, l, drop))


#' @export
#' @family H5Cluster
setMethod("[",
  signature(x = "H5ClusterRun", i = "ANY", j = "missing", drop = "ANY"),
  definition = function(x, i, ..., drop = TRUE) {
       # Handle cases: x[mask_indices], x[coords_matrix], x[i,j,k] (falls through if l missing)
       # Let the main helper sort it out.
      .subset_h5crunfull(x, i = i, j = NULL, k = NULL, l = NULL, drop = drop)
  })


#' @export
#' @family H5Cluster
#' @rdname dim-methods
setMethod("dim", "H5ClusterRun",
  function(x) {
    n_time <- x@n_time
    if (is.null(n_time) || is.na(n_time) || length(n_time) != 1 || n_time <= 0 || floor(n_time) != n_time) {
      stop(sprintf("[dim,H5ClusterRun] Slot 'n_time' (%s) is invalid.", as.character(n_time)))
    }
    c(dim(x@mask), n_time)
  }
)


#' @rdname series-methods
#' @export
#' @family H5Cluster
setMethod(
  f = "series",
  signature = signature(x = "H5ClusterRun", i = "numeric"),
  definition = function(x, i, j, k, ...) {

    dims_mask <- dim(x@mask)
    n_vox_mask <- x@n_voxels
    nt <- x@n_time
    mask_indices_req <- NULL

    if (missing(j) && missing(k)) {
        # Case 1: i is numeric mask indices
        mask_indices_req <- as.integer(i)
        if (any(mask_indices_req < 1 | mask_indices_req > n_vox_mask)) {
            stop(sprintf("[series,H5ClusterRun] Mask indices out of range [1..%d]", n_vox_mask))
        }
    } else if (!missing(j) && !missing(k)) {
        # Case 2: i, j, k are single coordinates
        if (length(i) != 1 || length(j) != 1 || length(k) != 1) {
             stop("[series,H5ClusterRun] If providing i, j, k, they must be single values.")
        }
        coords <- matrix(c(i, j, k), nrow = 1, ncol = 3)
        # --- Convert coordinate to mask index ---
        mask_array <- as.logical(as.array(x@mask))
        global_mask_indices <- which(mask_array)
        if (any(coords[,1] < 1 | coords[,1] > dims_mask[1]) ||
            any(coords[,2] < 1 | coords[,2] > dims_mask[2]) ||
            any(coords[,3] < 1 | coords[,3] > dims_mask[3])) {
            stop("[series,H5ClusterRun] Coordinates are out of bounds.")
        }
        linear_idx_full <- coords[,1] + (coords[,2]-1)*dims_mask[1] + (coords[,3]-1)*dims_mask[1]*dims_mask[2]
        if (!mask_array[linear_idx_full]) {
             warning("[series,H5ClusterRun] Requested coordinate falls outside the mask.")
             return(matrix(numeric(0), nrow = nt, ncol = 0))
        }
        mask_indices_req <- match(linear_idx_full, global_mask_indices)
        if (is.na(mask_indices_req)) {
             stop("[series,H5ClusterRun] Internal error: Failed to map valid coordinate to mask index.")
        }
        # --- End conversion ---
    } else {
        stop("[series,H5ClusterRun] Invalid arguments. Provide numeric mask indices (i), a 3-col matrix (i), or single coordinates (i, j, k).")
    }

    if (is.null(mask_indices_req) || length(mask_indices_req) == 0) {
       return(matrix(numeric(0), nrow = nt, ncol = 0))
    }

    # Call helper with n_time
    voxel_data <- .get_cluster_timeseries_by_mask_index(x, mask_indices = mask_indices_req,
                                                        time_indices = NULL, n_time = nt)
    return(t(voxel_data))
  }
)

#' @rdname series-methods
#' @export
#' @family H5Cluster
setMethod(
  f = "series",
  signature = signature(x = "H5ClusterRun", i = "matrix"),
  definition = function(x, i, ...) {
    coords <- i
    if (ncol(coords) != 3) {
        stop("[series,H5ClusterRun] Coordinate matrix 'i' must have 3 columns.")
    }

    dims_mask <- dim(x@mask)
    n_vox_mask <- x@n_voxels
    nt <- x@n_time
    mask_array <- as.logical(as.array(x@mask))
    global_mask_indices <- which(mask_array)

    if (any(coords[,1] < 1 | coords[,1] > dims_mask[1]) ||
        any(coords[,2] < 1 | coords[,2] > dims_mask[2]) ||
        any(coords[,3] < 1 | coords[,3] > dims_mask[3])) {
        stop("[series,H5ClusterRun] Coordinates are out of bounds.")
    }

    linear_idx_full <- coords[,1] + (coords[,2]-1)*dims_mask[1] + (coords[,3]-1)*dims_mask[1]*dims_mask[2]
    in_mask_subset <- mask_array[linear_idx_full]

    if (!all(in_mask_subset)) {
        warning("[series,H5ClusterRun] Some coordinates fall outside mask and will be ignored.")
        coords <- coords[in_mask_subset, , drop = FALSE]
        linear_idx_full <- linear_idx_full[in_mask_subset]
        if (length(linear_idx_full) == 0) {
            return(matrix(numeric(0), nrow = nt, ncol = 0))
        }
    }

    mask_indices_req <- match(linear_idx_full, global_mask_indices)
    if (any(is.na(mask_indices_req))) {
         stop("[series,H5ClusterRun] Internal error: Failed to map valid coordinates to mask indices.")
    }
    if (length(mask_indices_req) == 0) {
       return(matrix(numeric(0), nrow = nt, ncol = 0))
    }

    # Call helper with n_time
    voxel_data <- .get_cluster_timeseries_by_mask_index(x, mask_indices = mask_indices_req,
                                                        time_indices = NULL, n_time = nt)
    voxel_data_transposed <- t(voxel_data)

    if (nrow(coords) == ncol(voxel_data_transposed)) { # Check dimensions match
       colnames(voxel_data_transposed) <- apply(coords, 1, paste, collapse=",")
    }
    return(voxel_data_transposed)
  }
)


#' @importFrom neuroim2 linear_access NeuroSpace space 
#' @export
#' @family H5Cluster
#' @rdname linear_access-methods
#' @description Provides 4D linear access to data in an \code{H5ClusterRun} object.
#' It reconstructs voxel values on the fly from the HDF5 file based on their cluster assignments.
#' Values for voxels outside the mask are returned as 0.
#'
#' @param x An \code{H5ClusterRun} object.
#' @param i A numeric vector of 4D linear indices.
#' @param ... Additional arguments (not used for this method).
#'
#' @return A numeric vector of values corresponding to the provided linear indices.
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     exists("H5ClusterExperiment", where = "package:fmristore") &&
#'     exists("linear_access", where = "package:neuroim2") &&
#'     !is.null(fmristore:::create_minimal_h5_for_H5ClusterExperiment)) {
#'
#'   temp_exp_file <- NULL
#'   exp_obj <- NULL
#'   run_full <- NULL
#' 
#'   tryCatch({
#'     # Create a minimal H5ClusterExperiment
#'     temp_exp_file <- fmristore:::create_minimal_h5_for_H5ClusterExperiment(
#'       master_mask_dims = c(3L, 3L, 2L), # Small dimensions
#'       num_master_clusters = 2L,
#'       n_time_run1 = 4L, # For Run1_Full
#'       n_time_run2 = 0   # No need for Run2_Summary here
#'     )
#'     exp_obj <- fmristore::H5ClusterExperiment(file_path = temp_exp_file)
#'     
#'     # Access the H5ClusterRun object (helper creates "Run1_Full")
#'     # The runs() method should give access to the list of runs
#'     available_runs <- runs(exp_obj)
#'     run_full <- available_runs[["Run1_Full"]] # Assuming helper creates this scan name
#'     
#'     if (!is.null(run_full)) {
#'       # Get dimensions: X, Y, Z, T
#'       run_dims <- dim(run_full) # Should be c(3,3,2,4)
#'       total_elements <- prod(run_dims)
#'       
#'       # Example: Access first 5 linear indices and last 5
#'       indices_to_access <- c(1:5, (total_elements-4):total_elements)
#'       # Ensure indices are within bounds if total_elements is small
#'       indices_to_access <- indices_to_access[indices_to_access <= total_elements & indices_to_access > 0]
#'       indices_to_access <- unique(indices_to_access)
#'       
#'       if (length(indices_to_access) > 0) {
#'          accessed_values <- neuroim2::linear_access(run_full, indices_to_access)
#'          cat("Accessed values for H5ClusterRun:\n")
#'          print(accessed_values)
#'          cat("Number of values accessed:", length(accessed_values), "\n")
#'       } else {
#'          message("No valid indices to access for linear_access example.")
#'       }
#'     } else {
#'       message("Could not retrieve Run1_Full from the experiment for linear_access example.")
#'     }
#'     
#'   }, error = function(e) {
#'     message("linear_access example for H5ClusterRun failed: ", e$message)
#'     if (!is.null(temp_exp_file)) message("Temporary file was: ", temp_exp_file)
#'   }, finally = {
#'     if (!is.null(exp_obj)) try(close(exp_obj), silent = TRUE)
#'     # run_full is part of exp_obj, its resources are managed by exp_obj$close()
#'     if (!is.null(temp_exp_file) && file.exists(temp_exp_file)) {
#'       unlink(temp_exp_file)
#'     }
#'   })
#' } else {
#'   message("Skipping linear_access H5ClusterRun example: dependencies/helpers not available.")
#' }
setMethod(
  f = "linear_access",
  signature = signature(x="H5ClusterRun", i="numeric"),
  definition = function(x, i, ...) {

    full_dims <- dim(x) # Uses dim method for H5ClusterRun
    nt <- full_dims[4]
    n_request <- length(i)
    if (n_request == 0) return(numeric(0))

    max_elements <- prod(full_dims)
    if (any(i < 1 | i > max_elements)) {
        stop(sprintf("[linear_access,H5ClusterRun] 4D indices out of range [1..%d]", max_elements))
    }

    coords4d <- arrayInd(i, .dim = full_dims)
    mask_array <- as.logical(as.array(x@mask))
    dims_mask <- dim(mask_array)
    global_mask_indices <- which(mask_array)

    coords3d <- coords4d[, 1:3, drop = FALSE]
    lin3d <- coords3d[,1] + (coords3d[,2]-1L)*dims_mask[1] + (coords3d[,3]-1L)*dims_mask[1]*dims_mask[2]
    in_mask_logical <- mask_array[lin3d]
    result_vec <- rep(NA_real_, n_request)

    if (any(in_mask_logical)) {
        valid_indices_in_i <- which(in_mask_logical)
        valid_coords4d <- coords4d[valid_indices_in_i, , drop = FALSE]
        valid_lin3d <- lin3d[valid_indices_in_i]
        mask_indices_req <- match(valid_lin3d, global_mask_indices)
        time_coords <- valid_coords4d[, 4]
        requests_by_time <- split(data.frame(orig_idx = valid_indices_in_i, mask_idx = mask_indices_req), time_coords)

        for (t_val_str in names(requests_by_time)) {
            t_val <- as.integer(t_val_str)
            current_requests <- requests_by_time[[t_val_str]]
            mask_indices_for_t <- current_requests$mask_idx
            original_indices_for_t <- current_requests$orig_idx

            # Call helper with n_time
            data_subset <- .get_cluster_timeseries_by_mask_index(x, mask_indices = mask_indices_for_t,
                                                                 time_indices = t_val, n_time = nt)
            result_vec[original_indices_for_t] <- data_subset[, 1]
        }
    }

    result_vec[is.na(result_vec)] <- 0
    return(result_vec)
  }
)


#' @rdname show-methods
#' @export
#' @family H5Cluster
setMethod(
  f = "show",
  signature = "H5ClusterRun",
  definition = function(object) {
    cat("\n", crayon::bold(crayon::blue("H5ClusterRun")), "\n", sep = "")
    cat(crayon::silver("────────────────────────────────────────\n"))
    cat(crayon::bold(crayon::yellow("Run Info")), "\n")
    cat(crayon::silver(" • "), crayon::green("Scan Name:"), object@scan_name, "\n")
    cat(crayon::silver(" • "), crayon::green("Time points:"), object@n_time, "\n")
    cat(crayon::bold("\nShared Info (from H5ClusteredArray)"), "\n")
    cat(crayon::silver(" • "), crayon::green("Active voxels in mask:"), object@n_voxels, "\n")
    if (!is.null(object@clusters) && length(object@clusters@clusters) > 0) {
        cluster_ids <- unique(object@clusters@clusters)
        n_clusters  <- length(cluster_ids)
        cat(crayon::silver(" • "), crayon::green("Number of clusters:"), n_clusters, "\n")
    } else {
        cat(crayon::silver(" • "), crayon::green("Number of clusters:"), "(NA)", "\n")
    }
    cat(crayon::bold("\nStorage: HDF5 File"), "\n")
    if (!is.null(object@obj) && inherits(object@obj, "H5File") && object@obj$is_valid) {
      cat(crayon::silver(" • "), "Path: ",
          crayon::magenta(object@obj$get_filename()), "\n", sep="")
      # Check if the specific run path exists
      scan_clusters_path <- tryCatch(.dataset_path(object, 1), # Use helper for base path logic
                                      error = function(e) NULL)
      if (!is.null(scan_clusters_path)) {
          scan_clusters_path <- dirname(scan_clusters_path) # Get parent dir /scans/../clusters
           cat(crayon::silver(" • "), "Run clusters path: ",
               crayon::magenta(scan_clusters_path), " (",
               ifelse(object@obj$exists(scan_clusters_path), crayon::green("exists"), crayon::red("missing")), ")\n", sep="")
      } else {
           cat(crayon::silver(" • "), "Run clusters path: unable to determine\n")
      }

    } else {
      cat(crayon::silver(" • "), "HDF5 file is ",
          crayon::red("INVALID or CLOSED"), "\n", sep="")
    }
    cat("\n")
  }
)


#' Constructor for H5ClusterRun Objects
#'
#' @description
#' Creates a new `H5ClusterRun` object, representing a single run of full
#' voxel-level clustered data from an HDF5 file.
#'
#' It performs necessary validation and can read metadata like `n_time` directly
#' from the HDF5 file attributes or metadata dataset.
#'
#' @param file_source Either an open `H5File` object or a character string path to the HDF5 file.
#' @param scan_name The character string name of the scan (e.g., "run1").
#' @param mask A `LogicalNeuroVol` object for the brain mask.
#' @param clusters A `ClusteredNeuroVol` object for cluster assignments.
#' @param n_time (Optional) The number of time points. If `NULL` (default), the function
#'   will attempt to read it from the HDF5 file attributes (e.g., `/scans/<scan_name>@n_time`
#'   or `/scans/<scan_name>/metadata/n_time`).
#' @param compress (Optional) Logical indicating compression status (metadata).
#'
#' @return A new `H5ClusterRun` object.
#' @importFrom hdf5r H5File h5attr h5attr_names H5A 
#' @importFrom methods new is
#' @export
#' @family H5Cluster
make_run_full <- function(file_source, scan_name,
                            mask, clusters,
                            n_time = NULL, compress = FALSE) {

  lifecycle::deprecate_warn(
    when = "0.2.0", 
    what = "make_run_full()",
    with = "H5ClusterRun()",
    details = "The make_run_* functions are deprecated in favor of direct H5* constructors."
  )
  
  # --- 1. Handle file source --- 
  fh <- open_h5(file_source, mode = "r")
  # Ensure file is closed if this function opened it.
  defer(if (fh$owns) fh$h5$close_all(), envir = parent.frame())
  h5obj <- fh$h5
  
  # --- 2. Validate inputs --- 
  if (!is.character(scan_name) || length(scan_name) != 1 || !nzchar(scan_name)) {
    stop("[make_run_full] 'scan_name' must be a non-empty character string.")
  }
  if (!is(mask, "LogicalNeuroVol")) {
    stop("[make_run_full] 'mask' must be a LogicalNeuroVol object.")
  }
  if (!is(clusters, "ClusteredNeuroVol")) {
     stop("[make_run_full] 'clusters' must be a ClusteredNeuroVol object.")
  }
  # Use helper to check dimensions 1-3
  check_same_dims(mask, clusters, dims_to_compare = 1:3, 
                  msg = "[make_run_full] Dimensions of 'mask' and 'clusters' must match.")
  
  n_vox <- sum(mask)
  if (!identical(length(clusters@clusters), as.integer(n_vox))) {
    stop(sprintf(
      "[make_run_full] Mismatch: clusters@clusters length (%d) != sum(mask) (%d).",
      length(clusters@clusters), n_vox
    ))
  }

  # --- 3. Determine n_time --- 
  if (is.null(n_time)) {
    # Try to read n_time from HDF5 attributes or metadata dataset
    scan_group_path <- paste0("/scans/", scan_name)
    scan_group <- NULL
    ds <- NULL # Ensure ds is NULL initially for cleanup
    
    # Check if scan group exists first, catching errors
    scan_group_exists <- tryCatch({
        h5obj$exists(scan_group_path)
    }, error = function(e) {
        warning(sprintf("[make_run_full] Suppressed HDF5 error during existence check for scan group '%s': %s", scan_group_path, conditionMessage(e)))
        FALSE
    })
    
    if (scan_group_exists) {
        tryCatch({
            scan_group <- h5obj[[scan_group_path]]
            on.exit(if(!is.null(scan_group) && inherits(scan_group, "H5Group") && scan_group$is_valid) try(scan_group$close()), add = TRUE)

            if ("n_time" %in% h5attr_names(scan_group)) {
                n_time <- h5attr(scan_group, "n_time")
            } else if (tryCatch(scan_group$exists("metadata/n_time"), error = function(e){ warning(sprintf("Suppressed HDF5 error checking metadata/n_time existence: %s", conditionMessage(e))); FALSE})) {
                 md_time <- NULL
                 tryCatch({
                     md_time <- scan_group[["metadata/n_time"]]
                     n_time <- md_time$read()
                 }, finally = {
                     if(!is.null(md_time) && inherits(md_time, "H5D") && md_time$is_valid) try(md_time$close())
                 })
            } else if (tryCatch(scan_group$exists("clusters"), error = function(e){ warning(sprintf("Suppressed HDF5 error checking clusters existence: %s", conditionMessage(e))); FALSE})) {
                # Try to infer from first cluster dataset
                first_cid <- clusters@clusters[1]
                if (!is.null(first_cid)) {
                    # Construct path directly, avoiding dummy object creation
                    dset_path_cid1 <- sprintf("/scans/%s/clusters/cluster_%d", scan_name, as.integer(first_cid))
                    
                    # Use .dataset_path to get the correct path for the first cluster
                    # dset_path_cid1 <- tryCatch(.dataset_path(new("H5ClusterRun", scan_name=scan_name), first_cid), # Create dummy obj for path gen
                    #                        error=function(e) NULL)
                    if (!is.null(dset_path_cid1) && tryCatch(h5obj$exists(dset_path_cid1), error = function(e) {warning(sprintf("Suppressed HDF5 error checking %s: %s", dset_path_cid1, conditionMessage(e))); FALSE})) {
                        tryCatch({
                            ds <- h5obj[[dset_path_cid1]] # Open the dataset
                            dims <- ds$dims
                            if (length(dims) == 2) {
                                n_time <- dims[2]
                                message(sprintf("[make_run_full] Inferred n_time = %d from dataset '%s'.", n_time, dset_path_cid1))
                            } else {
                                stop(sprintf("[make_run_full] Dataset '%s' is malformed per specification: expected 2 dimensions, got %d.",
                                             dset_path_cid1, length(dims)))
                            }
                        }, error = function(e) {
                            warning(sprintf("[make_run_full] Error reading dimensions from '%s': %s", dset_path_cid1, e$message))
                        }, finally = {
                             if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) close_h5_safely(ds) # Close dataset handle
                        })
                    } else {
                       warning(sprintf("[make_run_full] Could not find or access dataset for first cluster (%d) at path '%s' to infer n_time.", first_cid, dset_path_cid1 %||% "(path generation failed)"))
                    }
                } else {
                     warning("[make_run_full] No clusters found in provided 'clusters' object to infer n_time.")
                }
            }
        }, error = function(e) {
            # Ensure scan_group is closed if error occurred after opening it
            if (!is.null(scan_group) && inherits(scan_group, "H5Group") && scan_group$is_valid) try(scan_group$close(), silent=TRUE)
            stop(sprintf("[make_run_full] Error accessing or validating scan group '%s'. Original error: %s", scan_group_path, e$message))
        })
    } else {
        warning(sprintf("[make_run_full] Scan group path '%s' not found. Cannot automatically determine n_time.", scan_group_path))
    }

    if (is.null(n_time)) {
      stop(sprintf("[make_run_full] Could not determine 'n_time' for scan '%s'. Provide it explicitly or ensure it exists in HDF5 attributes/metadata.", scan_name))
    }
  }

  # Final validation of n_time
  if (!is.numeric(n_time) || length(n_time) != 1 || n_time <= 0 || floor(n_time) != n_time) {
    stop(sprintf("[make_run_full] 'n_time' (%s) must be a single positive integer.", as.character(n_time)))
  }
  n_time <- as.integer(n_time)

  # --- 4. Create the object --- 
  run_obj <- new("H5ClusterRun",
                 obj       = h5obj,
                 scan_name = scan_name,
                 mask      = mask,
                 clusters  = clusters,
                 n_voxels  = as.integer(n_vox),
                 n_time    = n_time,
                 compress  = as.logical(compress)
                )

  # --- 5. Important: Detach the object handle if we opened the file --- 
  # The returned object now owns the handle if fh$owns is FALSE.
  # If fh$owns is TRUE, the defer() handler will close it when the function exits.
  # We need to prevent the created object from trying to close a handle
  # that the calling function might still own or that defer will close.
  # If the user passed an open handle (fh$owns == FALSE), run_obj keeps it.
  # If we opened it (fh$owns == TRUE), run_obj's handle will be invalid after exit, 
  # which is the desired behavior unless the user manages the file handle lifecycle outside.
  if (fh$owns) {
    # If we opened the file, the created object should not manage the handle.
    # The validity checks within methods should handle this.
    # The 'defer' takes care of closing. We don't need to nullify run_obj@obj.
    # The key is that the caller understands the handle's lifecycle.
    
    # Alternative: If the design requires the returned object *always* has a valid 
    # handle *if* the source was a path, we'd need a different approach (e.g., 
    # clone the handle, manage externally). Sticking with simple defer for now.
  }

  # Return the created object
  return(run_obj)
}



#' @rdname as.matrix-methods
#' @export
#' @family H5Cluster
setMethod(
  f = "as.matrix",
  signature = signature(x="H5ClusterRunSummary"),
  definition = function(x) {
    summary_grp <- NULL
    ds <- NULL
    mat_data <- NULL

    # Validate inputs from the object
    if (is.null(x@obj) || !x@obj$is_valid) stop("[as.matrix,H5ClusterRunSummary] HDF5 file handle is invalid or closed.")
    if (!is.character(x@scan_name) || !nzchar(x@scan_name)) stop("[as.matrix,H5ClusterRunSummary] Invalid 'scan_name' slot.")
    if (!is.character(x@summary_dset) || !nzchar(x@summary_dset)) stop("[as.matrix,H5ClusterRunSummary] Invalid 'summary_dset' slot.")

    # Construct path to the specific summary dataset
    dset_path <- file.path("/scans", x@scan_name, "clusters_summary", x@summary_dset)

    tryCatch({
        assert_h5_path(x@obj, dset_path,
                       "[as.matrix,H5ClusterRunSummary] Summary dataset")
        ds <- x@obj[[dset_path]]
        on.exit(if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) close_h5_safely(ds), add = TRUE)

        mat_data <- ds$read() # Use $read() instead of []

    }, error = function(e) {
        # Ensure ds handle is closed on error
        if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) {
            close_h5_safely(ds)
        }
        stop(sprintf("[as.matrix,H5ClusterRunSummary] Failed to read summary data for scan '%s' from '%s'. Original error: %s",
                     x@scan_name, dset_path, e$message))
    })

    if (is.null(mat_data)) {
        stop(sprintf("[as.matrix,H5ClusterRunSummary] Failed to retrieve summary data matrix for scan '%s'. Result is NULL.", x@scan_name))
    }
    
    # Optional: Validate dimensions against n_time if available?
    # if (!is.na(x@n_time) && nrow(mat_data) != x@n_time) { ... warning ... }
    
    # Set column names if available and dimensions match
    if (length(x@cluster_names) > 0) {
        if (length(x@cluster_names) == ncol(mat_data)) {
            colnames(mat_data) <- x@cluster_names
        } else {
            warning(sprintf("[as.matrix,H5ClusterRunSummary] Length of cluster_names (%d) != number of columns (%d) for scan '%s'. Names not set.",
                          length(x@cluster_names), ncol(mat_data), x@scan_name))
        }
    }

    mat_data
  }
)

#' @rdname as.data.frame-methods
#' @export
#' @family H5Cluster
setMethod(
  f = "as.data.frame",
  signature = signature(x="H5ClusterRunSummary"),
  definition = function(x, row.names=NULL, optional=FALSE, ...) {
    # Calls the as.matrix method defined above for H5ClusterRunSummary
    mat_data <- as.matrix(x)
    df <- as.data.frame(mat_data, row.names=row.names, optional=optional, ...)
    df
  }
)

# Override voxel-level accessors to prevent misuse


#' @export
#' @family H5Cluster
setMethod("[",
  signature(x = "H5ClusterRunSummary", i = "ANY", j = "ANY", drop = "ANY"),
  function(x, i, j, k, l, ..., drop = TRUE) {
    stop("Voxel-level subsetting ([i,j,k,l]) is not available for H5ClusterRunSummary objects. Use as.matrix() or as.data.frame() to access summary data.")
  })


#' @export
#' @family H5Cluster
setMethod("[",
  signature(x = "H5ClusterRunSummary", i = "ANY", j = "missing", drop = "ANY"),
  definition = function(x, i, ..., drop = TRUE) {
    stop("Voxel-level subsetting (e.g., x[indices]) is not available for H5ClusterRunSummary objects. Use as.matrix() or as.data.frame() to access summary data.")
  })

#' @rdname series-methods
#' @export
#' @family H5Cluster
setMethod(
  f = "series",
  signature = signature(x = "H5ClusterRunSummary", i = "ANY"), # Catch numeric or matrix
  definition = function(x, i, ...) {
    stop("Voxel-level time series extraction (series()) is not available for H5ClusterRunSummary objects. Use as.matrix() or as.data.frame() to access summary data.")
  }
)


#' @rdname linear_access-methods
#' @export
#' @family H5Cluster
#' @importFrom neuroim2 linear_access
setMethod(
  f = "linear_access",
  signature = signature(x="H5ClusterRunSummary", i="numeric"),
  definition = function(x, i, ...) {
    stop("Voxel-level linear access (linear_access()) is not available for H5ClusterRunSummary objects. Use as.matrix() or as.data.frame() to access summary data.")
  }
)

#' Constructor for H5ClusterRunSummary Objects
#'
#' @description
#' Creates a new `H5ClusterRunSummary` object, representing a single run of
#' summary cluster time-series data from an HDF5 file.
#'
#' It performs validation, checks for the existence of the summary dataset,
#' and determines `n_time` from the dataset dimensions.
#'
#' @param file_source Either an open `H5File` object or a character string path to the HDF5 file.
#' @param scan_name The character string name of the scan (e.g., "run1").
#' @param mask A `LogicalNeuroVol` object for the brain mask (used for reference/consistency).
#' @param clusters A `ClusteredNeuroVol` object for cluster assignments. This is required
#'   for consistency and reference, even though summary data is accessed.
#' @param cluster_names (Optional) Character vector of names for the clusters.
#' @param cluster_ids (Optional) Integer vector of IDs for the clusters.
#' @param summary_dset (Optional) The name of the dataset within the run's summary group
#'   (default: "summary_data").
#'
#' @return A new `H5ClusterRunSummary` object.
#' @importFrom hdf5r H5File H5D
#' @importFrom methods new is
#' @importFrom lifecycle deprecate_warn
#' @export
#' @family H5Cluster
make_run_summary <- function(file_source, scan_name,
                               mask, clusters,
                               cluster_names = character(), cluster_ids = integer(),
                               summary_dset = "summary_data") {

  lifecycle::deprecate_warn(
    when = "0.2.0", 
    what = "make_run_summary()",
    with = "H5ClusterRunSummary()",
    details = "The make_run_* functions are deprecated in favor of direct H5* constructors."
  )

  # --- 1. Handle file source ---
  fh <- open_h5(file_source, mode = "r")
  # Ensure file is closed if this function opened it.
  defer(if (fh$owns) fh$h5$close_all(), envir = parent.frame())
  h5obj <- fh$h5

  # --- 2. Validate common inputs ---
  if (!is.character(scan_name) || length(scan_name) != 1 || !nzchar(scan_name)) {
    stop("[make_run_summary] \'scan_name\' must be a non-empty character string.")
  }
  if (!is(mask, "LogicalNeuroVol")) {
    stop("[make_run_summary] \'mask\' must be a LogicalNeuroVol object.")
  }
  n_vox <- sum(mask)
  # Validate clusters if provided, but allow NULL
  if (!is.null(clusters)) {
      if (!is(clusters, "ClusteredNeuroVol")) {
        stop("[make_run_summary] \'clusters\' must be a ClusteredNeuroVol object.")
      }
      # Use helper to check dimensions 1-3
      check_same_dims(mask, clusters, dims_to_compare = 1:3,
                      msg = "[make_run_summary] Dimensions of 'mask' and provided 'clusters' must match.")
      
      if (!identical(length(clusters@clusters), as.integer(n_vox))) {
        stop(sprintf(
          "[make_run_summary] Mismatch: provided clusters@clusters length (%d) != sum(mask) (%d).",
          length(clusters@clusters), n_vox
        ))
      }
  } else {
       # If clusters is NULL, we still need n_voxels, which we got from sum(mask)
  }
  
  if (!is.character(summary_dset) || length(summary_dset) != 1 || !nzchar(summary_dset)) {
      stop("[make_run_summary] \'summary_dset\' must be a non-empty character string.")
  }

  # --- 3. Validate summary dataset and get n_time ---
  dset_path <- file.path("/scans", scan_name, "clusters_summary", summary_dset)
  n_time <- NA_integer_
  n_clusters_in_dset <- NA_integer_
  ds <- NULL


  # Ensure the summary dataset exists
  assert_h5_path(h5obj, dset_path,
                 "[make_run_summary] Summary dataset")

  tryCatch({
      ds <- h5obj[[dset_path]]
      dims <- ds$dims
      if (length(dims) != 2) {
          stop(sprintf("[make_run_summary] Summary dataset at \'%s\' must have 2 dimensions [nTime, nClusters], but found %d dimensions.",
                       dset_path, length(dims)))
      }
      n_time <- as.integer(dims[1])
      n_clusters_in_dset <- as.integer(dims[2])

      # Validate n_time
      if (is.na(n_time) || n_time <= 0) {
          stop(sprintf("[make_run_summary] Invalid time dimension (%d) read from summary dataset \'%s\'.", n_time, dset_path))
      }

      # Validate cluster info against dataset dimensions
      # Determine the source of truth for cluster IDs
      has_provided_ids <- length(cluster_ids) > 0
      has_clusters_object <- !is.null(clusters) && inherits(clusters, "ClusteredNeuroVol") && length(clusters@clusters) > 0
      
      # 1. Determine actual cluster IDs
      if (has_provided_ids) {
          actual_cluster_ids <- cluster_ids
      } else if (has_clusters_object) {
          actual_cluster_ids <- unique(clusters@clusters)
      } else {
          # No IDs provided and no clusters object: cannot determine IDs reliably
          actual_cluster_ids <- integer() 
          warning(sprintf("[make_run_summary] Cannot determine cluster IDs for scan '%s' without 'cluster_ids' or a valid 'clusters' object.", scan_name))
      }
      
      # 2. Determine cluster names: Prioritize provided, then generate if needed
      has_provided_names <- length(cluster_names) > 0
      final_cluster_names <- character()
      generated_names <- FALSE
      
      if (has_provided_names) {
          final_cluster_names <- cluster_names
      } else if (length(actual_cluster_ids) > 0) {
          # Generate names from IDs if names are missing but IDs are known
          final_cluster_names <- paste0("Clus_", actual_cluster_ids)
          # If we couldn't determine IDs above, we can't generate ID-based names
      } else if (n_clusters_in_dset > 0) { 
          # Fallback: Generate names like "Col_X" if dataset has columns but no names/IDs known
          final_cluster_names <- paste0("Col_", seq_len(n_clusters_in_dset))
          warning(sprintf("[make_run_summary] No cluster names or IDs provided/derivable for scan '%s'; generated default column names (Col_X).", scan_name))
          generated_names <- TRUE
      } else {
          # No names provided, no IDs known, dataset has 0 columns -> empty names is correct
          final_cluster_names <- character()
      }

      # 3. Check consistency of final names/IDs against dataset dimensions
      if (length(final_cluster_names) != n_clusters_in_dset) {
           warning(sprintf("[make_run_summary] Final number of cluster names (%d) does not match dataset columns (%d) for scan '%s'. Check inputs/clusters object/dataset.",
                        length(final_cluster_names), n_clusters_in_dset, scan_name))
           # If names mismatched and we *generated* Col_X names, this indicates an issue
           # If names were provided or derived from IDs, this warning stands.
      }
      
      # Check IDs consistency - only if we didn't just generate Col_X names (as IDs might be unknown)
      if (!generated_names && length(actual_cluster_ids) != n_clusters_in_dset) {
           warning(sprintf("[make_run_summary] Final number of cluster IDs (%d) does not match dataset columns (%d) for scan '%s'. Check inputs/clusters object/dataset.",
                         length(actual_cluster_ids), n_clusters_in_dset, scan_name))
            # If IDs mismatch, should we force sequential IDs? Risky. Let's warn.
           # For safety, if using generated Col_X names, ensure IDs are sequential too.
           # Let's assign default sequential IDs if names were generated OR if ID length mismatches
           if (generated_names || length(actual_cluster_ids) != n_clusters_in_dset) {
                actual_cluster_ids <- seq_len(n_clusters_in_dset)
                warning(sprintf("[make_run_summary] Assigning default sequential IDs (1..%d) for scan '%s' due to mismatch or generated names.", n_clusters_in_dset, scan_name))
           }
      }

      # If cluster_ids were initially empty and we *didn't* generate Col_X names, but derived IDs from cluster object
      # ensure we now have IDs matching the columns, generating if necessary
      if (!has_provided_ids && !generated_names && length(actual_cluster_ids) != n_clusters_in_dset && n_clusters_in_dset > 0) {
           actual_cluster_ids <- seq_len(n_clusters_in_dset)
           warning(sprintf("[make_run_summary] Generating default sequential IDs (1..%d) for scan '%s' as derived IDs didn't match dataset columns.", n_clusters_in_dset, scan_name))
      }
      
      # ... after generating default names ...
      if (generated_names && length(actual_cluster_ids) == 0 && n_clusters_in_dset > 0) {
           actual_cluster_ids <- seq_len(n_clusters_in_dset)
           warning(sprintf("[make_run_summary] Generated default IDs 1..%d for scan '%s' to match dataset columns.", n_clusters_in_dset, scan_name))
       }

  }, error = function(e) {
      stop(sprintf("[make_run_summary] Error accessing or validating summary dataset \'%s\'. Original error: %s", dset_path, e$message))
  }, finally = {
      if (!is.null(ds) && inherits(ds, "H5D") && ds$is_valid) {
          close_h5_safely(ds)
      }
  })

  # --- 4. Create the object ---
  run_obj <- new("H5ClusterRunSummary",
                 obj           = h5obj,
                 scan_name     = scan_name,
                 mask          = mask,
                 clusters      = clusters,
                 n_voxels      = as.integer(n_vox),
                 n_time        = n_time,
                 cluster_names = as.character(final_cluster_names),
                 cluster_ids   = as.integer(actual_cluster_ids),
                 summary_dset  = summary_dset
                )

  # Similar handle lifecycle considerations as make_run_full
  # If fh$owns is TRUE, defer() will close the handle. The returned object's
  # handle will become invalid upon function exit.

  return(run_obj)
}

#' Show Method for H5ClusterRunSummary
#' @param object An H5ClusterRunSummary object
#' @importFrom cli cli_h1 cli_h2 cli_li col_blue col_green col_yellow col_silver col_red col_magenta symbol cli_text cli_alert_info
#' @importFrom utils head
#' @importFrom methods show
#' @export
#' @family H5Cluster
#' @rdname show-methods
setMethod(
  f = "show",
  signature = "H5ClusterRunSummary",
  definition = function(object) {

    cli::cli_h1(cli::col_blue("H5ClusterRunSummary"))

    cli::cli_h2(cli::col_yellow("Run Summary Info"))
    cli::cli_li(items = c(
        "{cli::col_green(\'Scan Name\')} : {object@scan_name}",
        "{cli::col_green(\'Time points\')} : {object@n_time}",
        "{cli::col_green(\'Summary Dset\')} : {object@summary_dset}"
    ))

    # Display cluster names/IDs (show first few)
    n_clus_names <- length(object@cluster_names)
    n_clus_ids   <- length(object@cluster_ids)
    cli::cli_text("{cli::col_green(\'Cluster Names\')} ({n_clus_names}):")
    if (n_clus_names > 0) {
        name_preview <- utils::head(object@cluster_names, 5)
        cli::cli_alert_info("  {paste(name_preview, collapse=\', \')}{ifelse(n_clus_names > 5, \' ...\', \'\')}")
    } else {
        cli::cli_alert_info("  (No cluster names provided)")
    }
    cli::cli_text("{cli::col_green(\'Cluster IDs\')} ({n_clus_ids}):")
     if (n_clus_ids > 0) {
        id_preview <- utils::head(object@cluster_ids, 5)
        cli::cli_alert_info("  {paste(id_preview, collapse=\', \')}{ifelse(n_clus_ids > 5, \' ...\', \'\')}")
    } else {
        cli::cli_alert_info("  (No cluster IDs provided)")
    }


    cli::cli_h2(cli::col_yellow("Shared Info (from H5ClusteredArray)"))
    # Check if clusters slot is valid and has data
    has_clusters <- !is.null(object@clusters) && inherits(object@clusters, "ClusteredNeuroVol") && length(object@clusters@clusters) > 0
    n_clusters_from_slot <- if(has_clusters) length(unique(object@clusters@clusters)) else NA

    cli::cli_li(items = c(
        "{cli::col_green(\'Active voxels in mask\')} : {object@n_voxels}",
        "{cli::col_green(\'Number of clusters (map)\')} : {ifelse(has_clusters, n_clusters_from_slot, \'(NA or Map not loaded)\')}"
    ))

    cli::cli_h2(cli::col_yellow("Storage: HDF5 File"))
    h5_valid <- !is.null(object@obj) && inherits(object@obj, "H5File") && object@obj$is_valid
    if (h5_valid) {
      filename <- tryCatch(object@obj$get_filename(), error = function(e) "Error getting name")
      summary_path <- file.path("/scans", object@scan_name, "clusters_summary", object@summary_dset)
      path_exists <- tryCatch(object@obj$exists(summary_path), error = function(e) FALSE)
      cli::cli_li(items = c(
        "{cli::col_green(\'Path\')} : {cli::col_magenta(filename)}",
        "{cli::col_green(\'Status\')} : {cli::col_green(\'Valid and Open\')}",
        "{cli::col_green(\'Summary Dataset\')} : {cli::col_magenta(summary_path)} ({ifelse(path_exists, cli::col_green(cli::symbol$tick), cli::col_red(cli::symbol$cross))} exists)"
      ))
    } else {
      cli::cli_li(items = c(
        "{cli::col_green(\'Status\')} : {cli::col_red(\'INVALID or CLOSED\')}"
      ))
    }
    cli::cli_text("") # Add a newline at the end
  }
)
</file>

<file path="R/h5_utils.R">
#' @importFrom hdf5r H5File
NULL

#' Open an HDF5 file or use an existing handle
#'
#' Handles the boilerplate of checking if the input is a file path or an
#' existing H5File object, opening the file if necessary, and returning
#' a list containing the handle and a flag indicating ownership (whether
#' the function opened the file and is responsible for closing it).
#'
#' @param src A character string path to an HDF5 file or an existing `H5File` object.
#' @param mode The mode to open the file in (e.g., "r", "r+", "w"). Defaults to "r".
#' @return A list with two elements:
#'   - `h5`: The `H5File` object handle.
#'   - `owns`: A logical flag, `TRUE` if the function opened the file (src was a path),
#'             `FALSE` otherwise (src was already an `H5File` object).
#' @keywords internal
open_h5 <- function(src, mode = "r") {
  if (inherits(src, "H5File")) {
    # If it's already an H5File object, check if it's open
    if (!src$is_valid) {
        stop("open_h5: provided H5File object is not valid (likely closed).")
    }
    # User owns this handle, we didn't open it
    return(list(h5 = src, owns = FALSE))
  }

  if (!is.character(src) || length(src) != 1L || !nzchar(src)){
     stop("open_h5: 'src' must be a single, non-empty character string (file path) or an H5File object.")
  }

  # Only check for existence if opening in read-only mode.
  # Modes 'w', 'w+', 'a' are expected to create the file if it doesn't exist.
  if (mode == "r" && !file.exists(src)) {
      stop("open_h5: file path does not exist: ", src)
  }

  # Attempt to open the file
  h5 <- tryCatch({
      hdf5r::H5File$new(src, mode = mode)
  }, error = function(e) {
      stop("open_h5: failed to open HDF5 file '", src, "' in mode '", mode, "'. Original error: ", conditionMessage(e))
  })

  # We opened it, so we 'own' it (caller is responsible for closing)
  return(list(h5 = h5, owns = TRUE))
} 

#' Helper function to ensure a valid LogicalNeuroVol mask is available.
#' Reads from HDF5 if mask is NULL, otherwise validates the provided mask.
#'
#' @param mask A `LogicalNeuroVol` object, a file path string, or `NULL`.
#' @param h5 An open `H5File` handle (needed if mask is `NULL`).
#' @param space A `NeuroSpace` object representing the target space for validation.
#' @param path The HDF5 path where the mask should be read from (default: "/mask").
#' @return A validated `LogicalNeuroVol` object.
#' @keywords internal
ensure_mask <- function(mask, h5, space, path = "/mask") {
  assert_that(inherits(space, "NeuroSpace"), msg = "ensure_mask: 'space' must be a NeuroSpace object.")

  if (is.null(mask)) {
    if (is.null(h5) || !inherits(h5, "H5File") || !h5$is_valid) {
      stop("ensure_mask: 'h5' handle must be provided and valid when 'mask' is NULL.")
    }
    # Assume read_h5_mask_to_LogicalNeuroVol handles H5File validation and path existence
    # Use the internal helper function and pass the reference space
    m <- fmristore:::read_h5_mask_to_LogicalNeuroVol(h5, path, space)
  } else {
    # Validate provided mask type
    if (!is(mask, "LogicalNeuroVol")) {
      stop("Provided 'mask' must be a LogicalNeuroVol object.")
    }
    m <- mask
  }

  # Check spatial dimensions against the provided space using helper
  check_same_dims(m, space, dims_to_compare = 1:3,
                  msg = "Mask dimensions do not match space dimensions")

  # Ensure the mask's space matches the provided space object exactly
  if (!identical(space(m), space)) {
      warning("Mask's internal NeuroSpace object does not match the provided reference space. Consider updating the mask's space for consistency.")
      # Optionally, force the space to be identical?
      # space(m) <- space
      # For now, just warn, as dim check passed.
  }

  return(m)
}

#' Assert that an HDF5 path exists
#'
#' Checks for the existence of a dataset or group within an HDF5 file and
#' stops with a clear error message if the path is missing.
#'
#' @param h5 A valid open `H5File` object.
#' @param path The path to check within the file.
#' @param desc Optional description of what the path represents. This is used
#'   in the error message.
#' @return Invisible `TRUE` if the path exists.
#' @keywords internal
assert_h5_path <- function(h5, path, desc = "Path") {
  if (!inherits(h5, "H5File") || !h5$is_valid) {
    stop("assert_h5_path: 'h5' must be a valid and open H5File object.")
  }
  if (!is.character(path) || length(path) != 1 || !nzchar(path)) {
    stop("assert_h5_path: 'path' must be a single, non-empty character string.")
  }

  exists <- tryCatch(h5$exists(path), error = function(e) {
    stop(sprintf("assert_h5_path: Error checking existence of '%s': %s",
                 path, conditionMessage(e)))
  })

  if (!isTRUE(exists)) {
    fname <- tryCatch(h5$get_filename(), error = function(e) "<unknown>")
    stop(sprintf("%s not found at path '%s' in HDF5 file '%s'",
                 desc, path, fname))
  }

  invisible(TRUE)
}

#' Read data from an HDF5 dataset
#'
#' Safely reads data from a specified dataset path within an HDF5 file,
#' handling existence checks and optional return of NULL if missing.
#'
#' @param h5 An open `H5File` object handle.
#' @param path The full path to the dataset within the HDF5 file (e.g., "/data/my_dataset").
#' @param missing_ok Logical, if `TRUE`, returns `NULL` if the dataset doesn't exist. 
#'   If `FALSE` (default), stops with an error if the dataset is missing.
#' @param read_args A list of additional arguments passed to the HDF5 dataset's `$read()` method
#'   (e.g., for subsetting: `list(args = list(i = 1:10))`)
#'
#' @return The data read from the dataset, or `NULL` if `missing_ok=TRUE` and the dataset is not found.
#' @keywords internal
h5_read <- function(h5, path, missing_ok = FALSE, read_args = NULL) {
  if (!inherits(h5, "H5File") || !h5$is_valid) {
      stop("h5_read: 'h5' must be a valid and open H5File object.")
  }
  if (!is.character(path) || length(path) != 1 || !nzchar(path)) {
      stop("h5_read: 'path' must be a single, non-empty character string.")
  }
  
  path_exists <- tryCatch({
      h5$exists(path)
  }, error = function(e) {
      # Handle cases where checking existence itself fails (e.g., permission issue, intermediate group missing)
      warning(sprintf("h5_read: Suppressed HDF5 error during existence check for path '%s': %s", path, conditionMessage(e)))
      FALSE
  })
  
  if (!path_exists) {
    if (missing_ok) {
      return(NULL)
    } else {
      stop(sprintf("h5_read: Dataset or group not found at path '%s' in HDF5 file '%s'", 
                   path, h5$get_filename()))
    }
  }

  # Object exists, now try to open and read
  dset <- NULL
  data <- NULL
  tryCatch({
    # Check if it's a dataset before trying to read
    obj_info <- h5$link_info(path)
    if (obj_info$type != "H5L_TYPE_HARD") {
         # Could be a group or something else; h5[[path]] might work for groups but read() will fail.
         # Treat non-datasets as an error unless specifically handled.
         stop(sprintf("Object at path '%s' is not a dataset (type: %s)."), path, obj_info$type)
    }
    
    dset <- h5[[path]] # Open the dataset
    if (is.null(read_args)) {
        data <- dset$read()
    } else {
        # Pass arguments using do.call
        data <- do.call(dset$read, read_args)
    }
  }, error = function(e) {
    stop(sprintf("h5_read: Failed to read data from path '%s'. Original error: %s", 
                 path, conditionMessage(e)))
  }, finally = {
    # Ensure dataset handle is closed
    if (!is.null(dset) && inherits(dset, "H5D") && dset$is_valid) {
      close_h5_safely(dset)
    }
  })

  return(data)
}

#' Convenience wrapper to read a subset from an HDF5 dataset
#'
#' Opens the requested dataset, reads a subset using \code{$read()} and then
#' ensures the dataset handle is closed.  This avoids repetitive open/read/close
#' boilerplate when subsetting.
#'
#' @param h5 An open \code{H5File} handle.
#' @param path Character string path to the dataset within \code{h5}.
#' @param index Optional list of indices passed to \code{$read()} via the
#'   \code{index} argument.
#' @return The subset of data read from the dataset.
#' @keywords internal
h5_read_subset <- function(h5, path, index = NULL) {
  if (!inherits(h5, "H5File") || !h5$is_valid) {
    stop("h5_read_subset: 'h5' must be a valid open H5File object.")
  }
  if (!is.character(path) || length(path) != 1L || !nzchar(path)) {
    stop("h5_read_subset: 'path' must be a single, non-empty character string.")
  }

  if (!h5$exists(path)) {
    stop(sprintf("h5_read_subset: dataset '%s' not found in file '%s'",
                 path, h5$get_filename()))
  }

  dset <- NULL
  out <- NULL
  tryCatch({
    dset <- h5[[path]]
    read_args <- list()
    if (!is.null(index)) {
      if (!is.list(index)) {
        stop("h5_read_subset: 'index' must be a list when provided.")
      }
      read_args$index <- index
    }
    if (length(read_args) == 0L) {
      out <- dset$read()
    } else {
      out <- do.call(dset$read, read_args)
    }
  }, error = function(e) {
    stop(sprintf("h5_read_subset: failed reading subset from '%s': %s",
                 path, conditionMessage(e)))
  }, finally = {
    fmristore:::close_h5_safely(dset)
  })

  out
}

# -------------------------------------------------------------------------
# Helpers
# -------------------------------------------------------------------------

# -------------------------------------------------------------------------
# Helpers
# -------------------------------------------------------------------------

# Recursively create missing groups, preserving the leading "/"
ensure_h5_groups <- function(h5, group_path) {
  if (group_path == "/" || h5$exists(group_path)) return(invisible(TRUE))

  parts <- strsplit(sub("^/", "", group_path), "/", fixed = TRUE)[[1]]
  cur   <- ""                          # we build "/a", "/a/b", ...
  for (p in parts) {
    cur <- paste0(cur, "/", p)
    if (!h5$exists(cur)) h5$create_group(cur)
  }
  invisible(TRUE)
}

obj_dims <- function(x) {
  d <- dim(x)
  if (is.null(d)) if (length(x) == 1L) integer(0) else length(x) else d
}

# -------------------------------------------------------------------------
# Main writer
# -------------------------------------------------------------------------

h5_write <- function(h5, path, data,
                     dtype         = NULL,
                     chunk_dims    = NULL,
                     compression   = 0L,
                     overwrite     = FALSE,
                     create_parent = TRUE,
                     write_args    = list(),
                     verbose       = getOption("h5.write.verbose", FALSE)) {

  # -- 1. Validate --------------------------------------------------------
  stopifnot(inherits(h5, "H5File") && h5$is_valid)
  if (!is.character(path) || length(path) != 1L || !nzchar(path) || substr(path, 1, 1) != "/")
    stop("`path` must be an absolute HDF5 path (beginning with '/').")
  if (!is.numeric(compression) || compression < 0 || compression > 9)
    stop("`compression` must be an integer 0-9.")
  compression <- as.integer(compression)

  # -- 2. Ensure parent groups -------------------------------------------
  parent_path <- dirname(path)
  if (create_parent) ensure_h5_groups(h5, parent_path)
  else if (!h5$exists(parent_path))
    stop("Parent group '", parent_path, "' does not exist (create_parent = FALSE).")

  # -- 3. Overwrite logic -------------------------------------------------
  if (h5$exists(path)) {
    if (!overwrite) stop("Dataset already exists at '", path, "'.")
    h5$link_delete(path)
  }

  # -- 4. Type / dims / chunk heuristics ---------------------------------
  if (is.null(dtype)) dtype <- hdf5r::guess_dtype(data)
  if (!inherits(dtype, "H5T")) stop("`dtype` must be an 'H5T' object.")

  dims   <- obj_dims(data)
  scalar <- length(dims) == 0L
  if (scalar) { chunk_dims <- NULL; compression <- 0L }
  else if (compression > 0L && is.null(chunk_dims)) {
    ## simple heuristic: one chunk ≈ 4 MB
    dtype_size <- tryCatch(hdf5r::h5const$type_sizes[[dtype@name]], error = function(e) 8)
    nk         <- max(1L, ceiling(prod(dims) * dtype_size / (4 * 1024^2)))
    chunk_dims <- pmax(1L, floor(dims / nk))
  }

  # -- 5. Create + write --------------------------------------------------
  dset <- h5$create_dataset(name       = path,
                            dims       = dims,
                            dtype      = dtype,
                            chunk_dims = chunk_dims,
                            gzip_level = compression)

  if (length(write_args) == 0L) {
    dset$write(NULL, value = data)                # full write
  } else {
    if (is.null(write_args$robj)) write_args$robj <- data
    do.call(dset$write, write_args)
  }

  if (verbose) message("h5_write: wrote ", path)
  invisible(dset)                           # leave open; caller may close
}

# Internal helper to guess the HDF5 type from an R vector
# (already present in read_write_hdf5.R - should be centralized)
#' @keywords internal
guess_h5_type <- function(x) {
  if (is.character(x)) {
    return(hdf5r::H5T_STRING$new(size = Inf))
  } else if (is.integer(x)) {
    return(hdf5r::h5types$H5T_NATIVE_INT32)
  } else if (is.numeric(x)) {
    return(hdf5r::h5types$H5T_NATIVE_DOUBLE)
  } else if (is.logical(x)) {
    # Using INT8 for logical as HBOOL can be problematic sometimes
    # return(hdf5r::h5types$H5T_NATIVE_HBOOL) 
    return(hdf5r::h5types$H5T_NATIVE_INT8) 
  }
  stop("Unsupported R data type for guess_h5_type().")
} 

#' Safely close an HDF5 file handle
#'
#' Checks if the object is a valid H5File handle before attempting
#' to close it, silencing potential errors from double-closing etc.
#' @param h5 An object, expected to be an H5File handle.
#' @keywords internal
safe_h5_close <- function(h5) {
  if (inherits(h5, "H5File") && h5$is_valid) {
    try(h5$close_all(), silent = TRUE)
  }
} 

# Robust dtype-to-NIfTI mapper
#' @keywords internal
#' @noRd
map_dtype <- function(h5t) {
  # Use tryCatch for safety as H5T methods might fail on invalid types
  sz <- tryCatch(h5t$get_size(), error = function(e) NA_integer_)
  cls_obj <- tryCatch(h5t$get_class(), error = function(e) NULL)

  if (is.null(cls_obj) || is.na(sz)) {
     warning("Could not reliably determine HDF5 type properties (class or size). Returning DT_UNKNOWN (0).")
     return(c(0L, 0L)) # DT_UNKNOWN, bitpix 0
  }

  # Use the H5T_CLASS object's equality comparison
  is_float <- cls_obj == hdf5r::h5const$H5T_FLOAT
  is_integer <- cls_obj == hdf5r::h5const$H5T_INTEGER
  # Close the class object handle once done
  try(cls_obj$close(), silent=TRUE)

  is_signed <- NA # Default
  if (is_integer) {
      # For integers, try check the sign property
      sign_const <- tryCatch(h5t$get_sign(), error = function(e) NA_integer_) # Use tryCatch
      if (is.na(sign_const)) {
          # This might happen if get_sign isn't applicable or fails
          warning("Could not determine sign for HDF5 integer type using get_sign(). Returning DT_UNKNOWN (0).")
          return(c(0L, 0L))
      }
      # H5T_SGN_NONE (0) means unsigned, H5T_SGN_2 (2) means signed (2's complement)
      is_signed <- (sign_const == hdf5r::h5const$H5T_SGN_2)
  } else if (is_float) {
      # Assume standard floats are signed
      is_signed <- TRUE
  } else {
      # Handle other classes (String, Compound, etc.) - currently map to unknown
      is_signed <- NA
  }

  if (is.na(is_signed)) {
     warning("Unhandled HDF5 type class or failed sign detection. Returning DT_UNKNOWN (0).")
     return(c(0L, 0L))
  }

  # Construct key based on size, float status, and signedness
  key <- paste(sz, is_float, is_signed)

  switch(key,
    # Ints (is_float = FALSE)
    "1 FALSE FALSE" = c(2L, 8L),    # DT_UINT8
    "1 FALSE TRUE"  = c(256L, 8L),  # DT_INT8
    "2 FALSE FALSE" = c(512L, 16L), # DT_UINT16
    "2 FALSE TRUE"  = c(4L, 16L),   # DT_INT16
    "4 FALSE FALSE" = c(768L, 32L), # DT_UINT32
    "4 FALSE TRUE"  = c(8L, 32L),   # DT_INT32
    "8 FALSE FALSE" = c(1280L, 64L),# DT_UINT64
    "8 FALSE TRUE"  = c(1024L, 64L),# DT_INT64
    # Floats (is_float = TRUE, assumed signed = TRUE)
    "4 TRUE TRUE"   = c(16L, 32L),  # DT_FLOAT32
    "8 TRUE TRUE"   = c(64L, 64L),  # DT_FLOAT64
    # Default case
    {
      warning(paste("Unhandled HDF5 type combination:", key, "(Size, IsFloat, IsSigned). Returning DT_UNKNOWN (0)."))
      c(0L, 0L) # DT_UNKNOWN
    }
  )
}

#' Evaluate code with an open HDF5 dataset, closing it afterwards
#'
#' Opens a dataset, passes it to a function, and ensures the dataset handle is
#' closed when finished.
#'
#' @param h5 An open `H5File` handle.
#' @param path Path to the dataset within the file.
#' @param FUN Function executed with the open dataset as its single argument.
#' @return The result of `FUN`.
#' @keywords internal
with_h5_dataset <- function(h5, path, FUN) {
  ds <- NULL
  on.exit(if (!is.null(ds)) close_h5_safely(ds), add = TRUE)
  ds <- h5[[path]]
  FUN(ds)
}

#' Obtain the dimensions of an HDF5 dataset
#'
#' @inheritParams with_h5_dataset
#' @return Integer vector of dataset dimensions.
#' @keywords internal
h5_dataset_dims <- function(h5, path) {
  with_h5_dataset(h5, path, function(ds) ds$dims)
}
</file>

<file path="tests/testthat/test_labeled_vec.R">
# test-labeledVolumeSet.R

library(testthat)
library(hdf5r)
library(neuroim2)

# -------------------------------------------------------------------------
# helper: build a tiny 4-D NeuroVec + mask for round-trip tests
make_toy_vec <- function(nx = 4, ny = 3, nz = 2, nvol = 3) {
  arr  <- array(rnorm(nx * ny * nz * nvol), dim = c(nx, ny, nz, nvol))
  spc  <- neuroim2::NeuroSpace(c(nx, ny, nz, nvol), spacing = c(1, 1, 1))
  mask <- neuroim2::LogicalNeuroVol(array(runif(nx * ny * nz) > .4,
                                          dim = c(nx, ny, nz)),
                                    space = neuroim2::drop_dim(spc))
  vec  <- neuroim2::DenseNeuroVec(arr, spc)

  list(vec = vec, mask = mask)
}


test_that("Write and read a small LabeledVolumeSet", {
  # 1) Create small 4D data in memory
  set.seed(42)
  X <- 4; Y <- 5; Z <- 3; nVol <- 2  # small shape
  arr_data <- array(rnorm(X*Y*Z*nVol), dim=c(X,Y,Z,nVol))

  # 2) Create a NeuroSpace
  spc <- NeuroSpace(dim=c(X,Y,Z,nVol), spacing=c(1,1,1))

  # 3) Create a DenseNeuroVec
  #    Or if you have your own constructor: e.g. DenseNeuroVec(arr_data, space=spc)
  vec_obj <- DenseNeuroVec(arr_data, space=spc)

  # 4) Create a 3D mask (some pattern of 1/0)
  #    We'll mark ~half as valid
  mask_array <- array(sample(c(TRUE,FALSE), X*Y*Z, replace=TRUE, prob=c(0.6,0.4)), dim=c(X,Y,Z))
  mask_vol   <- LogicalNeuroVol(mask_array, space=drop_dim(spc))

  # 5) Create label names
  labels <- c("volume1", "volume2")

  # 6) Write to a temporary HDF5 file
  tmpfile <- tempfile(fileext=".h5")
  write_labeled_vec(vec=vec_obj, mask=mask_vol, labels=labels,
                    file=tmpfile, compression=4)

  # 7) Read it back as LabeledVolumeSet
  lvs <- read_labeled_vec(tmpfile)

  # 8) Check class
  expect_s4_class(lvs, "LabeledVolumeSet")
  expect_true(inherits(lvs, "NeuroVec"))
  expect_length(lvs@labels, 2)
  expect_equal(lvs@labels, labels)

  # 9) Check that sub-volumes match
  #    We'll load each volume and compare it to arr_data[,,, i] masked
  for (i in seq_along(labels)) {
    # Single volume via [[
    vol_i <- lvs[[i]]  # or lvs[[labels[i]]]
    expect_s4_class(vol_i, "DenseNeuroVol")
    # Compare data in non-zero mask
    mask_idx <- which(mask_array)
    orig_1d  <- arr_data[,,,i][mask_idx]
    read_1d  <- as.array(vol_i)[mask_idx]
    # They should match up to a small tolerance
    expect_equal(orig_1d, read_1d, tolerance=1e-8)
  }

  # 10) Test 4D subsetting with "["
  #    e.g. lvs[i_range, j_range, k_range, l_range]
  i_range <- 2:3
  j_range <- 1:4
  k_range <- 1:2
  l_range <- 1:2

  # Calculate expected result
  # Equivalent to arr_data[i_range, j_range, k_range, l_range] with mask applied
  expected_sub <- array(0, dim=c(length(i_range), length(j_range), length(k_range), length(l_range)))
  for (l_pos in seq_along(l_range)) {
    l_val <- l_range[l_pos]
    # Apply mask to this volume's data
    vol_data <- arr_data[,,,l_val] * as.numeric(mask_array)
    # Extract the requested subset
    expected_sub[,,,l_pos] <- vol_data[i_range, j_range, k_range]
  }

  sub_arr <- lvs[i_range, j_range, k_range, l_range, drop=FALSE]

  diff_array <- sub_arr - expected_sub
  diff_array[ !mask_array[i_range,j_range,k_range] ] <- 0
  expect_true( all(abs(diff_array) < 1e-8) )


  # 11) 1D linear_access style check
  #     Suppose we want indices c(1, X*Y*Z, X*Y + 2, etc.).
  #     We'll do a small set to confirm correctness
  # Not all tests show partial usage, but here's the gist:
  # (Requires 'linear_access' method for LabeledVolumeSet)
  if (any(grepl("linear_access", as.character(findMethods("linear_access"))))) {
    # Total number of elements in the 4D array (dimensions: X, Y, Z, nVol)
    tot <- X * Y * Z * nVol

    # Pick 5 random linear indices in the full 4D space.
    idx_samp <- sample.int(tot, 5)

    # Obtain the values using linear_access on the LabeledVolumeSet.
    lv <- linear_access(lvs, idx_samp)

    # The original array 'arr_data' is unmasked.
    # However, the LabeledVolumeSet only stores data for voxels where the mask is TRUE.
    # Thus, we construct an "expected" full 4D array by applying the mask.
    # Repeat the 3D mask for each volume:
    mask_4d <- array(rep(as.logical(mask_array), nVol), dim = c(X, Y, Z, nVol))
    expected_full <- arr_data * mask_4d

    # Convert the sampled linear indices to subscripts.
    subs <- arrayInd(idx_samp, .dim = dim(expected_full))

    # For each sampled index, get the corresponding value from expected_full.
    revals <- numeric(length(idx_samp))
    for (r in seq_along(idx_samp)) {
      revals[r] <- expected_full[ subs[r, 1], subs[r, 2], subs[r, 3], subs[r, 4] ]
    }

    # Compare the output of linear_access with the expected values.
    expect_equal(lv, revals, tolerance = 1e-8)
  }

  # 12) Done
  # if we want to keep tmpfile for debug, we skip unlink
  unlink(tmpfile)
})


test_that("LabeledVolumeSet array subsetting with [] works correctly", {
  # 1) Create small predictable 4D data
  set.seed(123)
  X <- 3; Y <- 3; Z <- 3; nVol <- 3 # 3x3x3x3 cube
  arr_data <- array(1:(X*Y*Z*nVol), dim=c(X,Y,Z,nVol))

  # 2) Create a NeuroSpace
  spc <- NeuroSpace(dim=c(X,Y,Z,nVol), spacing=c(1,1,1))

  # 3) Create a DenseNeuroVec
  vec_obj <- DenseNeuroVec(arr_data, space=spc)

  # 4) Create a 3D mask (simple checkerboard-like pattern)
  mask_array <- array(FALSE, dim=c(X,Y,Z))
  mask_array[seq(1, X*Y*Z, by=2)] <- TRUE # Mask roughly half
  mask_vol   <- LogicalNeuroVol(mask_array, space=drop_dim(spc))

  # 5) Create label names
  labels <- paste0("vol", 1:nVol)

  # 6) Write to a temporary HDF5 file
  tmpfile <- tempfile(fileext=".h5")
  on.exit(unlink(tmpfile), add = TRUE) # Ensure cleanup
  write_labeled_vec(vec=vec_obj, mask=mask_vol, labels=labels,
                    file=tmpfile, compression=0) # No compression for simplicity

  # 7) Read it back
  lvs <- read_labeled_vec(tmpfile)

  # 8) Define Expected Data (Original data * Mask)
  #    The LVS stores 0 where the mask is FALSE.
  mask_3d_numeric <- array(as.numeric(mask_array), dim=c(X,Y,Z))
  expected_masked_data <- array(0, dim=dim(arr_data))
  for(v in 1:nVol) {
    expected_masked_data[,,,v] <- arr_data[,,,v] * mask_3d_numeric
  }

  # 9) Perform Subsetting Tests

  # Case 1: Single element access
  expect_equal(lvs[1, 1, 1, 1, drop=FALSE], expected_masked_data[1, 1, 1, 1, drop=FALSE])
  expect_equal(lvs[1, 1, 1, 1, drop=TRUE],  expected_masked_data[1, 1, 1, 1]) # Drop to scalar
  expect_equal(lvs[2, 2, 2, 2, drop=FALSE], expected_masked_data[2, 2, 2, 2, drop=FALSE])

  # Case 2: Slice access (drop=FALSE)
  expect_equal(lvs[1, , 1, 1, drop=FALSE], expected_masked_data[1, , 1, 1, drop=FALSE])
  expect_equal(lvs[, 2, , 2, drop=FALSE], expected_masked_data[, 2, , 2, drop=FALSE])
  expect_equal(lvs[, , 3, 3, drop=FALSE], expected_masked_data[, , 3, 3, drop=FALSE])
  expect_equal(lvs[1, 1, 1, , drop=FALSE], expected_masked_data[1, 1, 1, , drop=FALSE])

  # Case 3: Slice access (drop=TRUE)
  expect_equal(lvs[1, , 1, 1, drop=TRUE], expected_masked_data[1, , 1, 1]) # Drops 1st, 3rd, 4th dim
  expect_equal(lvs[, 2, 3, 2, drop=TRUE], expected_masked_data[, 2, 3, 2]) # Drops 2nd, 3rd, 4th dim
  expect_equal(lvs[1, 2, 3, , drop=TRUE], expected_masked_data[1, 2, 3, ]) # Drops 1st, 2nd, 3rd dim

  # Case 4: Range access
  expect_equal(lvs[1:2, 1:2, 1, 1:2, drop=FALSE], expected_masked_data[1:2, 1:2, 1, 1:2, drop=FALSE])
  expect_equal(lvs[1:X, Y, 1:Z, nVol, drop=FALSE], expected_masked_data[1:X, Y, 1:Z, nVol, drop=FALSE])

  # Case 5: Non-contiguous access
  idx_i <- c(1, X)
  idx_j <- c(1, Y)
  idx_k <- c(1, Z)
  idx_l <- c(1, nVol)
  expect_equal(lvs[idx_i, idx_j, idx_k, idx_l, drop=FALSE],
               expected_masked_data[idx_i, idx_j, idx_k, idx_l, drop=FALSE])

  # Case 6: Full dimension access (missing arguments)
  expect_equal(lvs[,,,1, drop=FALSE], expected_masked_data[,,,1, drop=FALSE]) # Full 3D volume
  expect_equal(lvs[1,,,, drop=FALSE], expected_masked_data[1,,,, drop=FALSE]) # Full Y, Z, L slice

  # Case 7: Test names() method added previously
  expect_equal(names(lvs), labels)

})

test_that("Write and read with special characters in labels", {
  # 1) Create small 4D data in memory
  set.seed(42)
  X <- 4; Y <- 5; Z <- 3; nVol <- 3  # small shape
  arr_data <- array(rnorm(X*Y*Z*nVol), dim=c(X,Y,Z,nVol))

  # 2) Create a NeuroSpace
  spc <- NeuroSpace(dim=c(X,Y,Z,nVol), spacing=c(1,1,1))

  # 3) Create a DenseNeuroVec
  vec_obj <- DenseNeuroVec(arr_data, space=spc)

  # 4) Create a mask
  mask_array <- array(sample(c(TRUE,FALSE), X*Y*Z, replace=TRUE, prob=c(0.6,0.4)), dim=c(X,Y,Z))
  mask_vol   <- LogicalNeuroVol(mask_array, space=drop_dim(spc))

  # 5) Create label names with special characters
  labels <- c("volume/1", "volume 2", "volume-3")

  # 6) Write to a temporary HDF5 file
  tmpfile <- tempfile(fileext=".h5")
  on.exit(unlink(tmpfile), add = TRUE)
  
  write_labeled_vec(vec=vec_obj, mask=mask_vol, labels=labels,
                    file=tmpfile, compression=4)
                    
  # 7) Read it back
  lvs <- read_labeled_vec(tmpfile)
  
  # 8) Check that original labels are preserved
  expect_equal(lvs@labels, labels)
  
  # 9) Check actual data
  for (i in seq_along(labels)) {
    # Single volume via [[
    vol_i <- lvs[[i]]
    expect_s4_class(vol_i, "DenseNeuroVol")
    # Compare data in non-zero mask
    mask_idx <- which(mask_array)
    orig_1d  <- arr_data[,,,i][mask_idx]
    read_1d  <- as.array(vol_i)[mask_idx]
    # They should match up to a small tolerance
    expect_equal(orig_1d, read_1d, tolerance=1e-8)
  }
})

# Test for duplicate labels error
test_that("Duplicate labels are detected and reported", {
  # 1) Create small 4D data
  set.seed(42)
  X <- 4; Y <- 5; Z <- 3; nVol <- 2
  arr_data <- array(rnorm(X*Y*Z*nVol), dim=c(X,Y,Z,nVol))
  
  # 2) Create a NeuroSpace
  spc <- NeuroSpace(dim=c(X,Y,Z,nVol), spacing=c(1,1,1))
  
  # 3) Create a DenseNeuroVec
  vec_obj <- DenseNeuroVec(arr_data, space=spc)
  
  # 4) Create a mask
  mask_array <- array(sample(c(TRUE,FALSE), X*Y*Z, replace=TRUE, prob=c(0.6,0.4)), dim=c(X,Y,Z))
  mask_vol   <- LogicalNeuroVol(mask_array, space=drop_dim(spc))
  
  # 5) Create duplicate labels
  labels <- c("volume1", "volume1")
  
  # 6) Write to a temporary HDF5 file - should error
  tmpfile <- tempfile(fileext=".h5")
  on.exit(unlink(tmpfile), add = TRUE)
  
  # Expect this to error with duplicate labels
  expect_error(
    write_labeled_vec(vec=vec_obj, mask=mask_vol, labels=labels,
                      file=tmpfile, compression=4),
    "Duplicate labels detected"
  )
})

# Test for empty mask handling - should now error
test_that("Empty mask causes write_labeled_vec to error", {
  # 1) Create small 4D data
  set.seed(42)
  X <- 4; Y <- 5; Z <- 3; nVol <- 2
  arr_data <- array(rnorm(X*Y*Z*nVol), dim=c(X,Y,Z,nVol))
  
  # 2) Create a NeuroSpace
  spc <- NeuroSpace(dim=c(X,Y,Z,nVol), spacing=c(1,1,1))
  
  # 3) Create a DenseNeuroVec
  vec_obj <- DenseNeuroVec(arr_data, space=spc)

  # 4) Create an empty mask
  mask_array <- array(FALSE, dim=c(X,Y,Z))
  mask_vol   <- LogicalNeuroVol(mask_array, space=drop_dim(spc))

  # 5) Create labels
  labels <- c("volume1", "volume2")

  # 6) Write to a temporary HDF5 file - should now error
  tmpfile <- tempfile(fileext=".h5")
  on.exit(unlink(tmpfile), add = TRUE)

  # Expect an error because the mask is empty
  expect_error(
    write_labeled_vec(vec=vec_obj, mask=mask_vol, labels=labels,
                      file=tmpfile, compression=4),
    regexp = "Mask is empty"
  )

  # 7) Verify the file was NOT created (or is empty if error occurred after creation)
  expect_false(file.exists(tmpfile) && file.info(tmpfile)$size > 0)
})


test_that("write_labeled_vec → read_labeled_vec round-trip is loss-less", {
  skip_if_not_installed("hdf5r")
  tmp <- withr::local_tempfile(fileext = ".h5")

  toy <- make_toy_vec()
  vec   <- toy$vec
  mask  <- toy$mask
  lbls  <- paste0("vol_", seq_len(dim(vec)[4]))

  # ---------- write ----------
  
  write_labeled_vec(vec, mask, lbls, tmp,
                      compression = 6,       # make sure deflate path is used
                      chunk_size  = 5)       # exercise chunk logic
  

  # ---------- read ----------
  lset <- read_labeled_vec(tmp)
  expect_s4_class(lset, "LabeledVolumeSet")
  expect_identical(lset@labels, lbls)

  # header / space
  expect_equal(dim(space(lset@mask)),
               dim(neuroim2::space(vec))[1:3])
  expect_equal(neuroim2::spacing(space(lset@mask)),
               neuroim2::spacing(neuroim2::space(vec))[1:3])

  # mask round-trip
  expect_identical(as.logical(as.array(lset@mask)),
                   as.logical(as.array(mask)))

  # each volume's voxels (only inside mask for speed)
  idx <- which(as.logical(as.array(mask)))
  for (v in seq_along(lbls)) {
    orig <- vec[idx + (v - 1) * prod(dim(mask))]
    read <- linear_access(lset,
                          idx + (v - 1) * prod(dim(mask)))
    expect_equal(read, orig, tolerance = 1e-10)
  }
})

test_that("label sanitisation & empty-mask corner cases behave correctly", {
  skip_if_not_installed("hdf5r")

  toy   <- make_toy_vec(nx = 2, ny = 2, nz = 1, nvol = 2)
  vec   <- toy$vec
  mask  <- toy$mask

  # ------------------------------------------------------------------
  # (a) duplicate after sanitisation  →  hard error
  dup_labels <- c("bad/label", "bad label")   # both sanitise to "bad_label"
  path <- paste0(withr::local_tempfile(), ".h5")
  expect_error(
    write_labeled_vec(vec, mask, dup_labels, path),
    "Duplicate labels",
    fixed = TRUE
  )

  # ------------------------------------------------------------------
  # (b) empty mask still produces valid zero volumes round-trip
  empty_mask <- neuroim2::LogicalNeuroVol(array(FALSE,
                                                dim = dim(mask)),
                                          space = space(mask))
  tmp <- withr::local_tempfile()
  tmp <- paste0(tmp, ".h5")
 
  expect_error(write_labeled_vec(vec, empty_mask,
                      labels = c("volA", "volB"), file = tmp),
               regexp = "Mask is empty")
  
})

# New test case for local_tempfile path is respected
test_that("local_tempfile path is respected", {
  skip_if_not_installed("hdf5r")

  # Create a DenseNeuroVec and an empty mask
  spc <- NeuroSpace(dim = c(2,2,1,2))
  vec <- DenseNeuroVec(array(1:8, dim=c(2,2,1,2)), space=spc)
  mask_arr <- array(FALSE, dim=c(2,2,1))
  mask <- LogicalNeuroVol(mask_arr, drop_dim(spc))

  tmp <- withr::local_tempfile()
  tmp <- paste0(tmp, ".h5")
  # Expect error when writing with empty mask
  expect_error(
    write_labeled_vec(vec = vec,
                      mask = mask,
                      labels = c("volA", "volB"),
                      file = tmp),
    regexp = "Mask is empty"
  )

  # Verify the file wasn't created
  expect_false(file.exists(tmp))
})



# Regression: label round-trip without /original_labels dataset

test_that("labels persist without /original_labels", {
  skip_if_not_installed("hdf5r")

  toy <- make_toy_vec(nx = 2, ny = 2, nz = 1, nvol = 2)
  vec  <- toy$vec
  mask <- toy$mask
  lbls <- c("cond/A", "cond B")

  tmp <- withr::local_tempfile(fileext = ".h5")
  write_labeled_vec(vec, mask, lbls, tmp)

  h5 <- hdf5r::H5File$new(tmp, "r")
  expect_false(h5$exists("/original_labels"))
  h5$close_all()

  lvs <- read_labeled_vec(tmp)
  expect_identical(lvs@labels, lbls)
  close(lvs)
})
</file>

<file path="R/latent_vec.R">
#' @importFrom neuroim2 matricized_access
NULL

#' Latent Space Representation of Neuroimaging Data
#'
#' @name LatentNeuroVec-class
#' @description
#' The \code{LatentNeuroVec} class provides a memory-efficient representation of neuroimaging
#' data in a latent space (e.g., from PCA or ICA). It stores data as a product of basis vectors
#' and loadings, allowing for efficient storage and computation of high-dimensional
#' neuroimaging data.
#'
#' @section Implementation Details:
#' The class implements a matrix factorization approach where the data is represented as:
#' \deqn{X = B \times L^T + c}
#' where:
#' \itemize{
#'   \item B is the basis matrix (\eqn{n \times k})
#'   \item L is the loadings matrix (\eqn{p \times k})
#'   \item c is an optional offset vector
#'   \item n is the number of time points
#'   \item p is the number of voxels
#'   \item k is the number of components
#' }
#'
#' @seealso
#' \code{\link[neuroim2]{NeuroVec-class}} for the base 4D brain image class.
#' \code{\link[neuroim2]{AbstractSparseNeuroVec-class}} for the sparse representation framework.
NULL

#' Internal Constructor for LatentNeuroVecSource
#'
#' @description
#' Creates a new LatentNeuroVecSource object for handling file-backed storage.
#'
#' @param file_name Character string specifying the path to the HDF5 file.
#' @return A new \code{LatentNeuroVecSource} object.
#' @keywords internal
#' @noRd
LatentNeuroVecSource <- function(file_name) {
  if (!is.character(file_name) || length(file_name) != 1) {
    stop("'file_name' must be a single character string")
  }
  if (!file.exists(file_name)) {
    stop("File '", file_name, "' does not exist")
  }
  new("LatentNeuroVecSource", file_name=file_name)
}

#' Construct a LatentNeuroVec Object
#'
#' @title Create a Latent Space Representation of Neuroimaging Data
#' @description
#' Constructs a \code{\link{LatentNeuroVec-class}} object, which provides a memory-efficient
#' representation of neuroimaging data using matrix factorization. This is particularly useful
#' for dimensionality reduction techniques (e.g., PCA or ICA).
#'
#' @param basis A numeric or \code{Matrix} object (\eqn{n \times k}) containing the temporal basis.
#' @param loadings A numeric or \code{Matrix} object (\eqn{p \times k}) containing spatial loadings.
#' @param space A \code{\link[neuroim2]{NeuroSpace-class}} defining the spatial/temporal dims.
#' @param mask A \code{\link[neuroim2]{LogicalNeuroVol-class}} defining the brain mask.
#' @param offset Optional numeric vector of length \eqn{p} (voxel-wise offsets).
#' @param label Optional character label for the object.
#'
#' @return A new \code{\link{LatentNeuroVec-class}} instance.
#'
#' @examples
#' \dontrun{
#' library(Matrix)
#' library(neuroim2)
#'
#' # Example data
#' n_timepoints <- 100
#' n_components <- 10
#' n_voxels <- 1000
#'
#' # Create basis & loadings
#' basis <- Matrix(rnorm(n_timepoints * n_components),
#'                 nrow = n_timepoints,
#'                 ncol = n_components)
#' loadings <- Matrix(rnorm(n_voxels * n_components),
#'                    nrow = n_voxels,
#'                    ncol = n_components,
#'                    sparse = TRUE)
#'
#' # Create space (10x10x10 volume, 100 timepoints)
#' spc <- NeuroSpace(c(10,10,10,n_timepoints))
#'
#' # Create mask
#' mask_array <- array(TRUE, dim=c(10,10,10))
#' mask_vol   <- LogicalNeuroVol(mask_array, NeuroSpace(c(10,10,10)))
#'
#' # Construct LatentNeuroVec
#' lvec <- LatentNeuroVec(basis = basis,
#'                        loadings = loadings,
#'                        space = spc,
#'                        mask = mask_vol)
#' }
#'
#' @export
LatentNeuroVec <- function(basis, loadings, space, mask, offset = NULL, label = "") {
  # Validate 'space'
  if (!inherits(space, "NeuroSpace")) {
    stop("'space' must be a NeuroSpace object")
  }
  # Validate 'basis' / 'loadings'
  if (!is.matrix(basis) && !inherits(basis, "Matrix")) {
    stop("'basis' must be a matrix or Matrix object")
  }
  if (!is.matrix(loadings) && !inherits(loadings, "Matrix")) {
    stop("'loadings' must be a matrix or Matrix object")
  }

  # Ensure we have a LogicalNeuroVol for mask
  # Initialize space_for_map using the main 3D space first
  space_3d <- drop_dim(space)
  space_for_map <- space_3d 

  
  if (!inherits(mask, "LogicalNeuroVol")) {
    # Construct 3D space from the main 4D space
    # space_3d already created above
    mask <- LogicalNeuroVol(as.logical(mask), space_3d)
  } else {
    # If mask is already a LogicalNeuroVol, ensure its space matches the 3D part of the main space
    # space_3d already created above
    mask_space <- neuroim2::space(mask)
    # Use check_same_dims for the dimension check
    check_same_dims(mask_space, space_3d, 
                    msg = "Space dimensions of provided mask do not match dimensions derived from the main 4D space.")
    # Keep the check for space equality separate for now
    if (!isTRUE(all.equal(mask_space, space_3d))) { 
        stop("Space object of provided mask does not match the space derived from the main 4D space. Cannot create IndexLookupVol.")
    }
  }

  cardinality <- sum(mask)

  # Handle offset: treat NULL or empty numeric offset as no offset
  if (is.null(offset) || length(offset) == 0) {
    # No offset provided or empty offset, use numeric(0)
    offset <- numeric(0)
  } else {
    # Check that provided offset matches loadings rows
    if (length(offset) != nrow(loadings)) {
      stop("'offset' length must match number of rows in 'loadings'")
    }
  }

  # Dimension checks - check columns first
  if (ncol(loadings) != ncol(basis)) {
    stop("'basis' and 'loadings' must have the same number of columns")
  }
  if (nrow(basis) != dim(space)[4]) {
    stop("'basis' must have ", dim(space)[4], " rows (the 4th dimension of space)")
  }
  if (nrow(loadings) != cardinality) {
    stop(paste0("'loadings' must have ", cardinality, " rows (i.e. #non-zero in mask)"))
  }

  # Ensure all numeric inputs are finite
  if (!all(is.finite(basis))) {
    stop("'basis' must contain only finite values")
  }
  if (!all(is.finite(loadings))) {
    stop("'loadings' must contain only finite values")
  }
  if (length(offset) > 0 && !all(is.finite(offset))) {
    stop("'offset' must contain only finite values")
  }

  # Convert basis/loadings to Matrix objects, choosing dense/sparse based on density
  if (is.matrix(basis) && !is(basis, "Matrix")) {
      density_basis <- sum(basis != 0) / length(basis)
      if (density_basis > 0.5) {
          message("Input 'basis' is dense (", round(density_basis * 100), "% non-zero); storing as dense dgeMatrix.")
          basis <- Matrix::Matrix(basis, sparse = FALSE)
      } else {
          basis <- Matrix::Matrix(basis)
      }
  } # else: Already a Matrix object
  
  if (is.matrix(loadings) && !is(loadings, "Matrix")) {
      density_loadings <- sum(loadings != 0) / length(loadings)
      if (density_loadings > 0.5) {
           message("Input 'loadings' is dense (", round(density_loadings * 100), "% non-zero); storing as dense dgeMatrix.")
           loadings <- Matrix::Matrix(loadings, sparse = FALSE)
      } else {
          loadings <- Matrix::Matrix(loadings)
      }
  } # else: Already a Matrix object

  # Check component count
  k <- ncol(basis)
  if (k < 1) {
    stop("Number of components (columns in basis and loadings) must be >= 1")
  }

  # Create the object
  new("LatentNeuroVec",
      basis    = basis,
      loadings = loadings,
      space    = space,
      mask     = mask,
      map      = IndexLookupVol(space_for_map, as.integer(which(mask@.Data))), # Use determined space
      offset   = offset,
      label    = label)
}

#' Write LatentNeuroVec to HDF5 File
#'
#' @description
#' Writes a \code{LatentNeuroVec} to an HDF5 file with optional compression.
#'
#' @param x A \code{\link[neuroim2]{LatentNeuroVec-class}} to write.
#' @param file_name \code{character} file path to the output HDF5.
#' @param nbit \code{logical}; if TRUE, uses N-bit compression (default: FALSE).
#' @param compression \code{integer} in [1..9] specifying compression level (default: 9).
#' @param chunk_dim Optional numeric vector specifying chunk dimensions.
#'
#' @return Invisible \code{NULL}, called for side effects (writes to disk).
#'
#' @details
#' This function saves:
#' \itemize{
#'   \item \code{basis} matrix
#'   \item \code{loadings} matrix
#'   \item \code{offset} vector
#'   \item Spatial metadata
#'   \item Mask information
#' }
#' all inside an HDF5 file for future loading.
#'
#' @examples
#' if (requireNamespace("neuroim2", quietly = TRUE) &&
#'     requireNamespace("hdf5r", quietly = TRUE) &&
#'     requireNamespace("Matrix", quietly = TRUE) && 
#'     exists("write_vec", where = "package:neuroim2") &&
#'     !is.null(fmristore:::create_minimal_LatentNeuroVec)) {
#'   
#'   lnv <- NULL
#'   temp_h5_file <- NULL
#'   
#'   tryCatch({
#'     # Create a minimal LatentNeuroVec
#'     lnv <- fmristore:::create_minimal_LatentNeuroVec(
#'       space_dims = c(3L, 3L, 2L),
#'       n_time = 5L,
#'       n_comp = 2L
#'     )
#'     
#'     # Create a temporary file for output
#'     temp_h5_file <- tempfile(fileext = ".h5")
#'     
#'     # Write the LatentNeuroVec to HDF5
#'     write_vec(lnv, temp_h5_file, compression = 6)
#'     
#'     # Verify file was created
#'     if (file.exists(temp_h5_file)) {
#'       cat("Successfully wrote LatentNeuroVec to:", temp_h5_file, "\n")
#'     }
#'     
#'   }, error = function(e) {
#'     message("write_vec example failed: ", e$message)
#'   }, finally = {
#'     # Clean up temporary file
#'     if (!is.null(temp_h5_file) && file.exists(temp_h5_file)) {
#'       unlink(temp_h5_file)
#'     }
#'   })
#' }
#'
#' @seealso
#' \code{\link[neuroim2]{read_vec}} for reading back the file.
#'
#' @importFrom neuroim2 write_vec
#' @rdname write_vec-methods
#' @export
setMethod(
  f = "write_vec",
  signature = signature(x="LatentNeuroVec", file_name="character", format="missing", data_type="missing"),
  definition = function(x, file_name, compression=9, nbit=FALSE) {
    if (!is.character(file_name) || length(file_name) != 1) {
      stop("'file_name' must be a single character string")
    }
    if (!is.numeric(compression) || compression < 1 || compression > 9) {
      stop("'compression' must be an integer between 1 and 9")
    }

    # Call internal writer. It handles opening/closing the file via on.exit.
    h5obj <- to_h5_latentvec(
      vec        = x,
      file_name  = file_name,
      compression= compression,
      nbit = nbit
    )
    # obj$close() # REMOVED: to_h5_latentvec manages closure via on.exit
    
    invisible(NULL)
  }
)

#' Internal Matrix-Based Access for LatentNeuroVec
#'
#' @description
#' Internal method providing efficient matrix-based access to elements.
#'
#' @param x A \code{LatentNeuroVec}.
#' @param i Index specification.
#' @return Computed values.
#'
#' @keywords internal
#' @noRd
setMethod(
  f = "matricized_access",
  signature = signature(x="LatentNeuroVec", i="matrix"),
  definition = function(x, i) {
    if (!is.numeric(i) || ncol(i) != 2L)
      stop("`i` must be a numeric matrix with 2 columns (time, spatial-index)")

    ## ── 1. split and sanity-check the two index columns ───────────────────────
    t_idx <- as.integer(i[, 1L])                       # time rows in @basis
    s_idx <- as.integer(i[, 2L])                       # spatial indices 1..X·Y·Z

    nt  <- nrow(x@basis)
    nxy <- prod(dim(x)[1:3])

    if (any(t_idx < 1L | t_idx > nt))
      stop("time index out of bounds")
    if (any(s_idx < 1L | s_idx > nxy))
      stop("spatial index out of bounds")

    ## ── 2. map spatial -> mask rows  (0 means 'outside the mask') ─────────────
    v_idx <- lookup(x@map, s_idx)                      # 0 / 1..nVox

    inside <- v_idx > 0L
    out    <- numeric(length(t_idx))                   # zeros by default

    if (any(inside)) {

      ## ── 3. gather the relevant rows  --------------------------------------
      b1 <- x@basis   [ t_idx[inside] , , drop = FALSE ]   # n_inside × k
      b2 <- x@loadings[ v_idx[inside], , drop = FALSE ]    # n_inside × k

      ## ── 4. pair-wise dot product + offset  --------------------------------
      out[inside] <- rowSums(b1 * b2) + x@offset[v_idx[inside]]
    }

    out
  }
)

#' @keywords internal
#' @noRd
#' @importFrom neuroim2 matricized_access
setMethod(
  f = "matricized_access",
  signature = signature(x="LatentNeuroVec", i="integer"),
  definition = function(x, i) {
    # Ensure component dimensions match
    stopifnot("[matricized_access,LatentNeuroVec,integer] Number of components mismatch." = 
              ncol(x@basis) == ncol(x@loadings))
              
    if (any(i < 1) || any(i > nrow(x@loadings))) {
      stop("Index out of bounds for 'loadings'")
    }
    b1 <- x@basis
    b2 <- x@loadings[as.integer(i),, drop=FALSE] # Use as.integer explicitly
    out <- tcrossprod(b1, b2)
    # Add offsets
    as.matrix(sweep(out, 2, x@offset[as.integer(i)], "+")) # Use as.integer here too
  }
)

#' @keywords internal
#' @noRd
setMethod(
  f = "matricized_access",
  signature = signature(x="LatentNeuroVec", i="numeric"),
  definition = function(x, i) {
    # Coerce to integer and call the integer method using callNextMethod
    callNextMethod(x = x, i = as.integer(i)) 
  }
)

#' @export 
#' @rdname linear_access-methods
setMethod(
  f = "linear_access",
  signature = signature(x="LatentNeuroVec", i="numeric"),
  definition = function(x, i) {
    # Coerce to integer and call the integer method using callNextMethod
    callNextMethod(x = x, i = as.integer(i)) 
  }
)

#' Internal Linear Access Method for LatentNeuroVec
#'
#' @description
#' Internal method providing linear access to elements.
#'
#' @param x A \code{LatentNeuroVec}.
#' @param i A numeric vector of indices.
#'
#' @return Computed values
#' @keywords internal
#' @noRd
setMethod(
  f = "linear_access",
  signature = signature(x="LatentNeuroVec", i="integer"),
  definition = function(x, i) {
    
    dims_full <- dim(x) # Get 4D dimensions
    nels_4d <- prod(dims_full)
    nels_3d <- prod(dims_full[1:3])
    n_time <- dims_full[4]
    
    # 1. Validate 4D linear indices
    if (!is.numeric(i) || any(is.na(i))) {
      stop("[linear_access,LatentNeuroVec] Index `i`` must be numeric without NA values")
    }
    if (any(i < 1) || any(i > nels_4d)) {
      stop(paste0("[linear_access,LatentNeuroVec] Index out of bounds for 4D volume [1..", nels_4d, "]"))
    }
    
    # 2. Convert 4D linear indices to 3D spatial index + time index
    # Time index (1-based)
    time_idx <- ceiling(i / nels_3d) 
    # 3D spatial linear index (1-based)
    spatial_idx_3d <- i %% nels_3d
    spatial_idx_3d[spatial_idx_3d == 0] <- nels_3d

    # 3. Map 3D spatial indices to mask indices (rows in loadings/offset)
    rowmap <- lookup(x@map, spatial_idx_3d) # Length = length(i)
    
    # 4. Identify unique needed mask indices and time indices
    unique_valid_mask_idx <- unique(rowmap[rowmap > 0])
    unique_time_idx <- unique(time_idx)
    
    # 5. If no requested indices fall within the mask, return zeros
    if (length(unique_valid_mask_idx) == 0) {
      return(numeric(length(i))) # Return vector of zeros
    }

    # --- Optimization Start ---
    # Pre-calculate transposed loadings (k x nVox_mask) - used multiple times
    t_loadings <- t(x@loadings) 
    basis_subset <- x@basis[unique_time_idx, , drop = FALSE] # [nUniqueTime, k]
    
    # Shortcut for single time point query
    if (length(unique_time_idx) == 1L) {
      # Compute values only for valid mask indices for this single time point
      # basis_subset [1,k] %*% t_loadings[k, unique_valid_mask_idx] => [1, nUniqueValidMask]
      computed_vals <- drop(basis_subset %*% t_loadings[, unique_valid_mask_idx, drop = FALSE])
      # Add offset if it exists
      if (length(x@offset) > 0) {
          computed_vals <- computed_vals + x@offset[unique_valid_mask_idx]
      }
      
      # Map these computed values back to the original query indices
      ovals <- numeric(length(i)) # Initialize with zeros
      # Find which original indices correspond to the computed valid mask indices
      original_indices_for_computed <- match(rowmap, unique_valid_mask_idx)
      # Select only those that were actually computed (non-NA in the match)
      valid_output_positions <- !is.na(original_indices_for_computed)
      # Place computed values into the correct spots in the output vector
      ovals[valid_output_positions] <- computed_vals[original_indices_for_computed[valid_output_positions]]
      return(ovals)
    }
    # --- Optimization End ---

    # 6. Calculate the required data block for multiple time points
    # Result is [length(unique_time_idx), length(unique_valid_mask_idx)]
    # Use pre-calculated t_loadings and basis_subset
    data_block <- basis_subset %*% t_loadings[, unique_valid_mask_idx, drop = FALSE]
    
    # 7. Add offsets if they exist
    if (length(x@offset) > 0) {
        data_block <- sweep(data_block, 2, x@offset[unique_valid_mask_idx], "+")
    }
    
    # 8. Create the output vector
    ovals <- numeric(length(i))
    
    # 9. Map results back to the original requested indices
    #    Need to find the correct row (time) and column (mask index) in data_block
    #    for each element of the original index 'i'.
    
    # Create lookup maps for row/column indices in data_block
    time_map <- match(time_idx, unique_time_idx)
    mask_map <- match(rowmap, unique_valid_mask_idx) # NA for out-of-mask
    
    # Identify indices that were *in* the mask
    in_mask_selector <- which(rowmap > 0)
    
    # Extract the corresponding row and column indices *within data_block*
    row_indices_in_block <- time_map[in_mask_selector]
    col_indices_in_block <- mask_map[in_mask_selector]
    
    # Convert 2D indices (row, col) in data_block to linear indices
    linear_indices_in_block <- row_indices_in_block + (col_indices_in_block - 1) * nrow(data_block)
    
    # Assign values from data_block to the output vector
    ovals[in_mask_selector] <- data_block[linear_indices_in_block]
    
    return(ovals)
  }
)

#' Extract a Single Volume from LatentNeuroVec
#'
#' @description
#' Extracts a single volume from a \code{LatentNeuroVec} as a \code{SparseNeuroVol}.
#'
#' @param x A \code{\link[neuroim2]{LatentNeuroVec-class}} object.
#' @param i A numeric index specifying which volume to extract (must be a single value).
#'
#' @return A \code{\link[neuroim2]{SparseNeuroVol-class}} containing:
#' \itemize{
#'   \item The computed volume data
#'   \item A new 3D \code{NeuroSpace} object
#'   \item The original spatial indices
#' }
#'
#' @details
#' 1. Validates index \code{i}
#' 2. Computes the volume data with matrix operations
#' 3. Adds the offset
#' 4. Builds a new 3D \code{NeuroSpace}
#' 5. Returns a \code{SparseNeuroVol}
#'
#' @examples
#' \dontrun{
#' # Extract volumes
#' vol1 <- lvec[[1]]
#' vol_mid <- lvec[[dim(lvec)[4] / 2]]
#' vol_last <- lvec[[dim(lvec)[4]]]
#' }
#'
#' @seealso
#' \code{\link[neuroim2]{SparseNeuroVol-class}} for the return type,
#' \code{\link[neuroim2]{NeuroSpace-class}} for spatial metadata.
#'
#' @rdname extract-methods
#' @export
# single volume at time i
setMethod("[[", signature(x="LatentNeuroVec", i="numeric"),
          function(x, i) {
            if (length(i) != 1) stop("Index must be a single number")
            if (i < 1 || i > dim(x)[4]) stop("Index out of range")

            # basis[i,,drop=FALSE] => shape (1 x k)
            # loadings => shape (p x k)
            # We want (1 x p).
            # Calculate using tcrossprod(B, L) for B %*% t(L)
            # tcrossprod(x@basis[i,,drop=FALSE], x@loadings) => (1 x k) %*% (k x p) => (1 x p)
            dat <- as.numeric(tcrossprod(x@basis[i,,drop=FALSE], x@loadings))
            dat <- dat + x@offset  # length p

            # Now place them in a SparseNeuroVol with the known mask
            newdim <- dim(x)[1:3]
            bspace <- NeuroSpace(newdim,
                                 spacing=neuroim2::spacing(x),
                                 origin=neuroim2::origin(x),
                                 axes=neuroim2::axes(x@space),
                                 trans=neuroim2::trans(x))

            SparseNeuroVol(dat, bspace, indices=neuroim2::indices(x))
          }
)

#' 4D subsetting for LatentNeuroVec
#'
#' @description
#' Allows \code{latent_vec[i, j, k, l]} style subsetting:
#' \itemize{
#'   \item \code{i,j,k} are currently assumed missing or full range
#'   \item \code{l} can be a numeric vector (subset of timepoints)
#' }
#'
#' @param x A \code{LatentNeuroVec} object
#' @param i,j,k Numeric index vectors for the 3D spatial dims. If missing,
#'   all voxels are included.
#' @param l Numeric index vector (timepoints). If missing, all time.
#' @param drop \code{logical} drop dims of size 1?
#' @param ... Not used
#'
#' @return An array with shape \code{[length(i), length(j), length(k), length(l)]}
#'   or, if \code{i,j,k} are missing, \code{[dim(x)[1], dim(x)[2], dim(x)[3], length(l)]}.
#'   Then we reconstruct each timepoint on-the-fly using \code{basis[l, ]} and
#'   \code{loadings}, plus \code{offset}.
#'
#' @export
setMethod(
  f = "[",
  signature = signature(x="LatentNeuroVec", i="numeric", j="numeric", drop="ANY"),
  definition = function(x, i, j, k, l, ..., drop=TRUE) {

    # Determine full dimensions
    dims_full <- dim(x)
    if (missing(i)) i <- seq_len(dims_full[1])
    if (missing(j)) j <- seq_len(dims_full[2])
    if (missing(k)) k <- seq_len(dims_full[3])
    if (missing(l)) l <- seq_len(dims_full[4])

    # Basic bounds check
    if (any(i < 1 | i > dims_full[1]) || any(j < 1 | j > dims_full[2]) ||
        any(k < 1 | k > dims_full[3]) || any(l < 1 | l > dims_full[4])) {
      stop("Subscript out of range for LatentNeuroVec.")
    }

    i <- as.integer(i)
    j <- as.integer(j)
    k <- as.integer(k)
    l <- as.integer(l)

    # Calculate the output dimensions
    out_dim <- c(length(i), length(j), length(k), length(l))
    n_vox_req <- prod(out_dim[1:3]) # Number of voxels requested in the output slice
    n_time_req <- out_dim[4]

    # 1. Map requested (i, j, k) coordinates to 1D indices in the full 3D space
    #    Avoid `expand.grid(i, j, k)` which can allocate a very large
    #    intermediate data frame.  Instead compute linear indices in chunks
    #    with `k` as the outer loop and the i/j grid precomputed once.
    dims_space_3d <- dims_full[1:3]
    linear_idx_3d <- integer(n_vox_req)

    ij_grid <- outer(i, j, function(ii, jj) ii + (jj - 1L) * dims_space_3d[1])
    ij_step <- length(i) * length(j)

    for (kk in seq_along(k)) {
      start <- (kk - 1L) * ij_step + 1L
      end <- kk * ij_step
      linear_idx_3d[start:end] <-
        ij_grid + (k[kk] - 1L) * dims_space_3d[1] * dims_space_3d[2]
    }
    
    # 2. Map these 3D linear indices to rows in loadings/offset using the mask map
    #    `rowmap` will have 0 for out-of-mask voxels.
    rowmap <- lookup(x@map, linear_idx_3d) # Length = n_vox_req
    valid_mask_indices <- rowmap[rowmap > 0] # Indices within loadings/offset needed
    map_req_to_valid <- which(rowmap > 0)   # Mapping from requested voxel pos to valid pos

    # Initialize result array with zeros
    result <- array(0, dim = out_dim)

    # 3. If no requested voxels are within the mask, return the zero array
    if (length(valid_mask_indices) == 0) {
        if (drop) return(drop(result))
        return(result)
    }

    # 4. Calculate the requested time points for the *valid* voxels
    #    basis_subset (length(l) x k) %*% loadings[valid_mask_indices, k]^T 
    #    => result_valid (length(l) x length(valid_mask_indices))
    basis_subset <- x@basis[l, , drop = FALSE]
    # Use tcrossprod, subsetting loadings directly
    result_valid <- tcrossprod(basis_subset, x@loadings[valid_mask_indices, , drop = FALSE])

    # 5. Add offset (only for the valid mask indices)
    #    Use sweep along the columns (MARGIN=2) of result_valid
    result_valid <- sweep(result_valid, 2, x@offset[valid_mask_indices], "+")

    # 6. Place the calculated values into the final result array using linear indexing
    #    `map_req_to_valid` gives the linear index (1..n_vox_req) within each 3D slice
    #    for each valid column in `result_valid`.
    #    The `result` array is arranged [x, y, z, t].
    #    Linear index formula: voxel_linear_index + (time_index - 1) * n_voxels_per_slice
    n_vox_per_slice <- prod(out_dim[1:3])
    for (col_idx in seq_along(valid_mask_indices)) {
      # Get the linear index for this voxel within a *single* 3D slice of the output
      voxel_linear_idx_in_slice <- map_req_to_valid[col_idx]
      # Calculate the linear indices for *all time points* for this specific voxel
      # result_valid[, col_idx] holds the time series for this voxel
      linear_indices_in_result <- voxel_linear_idx_in_slice + (seq_len(n_time_req) - 1L) * n_vox_per_slice
      # Assign the time series to the calculated linear indices in the result array
      result[linear_indices_in_result] <- result_valid[, col_idx]
    }

    # --- Remove old loop ---
    # for(t_idx in seq_len(n_time_req)) {
    #     # Create a temporary 3D slice filled with zeros
    #     slice_3d <- array(0, dim = out_dim[1:3])
    #     # Place the valid voxel values for this time point into the slice
    #     # The t_idx row of result_valid corresponds to the current time point
    #     # The columns correspond to valid_mask_indices, mapped by map_req_to_valid
    #     slice_3d[map_req_to_valid] <- result_valid[t_idx, ]
    #     # Assign the populated slice to the result array
    #     result[,,, t_idx] <- slice_3d
    # }
    
    # 7. Drop dimensions if requested
    if (drop) result <- drop(result)
    result
  }
)

#' Concatenate LatentNeuroVec Objects
#'
#' @description
#' Concatenates two or more \code{LatentNeuroVec} objects along the temporal dimension.
#'
#' @param x First \code{LatentNeuroVec}.
#' @param y Second \code{LatentNeuroVec}.
#' @param ... Additional \code{LatentNeuroVec} objects to concatenate.
#'
#' @return A \code{\link[neuroim2]{NeuroVecSeq-class}} containing all input vectors.
#'
#' @details
#' 1. Checks for compatibility
#' 2. Creates a sequence container
#' 3. Preserves individual object properties
#'
#' @examples
#' \dontrun{
#' # Concatenate multiple LatentNeuroVec objects
#' combined <- concat(lvec1, lvec2, lvec3)
#' dim(combined)
#' }
#'
#' @seealso
#' \code{\link[neuroim2]{NeuroVecSeq-class}}
#'
#' @rdname concat-methods
#' @export
setMethod(
  f = "concat",
  signature = signature(x="LatentNeuroVec", y="LatentNeuroVec"),
  definition = function(x, y, ...) {
    # Get additional objects if any
    additional <- list(...)
    all_objects <- c(list(x, y), additional)
    all_lvecs <- all(sapply(all_objects, is, "LatentNeuroVec"))
    
    if (!all_lvecs) {
      # If not all objects are LatentNeuroVec, fall back to NeuroVecSeq
      return(do.call(NeuroVecSeq, list(x, y, ...)))
    }
    
    # Check if all objects have compatible spatial components
    # 1. Same dimensions (except time)
    # 2. Same mask
    # 3. Same column count in loadings (k value)
    
    # Check dimensions
    x_space <- space(x)
    x_dims_3d <- dim(x_space)[1:3]
    compatible_dims <- TRUE
    
    for (obj in all_objects[-1]) {  # Skip x
      obj_space <- space(obj)
      obj_dims_3d <- dim(obj_space)[1:3]
      # Use validate_same_dims and check if the result is NULL (success)
      validation_result <- validate_same_dims(x_dims_3d, obj_dims_3d, dims_to_compare = 1:3)
      if (!is.null(validation_result)) { # If result is NOT NULL, there was an error
        compatible_dims <- FALSE
        # Optionally, we could log the validation_result here
        # message("Dimension mismatch detected: ", validation_result)
        break
      }
    }
    
    if (!compatible_dims) {
      # Fall back to NeuroVecSeq if dimensions don't match
      return(do.call(NeuroVecSeq, list(x, y, ...)))
    }
    
    # Check masks
    x_mask_array <- as.array(mask(x))
    compatible_masks <- TRUE
    
    for (obj in all_objects[-1]) {  # Skip x
      obj_mask_array <- as.array(mask(obj))
      if (!identical(x_mask_array, obj_mask_array)) {
        compatible_masks <- FALSE
        break
      }
    }
    
    if (!compatible_masks) {
      # Fall back to NeuroVecSeq if masks don't match
      return(do.call(NeuroVecSeq, list(x, y, ...)))
    }
    
    # Check k values (component count)
    x_k <- ncol(x@loadings)
    compatible_k <- TRUE
    
    for (obj in all_objects[-1]) {  # Skip x
      obj_k <- ncol(obj@loadings)
      if (x_k != obj_k) {
        compatible_k <- FALSE
        break
      }
    }
    
    if (!compatible_k) {
      # Fall back to NeuroVecSeq if component counts don't match
      return(do.call(NeuroVecSeq, list(x, y, ...)))
    }
    
    # Check loadings - they must be identical for this optimization
    compatible_loadings <- TRUE
    x_loadings <- x@loadings
    
    for (obj in all_objects[-1]) {  # Skip x
      if (!identical(as.matrix(x_loadings), as.matrix(obj@loadings))) {
        compatible_loadings <- FALSE
        break
      }
    }
    
    if (!compatible_loadings) {
      # Fall back to NeuroVecSeq if loadings don't match
      return(do.call(NeuroVecSeq, list(x, y, ...)))
    }
    
    # If we get here, all spatial components are compatible
    # We can create a new LatentNeuroVec with concatenated basis matrices
    
    # Collect all basis matrices
    all_basis <- lapply(all_objects, function(obj) obj@basis)
    
    # Collect all time dimensions
    time_dims <- sapply(all_objects, function(obj) dim(obj@space)[4])
    total_time <- sum(time_dims)
    
    # Create new space with updated time dimension
    new_space_dims <- dim(x_space)
    new_space_dims[4] <- total_time
    new_space <- NeuroSpace(
      dim = new_space_dims,
      spacing = spacing(x_space),
      origin = origin(x_space),
      axes = axes(x_space),
      trans = trans(x_space)
    )
    
    # Concatenate basis matrices
    # We need to rbind them while preserving the Matrix class if possible
    if (all(sapply(all_basis, is, "sparseMatrix"))) {
      # Sparse case - use rbind from the Matrix package
      new_basis <- do.call(rbind, all_basis)
    } else {
      # Mixed or dense case - convert to matrices first
      all_basis_matrix <- lapply(all_basis, as.matrix)
      new_basis_matrix <- do.call(rbind, all_basis_matrix)
      # Convert back to Matrix object
      new_basis <- Matrix::Matrix(new_basis_matrix, sparse = (Matrix::nnzero(new_basis_matrix) / length(new_basis_matrix) < 0.5))
    }
    
    # Create a new LatentNeuroVec with concatenated basis
    new_label <- paste0(
      x@label,
      ifelse(nchar(x@label) > 0, "_plus_", ""),
      length(all_objects) - 1,
      "_more"
    )
    
    # Create new LatentNeuroVec
    LatentNeuroVec(
      basis = new_basis,
      loadings = x@loadings,  # Use loadings from the first object (all are identical)
      space = new_space,
      mask = x@mask,          # Use mask from the first object (all are identical)
      offset = x@offset,      # Use offset from the first object (should check if identical?)
      label = new_label
    )
  }
)

#' Convert LatentNeuroVec to HDF5 Format (Spec Compliant - Refactored)
#' @keywords internal
#' @noRd
to_h5_latentvec <- function(vec, file_name=NULL, data_type="FLOAT",
                            compression=6, nbit=FALSE) { 

  assert_that(inherits(vec, "LatentNeuroVec"))


  if (!is.null(file_name) && !endsWith(file_name, ".lv.h5")) {
    file_name <- paste0(file_name, ".lv.h5")
  } else if (is.null(file_name)) {
    file_name <- tempfile(fileext = ".lv.h5")
  }

  fh <- open_h5(file_name, mode = "w")
  h5obj <- fh$h5
  defer(if (fh$owns) h5obj$close_all(), envir = parent.frame())

  tryCatch({
      message("[to_h5_latentvec] Writing LatentNeuroVec to: ", file_name)

      hdf5r::h5attr(h5obj, "latent_spec_version") <- "1.0"
      hdf5r::h5attr(h5obj, "rtype") <- class(vec)
      hdf5r::h5attr(h5obj, "voxel_order") <- "column-major"

      sp <- neuroim2::space(vec)
      dims_vec <- dim(sp)
      if (length(dims_vec) != 4) stop("LatentNeuroVec space must be 4D.")
      X <- dims_vec[1]; Y <- dims_vec[2]; Z <- dims_vec[3]; T_vec <- dims_vec[4]

      tmat <- trans(sp)
      q <- tryCatch(neuroim2::matrixToQuatern(tmat),
                    error = function(e) {
                       warning("Failed to convert transform matrix to quaternions: ", e$message, ". Using default.")
                       list(quaternion=c(0,0,0), qoffset=c(0,0,0), qfac=1)
                    })
      sp_spacing <- spacing(sp)
      if (length(sp_spacing) < 3) sp_spacing <- c(sp_spacing, rep(1, 3 - length(sp_spacing)))
      TR <- attr(sp, "TR") %||% 0.0

      h5dtype_internal <- switch(toupper(data_type),
                        "FLOAT"   = hdf5r::h5types$H5T_NATIVE_FLOAT,
                        "DOUBLE"  = hdf5r::h5types$H5T_NATIVE_DOUBLE,
                        hdf5r::h5types$H5T_NATIVE_FLOAT)
      if (is.null(h5dtype_internal)) stop(paste0("Invalid data_type specified: ", data_type))

      dtype_text <- h5dtype_internal$to_text()
      nifti_dt_map <- list("H5T_NATIVE_FLOAT"=16L, "H5T_IEEE_F32LE"=16L,
                           "H5T_NATIVE_DOUBLE"=64L, "H5T_IEEE_F64LE"=64L,
                           "H5T_NATIVE_INT"=8L, "H5T_STD_I32LE"=8L,
                           "H5T_NATIVE_SHORT"=4L)
      nifti_bp_map <- list("H5T_NATIVE_FLOAT"=32L, "H5T_IEEE_F32LE"=32L,
                           "H5T_NATIVE_DOUBLE"=64L, "H5T_IEEE_F64LE"=64L,
                           "H5T_NATIVE_INT"=32L, "H5T_STD_I32LE"=32L,
                           "H5T_NATIVE_SHORT"=16L)
      nifti_datatype_code <- nifti_dt_map[[dtype_text]] %||% 0L
      nifti_bitpix <- nifti_bp_map[[dtype_text]] %||% 0L
      if (nifti_datatype_code == 0L) {
          warning("Could not map data_type ", data_type, " (", dtype_text, ") to NIfTI codes.")
      }

      hdr_fields <- list(
        sizeof_hdr = 348L, data_type = "", db_name = "", extents = 0L,
        session_error = 0L, regular = "", dim_info = 0L,
        dim = c(4L, X, Y, Z, T_vec, 1L, 1L, 1L),
        intent_p1 = 0.0, intent_p2 = 0.0, intent_p3 = 0.0, intent_code = 0L,
        datatype = nifti_datatype_code, bitpix = nifti_bitpix,
        slice_start = 0L,
        pixdim = c(q$qfac %||% 1.0, sp_spacing[1], sp_spacing[2], sp_spacing[3], TR, 0.0, 0.0, 0.0),
        vox_offset = 0.0, scl_slope = 1.0, scl_inter = 0.0,
        slice_end = as.integer(Z - 1), slice_code = 0L, xyzt_units = 10L,
        cal_max = 0.0, cal_min = 0.0, slice_duration = 0.0, toffset = 0.0,
        glmax = 0L, glmin = 0L,
        descrip = paste("LatentNeuroVec data:", vec@label %||% "(no label)"),
        aux_file = "", qform_code = 1L, sform_code = 0L,
        quatern_b = q$quaternion[1], quatern_c = q$quaternion[2], quatern_d = q$quaternion[3],
        qoffset_x = q$qoffset[1], qoffset_y = q$qoffset[2], qoffset_z = q$qoffset[3],
        srow_x = c(tmat[1,1], tmat[1,2], tmat[1,3], tmat[1,4]),
        srow_y = c(tmat[2,1], tmat[2,2], tmat[2,3], tmat[2,4]),
        srow_z = c(tmat[3,1], tmat[3,2], tmat[3,3], tmat[3,4]),
        intent_name = "", magic = "n+1"
      )

      .write_header(h5obj, hdr_fields, q$qfac %||% 1.0)

      nvox <- .write_mask(h5obj, vec@mask, compression)

      basis_info <- .write_basis(h5obj, vec@loadings, h5dtype_internal,
                                 compression, vec@offset, nvox)

      scan_name <- vec@label %||% tools::file_path_sans_ext(basename(file_name))
      .write_scans(h5obj, as.matrix(vec@basis), scan_name, TR,
                   basis_info$k, T_vec, h5dtype_internal, compression)

      message("[to_h5_latentvec] HDF5 write SUCCESSFUL.")
      return(h5obj)

  }, error = function(e) {
      warning(paste0("[to_h5_latentvec] ERROR during HDF5 write: ", e$message))
      return(NULL)
  })
}

#' @keywords internal
#' @noRd
.write_header <- function(h5, hdr_fields, qfac) {
  message("[to_h5_latentvec] Writing Header Group...")
  for (nm in names(hdr_fields)) {
    h5_write(h5, file.path("/header", nm), hdr_fields[[nm]],
             dtype = guess_h5_type(hdr_fields[[nm]]), overwrite = TRUE)
  }
  h5_write(h5, "/header/qfac", qfac,
           dtype = h5types$H5T_NATIVE_DOUBLE, overwrite = TRUE)
  message("[to_h5_latentvec] Header Group DONE.")
}

#' @keywords internal
#' @noRd
.write_mask <- function(h5, mask_vol, compression) {
  message("[to_h5_latentvec] Writing Mask Dataset...")
  if (!inherits(mask_vol, "LogicalNeuroVol"))
    stop("vec@mask is not a LogicalNeuroVol")
  mask_arr <- array(as.integer(mask_vol@.Data), dim = dim(mask_vol))
  dims <- dim(mask_vol)
  mask_chunk_dim <- c(min(32, dims[1]), min(32, dims[2]), min(32, dims[3]))
  h5_write(h5, "/mask", mask_arr,
           dtype = hdf5r::h5types$H5T_NATIVE_UCHAR,
           chunk_dims = mask_chunk_dim, compression = compression,
           overwrite = TRUE)
  nvox <- sum(mask_vol)

  message("[to_h5_latentvec] Writing Voxel Coords...")
  mask_indices <- which(mask_arr == 1L)
  coords_to_write <- if (length(mask_indices) > 0) {
    coords <- arrayInd(mask_indices, .dim = dim(mask_arr))
    matrix(as.integer(coords - 1), nrow = length(mask_indices), ncol = 3)
  } else {
    matrix(integer(), nrow = 0, ncol = 3)
  }
  if (nrow(coords_to_write) != nvox)
    stop("Internal dimension mismatch: voxel_coords rows != non-zero count in mask")
  coord_chunk <- if (nrow(coords_to_write) > 0) c(min(1024, nrow(coords_to_write)), 3) else NULL
  h5_write(h5, "/voxel_coords", coords_to_write,
           dtype = hdf5r::h5types$H5T_NATIVE_INT32,
           chunk_dims = coord_chunk, compression = compression,
           overwrite = TRUE)
  return(nvox)
}

#' @keywords internal
#' @noRd
.write_basis <- function(h5, loadings, h5dtype, compression, offset, nvox_mask) {
  message("[to_h5_latentvec] Writing Basis Group (spatial components)...")
  h5$create_group("/basis")
  t_loadings <- Matrix::t(loadings)
  k <- nrow(t_loadings)
  nvox <- ncol(t_loadings)
  if (nvox != nvox_mask)
    stop(paste0("Internal dimension mismatch: spatial basis columns (", nvox,
                ") != nVox in mask (", nvox_mask, ")"))
  density <- Matrix::nnzero(t_loadings) / length(t_loadings)
  write_sparse <- density < 0.30
  message(paste0("  Spatial basis density: ", round(density*100, 2),
                 "%. Writing as ", if(write_sparse) "SPARSE" else "DENSE", "."))
  if (!write_sparse) {
    dense_chunk <- c(k, min(1024, nvox))
    h5_write(h5, "/basis/basis_matrix", as.matrix(t_loadings),
             dtype = h5dtype,
             chunk_dims = dense_chunk, compression = compression,
             overwrite = TRUE)
  } else {
    sparse_grp <- h5$create_group("/basis/basis_matrix_sparse")
    hdf5r::h5attr(sparse_grp, "storage") <- "csc"
    hdf5r::h5attr(sparse_grp, "shape") <- dim(t_loadings)
    if (!inherits(t_loadings, "dgCMatrix"))
      t_loadings <- as(t_loadings, "CsparseMatrix")
    nnz <- length(t_loadings@x)
    target_min_chunk <- 128 * 1024
    element_size <- h5dtype$get_size()
    chunk_len <- max(1L, floor(target_min_chunk / element_size))
    if (nnz > 0 && chunk_len > nnz) chunk_len <- nnz else if(nnz == 0) chunk_len <- NULL
    h5_write(sparse_grp, "data", t_loadings@x, h5dtype,
             chunk_dims = chunk_len, compression = compression,
             overwrite = TRUE)
    h5_write(sparse_grp, "indices", as.integer(t_loadings@i),
             hdf5r::h5types$H5T_NATIVE_INT32,
             chunk_dims = chunk_len, compression = compression,
             overwrite = TRUE)
    h5_write(sparse_grp, "indptr", as.integer(t_loadings@p),
             hdf5r::h5types$H5T_NATIVE_INT32,
             chunk_dims = NULL, compression = 0,
             overwrite = TRUE)
  }
  message("[to_h5_latentvec] Basis Group DONE.")
  message("[to_h5_latentvec] Writing Offset...")
  if (length(offset) > 0) {
    if (length(offset) != nvox)
      stop(paste0("Offset length (", length(offset), ") does not match spatial basis nVox (", nvox, ")"))
    h5_write(h5, "/offset", offset, dtype = h5dtype,
             chunk_dims = NULL, compression = 0, overwrite = TRUE)
  } else {
    message("  Offset is empty, skipping write.")
  }
  invisible(list(k = k, nVox = nvox))
}

#' @keywords internal
#' @noRd
.write_scans <- function(h5, embedding_matrix, scan_name, TR, k, T_vec,
                         h5dtype, compression) {
  message("[to_h5_latentvec] Writing Scans Group...")
  h5$create_group("/scans")
  scan_name <- gsub("[^a-zA-Z0-9_.-]", "_", scan_name)
  if (!nzchar(scan_name)) scan_name <- "scan_1"

  meta_path <- file.path("/scans", scan_name, "metadata")
  h5$create_group(file.path("/scans", scan_name))
  h5$create_group(meta_path)
  run_length <- nrow(embedding_matrix)
  h5_write(h5, file.path(meta_path, "run_length"), as.integer(run_length), overwrite = TRUE)
  if (TR > 0) h5_write(h5, file.path(meta_path, "TR"), as.double(TR), overwrite = TRUE)
  h5_write(h5, file.path(meta_path, "subject_id"), "", dtype=hdf5r::H5T_STRING$new(size = Inf), overwrite = TRUE)
  h5_write(h5, file.path(meta_path, "task"), "", dtype=hdf5r::H5T_STRING$new(size = Inf), overwrite = TRUE)
  h5_write(h5, file.path(meta_path, "session"), "", dtype=hdf5r::H5T_STRING$new(size = Inf), overwrite = TRUE)

  message("  Writing embedding matrix...")
  embed_dims <- dim(embedding_matrix)
  if (embed_dims[1] != T_vec)
    warning(paste0("Embedding time points (", embed_dims[1], ") does not match header time dim (", T_vec, "). Using embedding dim."))
  if (embed_dims[2] != k)
    stop(paste0("Embedding components (", embed_dims[2], ") mismatch spatial basis components (", k, ")"))
  embed_chunk <- c(min(128, embed_dims[1]), k)
  h5_write(h5, file.path("/scans", scan_name, "embedding"), embedding_matrix,
           dtype = h5dtype, chunk_dims = embed_chunk,
           compression = compression, overwrite = TRUE)
  message("[to_h5_latentvec] Scans Group DONE.")
}
#' Internal helper to write each field from a list into an HDF5 group
#' @noRd
.write_nifti_header_fields <- function(hdr_grp, fields_list) {
  for (nm in names(fields_list)) {
    val <- fields_list[[nm]]
    if (is.null(val)) next
    .write_field(hdr_grp, nm, val)
  }
}

#' Internal helper for writing a single field
#' @noRd
.write_field <- function(hdr_grp, nm, val) {
  if (is.character(val)) {
    stype <- hdf5r::H5T_STRING$new(size = Inf)
    on.exit(stype$close(), add=TRUE)
    ds    <- hdr_grp$create_dataset(nm, dims=1, dtype=stype)
    on.exit(if (!is.null(ds) && ds$is_valid) ds$close(), add = TRUE)
    ds[]  <- if (length(val) > 1) paste(val, collapse=" ") else val
  } else if (is.integer(val)) {
    ds <- hdr_grp$create_dataset(nm, dims=length(val),
                                 dtype=hdf5r::h5types$H5T_NATIVE_INT)
    on.exit(if (!is.null(ds) && ds$is_valid) ds$close(), add = TRUE)
    ds[] <- as.integer(val)
  } else if (is.numeric(val)) {
    ds <- hdr_grp$create_dataset(nm, dims=length(val),
                                 dtype=hdf5r::h5types$H5T_NATIVE_DOUBLE)
    on.exit(if (!is.null(ds) && ds$is_valid) ds$close(), add = TRUE)
    ds[] <- as.double(val)
  } else {
    warning("Writing field '", nm, "' as string due to unrecognized type: ", class(val))
    stype <- hdf5r::H5T_STRING$new(size = Inf)
    on.exit(stype$close(), add=TRUE)
    ds    <- hdr_grp$create_dataset(nm, dims=1, dtype=stype)
    on.exit(if (!is.null(ds) && ds$is_valid) ds$close(), add = TRUE)
    ds[]  <- as.character(val)
  }
}

#' Load data from a LatentNeuroVecSource object (Spec Compliant)
#'
#' @description
#' Constructs a `LatentNeuroVec` from a spec-compliant HDF5 file.
#' Reads header, mask, basis, offset, and the embedding for a selected scan.
#'
#' @param x A `LatentNeuroVecSource` specifying the HDF5 file path.
#' @param scan_name Optional `character` string specifying which scan to load under `/scans/`. 
#'   If `NULL` (default), the first scan found will be loaded.
#'
#' @return A `LatentNeuroVec` object for the specified scan.
#'
#' @details
#' Reads data according to `BasisEmbeddingSpec.yaml`:
#' 1. Reads `/header` to build `NeuroSpace`.
#' 2. Reads `/mask` to build `LogicalNeuroVol`.
#' 3. Reads `/basis/basis_matrix` -> transposes -> `object@loadings`.
#' 4. Reads `/offset` (optional) -> `object@offset`.
#' 5. Reads `/scans/<scan_name>/embedding` -> `object@basis`.
#' 6. Performs dimension consistency checks.
#'
#' @note Requires `hdf5r`.
#' @seealso `LatentNeuroVecSource`, `LatentNeuroVec`, `validate_latent_file`
#'
#' @importFrom hdf5r H5File h5attr
#' @importFrom neuroim2 NeuroSpace NeuroVol LogicalNeuroVol IndexLookupVol drop_dim quaternToMatrix
#' @noRd
setMethod(
  f = "load_data",
  signature = c("LatentNeuroVecSource"),
  definition = function(x, scan_name = NULL) {
    # --- 1. Handle File Source ---
    fh <- open_h5(x@file_name, mode = "r") # Use helper
    h5obj <- fh$h5
    # Defer closing only if we opened the file via path
    defer(if (fh$owns) h5obj$close_all(), envir = parent.frame())
        
    # --- TRY CATCH REMOVED FOR PARSING DIAGNOSIS --- 
    
    # --- 2. Check Version Attribute --- 
    version_attr <- NULL
    if (h5obj$attr_exists("latent_spec_version")) { 
        version_attr <- hdf5r::h5attr(h5obj, "latent_spec_version")
    }
    if (is.null(version_attr)) {
        warning("[load_data,LatentNeuroVecSource] HDF5 file '", x@file_name, 
                "' is missing the 'latent_spec_version' attribute. Assuming version 1.0 format.")
    } else if (version_attr != "1.0") {
        warning("[load_data,LatentNeuroVecSource] HDF5 file '", x@file_name, 
                "' has unexpected 'latent_spec_version'='", version_attr, 
                "'. Attempting to load assuming version 1.0 format.")
    }
        
    # --- 3. Read Header and Reconstruct Space --- 
    .rd_hdr <- function(nm, required=TRUE) {
      h5_read(h5obj, file.path("/header", nm), missing_ok = !required)
    }
    dims_hdr   <- .rd_hdr("dim", required=TRUE)
    pixdim_hdr <- .rd_hdr("pixdim", required=FALSE) # Allow missing, default later
    qb         <- .rd_hdr("quatern_b", required=FALSE)
    qc         <- .rd_hdr("quatern_c", required=FALSE)
    qd         <- .rd_hdr("quatern_d", required=FALSE)
    qx         <- .rd_hdr("qoffset_x", required=FALSE)
    qy         <- .rd_hdr("qoffset_y", required=FALSE)
    qz         <- .rd_hdr("qoffset_z", required=FALSE)
    qfac       <- .rd_hdr("qfac", required=FALSE) # Default to 1.0 if missing
    
    if (length(dims_hdr) < 5 || dims_hdr[1] != 4) {
        stop("Invalid '/header/dim' dimensions found.")
    }
    dims_3d <- dims_hdr[2:4]
    nTime_hdr <- dims_hdr[5]
    
    # Rebuild transform matrix
    # qfac_val <- qfac %||% 1.0 # Use utils::`%||%` or ifelse
    qfac_val <- if (is.null(qfac)) 1.0 else qfac
    spacing_3d <- if (!is.null(pixdim_hdr) && length(pixdim_hdr) >= 4) pixdim_hdr[2:4] else c(1,1,1)
    # origin_3d <- c(qx %||% 0, qy %||% 0, qz %||% 0)
    origin_3d <- c(if(is.null(qx)) 0 else qx, 
                   if(is.null(qy)) 0 else qy, 
                   if(is.null(qz)) 0 else qz)
    
    if (!all(sapply(list(qb, qc, qd), function(q) !is.null(q) && is.numeric(q)))) {
         warning("Missing or non-numeric quaternion b,c,d parameters. Using default orientation.")
         trans_mat <- diag(4)
         trans_mat[1,1] <- spacing_3d[1]; trans_mat[2,2] <- spacing_3d[2]; trans_mat[3,3] <- spacing_3d[3]
         trans_mat[1:3, 4] <- origin_3d
    } else {
         trans_mat <- tryCatch(
             neuroim2::quaternToMatrix(
                 quat     = c(qb, qc, qd),
                 origin   = origin_3d,
                 stepSize = spacing_3d,
                 qfac     = qfac_val
             ), error = function(e) {
                 warning("Error calling quaternToMatrix: ", e$message, ". Using default orientation.")
                 mat_fallback <- diag(4)
                 mat_fallback[1,1] <- spacing_3d[1]; mat_fallback[2,2] <- spacing_3d[2]; mat_fallback[3,3] <- spacing_3d[3]
                 mat_fallback[1:3, 4] <- origin_3d
                 mat_fallback
             })
    }
    
    # Create 4D NeuroSpace (needed for LatentNeuroVec)
    full_space <- NeuroSpace(dim = dims_hdr[2:5], # Use 4D dims + time
                             spacing = spacing_3d, # Use 3D spacing
                             origin = origin_3d,
                             trans = trans_mat)
    space_3d <- drop_dim(full_space) # 3D space for mask

    # --- 4. Read Mask --- 
    mask_arr <- h5_read(h5obj, "/mask", missing_ok = FALSE)
    # Check dimensions of mask array against header dims - check_same_dims stops on error
    check_same_dims(dim(mask_arr), dims_3d, dims_to_compare = 1:3, 
                      msg = paste("load_data: Dimensions of /mask mismatch header dims"))
    
    mask_vol <- LogicalNeuroVol(as.logical(mask_arr), space = space_3d)
    nVox_mask <- sum(mask_vol)

    # --- 5. Read Voxel Coords --- 
    voxel_coords <- h5_read(h5obj, "/voxel_coords", missing_ok = TRUE)
    if (!is.null(voxel_coords)) {
        if (length(dim(voxel_coords)) != 2 || dim(voxel_coords)[2] != 3) {
            warning("/voxel_coords dataset does not have dimensions [nVox, 3]. Discarding.")
            voxel_coords <- NULL
        } else if (nrow(voxel_coords) != nVox_mask) {
            warning(paste0("/voxel_coords rows (", nrow(voxel_coords),
                             ") mismatch non-zero count in mask (", nVox_mask, "). Discarding."))
            voxel_coords <- NULL
        }
    }
    if (is.null(voxel_coords) && nVox_mask > 0) {
        # Derive if missing or failed to read correctly
        warning("Deriving voxel coordinates from mask.")
        voxel_coords <- t(arrayInd(which(mask_vol@.Data), .dim = dim(mask_vol))) 
    } else if (is.null(voxel_coords) && nVox_mask == 0) {
        voxel_coords <- matrix(integer(), nrow=0, ncol=3) # Empty matrix if no voxels
    }
    # `voxel_coords` now holds the [nVox, 3] matrix, either read or derived.
    # Not strictly needed for LatentNeuroVec construction if mask & map are correct, but good practice to load.

    
    # --- 6. Read Basis Matrix -> object@loadings --- 
    basis_grp_exists <- h5obj$exists("/basis")
    if (!basis_grp_exists) stop("Mandatory group '/basis' not found.")
    internal_loadings <- NULL
    has_dense <- h5obj$exists("/basis/basis_matrix")
    has_sparse_grp <- h5obj$exists("/basis/basis_matrix_sparse")
    
    if (has_dense && has_sparse_grp) {
        stop("Found both dense '/basis/basis_matrix' and sparse '/basis/basis_matrix_sparse' group. File is invalid.")
    } else if (has_dense) {
        basis_matrix <- h5_read(h5obj, "/basis/basis_matrix", missing_ok = FALSE)
        if (length(dim(basis_matrix)) != 2) stop("Dense basis matrix is not 2-dimensional.")
        k_basis <- dim(basis_matrix)[1]
        nVox_basis <- dim(basis_matrix)[2]
        if (nVox_basis != nVox_mask) {
            stop("Dense basis matrix nVox (", nVox_basis, ") mismatch non-zero count in mask (", nVox_mask, ")")
        }
        # NOTE: Mapping HDF5 /basis/basis_matrix (spatial, k x p) -> R @loadings (spatial, p x k)
        internal_loadings <- Matrix::Matrix(t(basis_matrix))
        message("[load_data] Dense spatial basis loaded.")

    } else if (has_sparse_grp) {
        sparse_grp <- h5obj[["/basis/basis_matrix_sparse"]]
        on.exit(if (!is.null(sparse_grp) && sparse_grp$is_valid) sparse_grp$close(), add=TRUE) # Keep this specific on.exit
        # ... (read attributes) ...
        data_data <- h5_read(h5obj, "/basis/basis_matrix_sparse/data", missing_ok = FALSE)
        indices_data <- h5_read(h5obj, "/basis/basis_matrix_sparse/indices", missing_ok = FALSE)
        indptr_data <- h5_read(h5obj, "/basis/basis_matrix_sparse/indptr", missing_ok = FALSE)
        # ... (sparse matrix reconstruction assigns to internal_loadings) ...
        message("[load_data] Sparse triplet datasets read.")
        
        # Reconstruct k x p matrix
        reconstructed_k_p <- NULL
        if (storage_fmt == "csc") {
            expected_indptr_len <- shape_attr[2] + 1
            if (length(indptr_data) != expected_indptr_len) stop("CSC indptr length mismatch.")
            reconstructed_k_p <- Matrix::sparseMatrix(i = indices_data, p = indptr_data, x = data_data, 
                                                      dims = shape_attr, index1 = FALSE) # Assume 0-based indices from HDF5
        } else if (storage_fmt == "csr") {
            expected_indptr_len <- shape_attr[1] + 1
             if (length(indptr_data) != expected_indptr_len) stop("CSR indptr length mismatch.")
            # Reconstruct as if it were CSC (i=row_indices, p=row_pointers), then transpose
            # Spec: CSR stores column indices in 'indices' and row pointers in 'indptr'
            # Matrix::sparseMatrix needs row indices (i) and column pointers (p) for CSC
            # We have column indices (j) and row pointers (p_row). To build k x p using sparseMatrix:
            # This requires iterating through row pointers to determine row index for each element. 
            # SIMPLER: Build the p x k matrix directly using CSR components
            #          (Need j = col_indices, p = row_pointers)
            message("  Reconstructing CSR matrix (p x k) from triplet...")
            reconstructed_p_k <- Matrix::sparseMatrix(j = indices_data, 
                                                      p = indptr_data, 
                                                      x = data_data, 
                                                      dims = rev(shape_attr), # Use p x k dimensions
                                                      index1 = FALSE) # HDF5 indices are 0-based
            # Transpose is not needed here, as we built p x k directly
            # internal_loadings <- Matrix::t(reconstructed_p_k) # No! reconstructed_p_k is already p x k
            internal_loadings <- reconstructed_p_k # Use p x k directly
        } else {
            stop(paste0("Unsupported sparse storage format: ", storage_fmt))
        }
        
        message("[load_data] Sparse spatial basis loaded.") # No transpose needed if built correctly
        
    } else {
         stop("No basis matrix found in '/basis'.")
    }

    # --- 7. Read Offset (Optional) --- 
    offset_vec <- h5_read(h5obj, "/offset", missing_ok = TRUE)
    if (!is.null(offset_vec) && length(offset_vec) != nVox_basis) {
        warning("Offset length (", length(offset_vec), ") mismatch basis nVox (", nVox_basis, "). Using zero offset.")
        offset_vec <- NULL
    }
    if (is.null(offset_vec)) {
        offset_vec <- numeric(nVox_basis) # Default to zero offset
    }

    # --- 8. Select Scan and Read Embedding -> object@basis --- 
    scans_grp_exists <- h5obj$exists("/scans")
    if (!scans_grp_exists) stop("Mandatory group '/scans' not found.")
    scans_grp <- h5obj[["/scans"]]
    available_scans <- scans_grp$ls()$name
    if (length(available_scans) == 0) stop("No scans found under '/scans'.")
    
    target_scan_name <- scan_name
    if (is.null(target_scan_name)) {
        target_scan_name <- available_scans[1]
        if (length(available_scans) > 1) {
            warning("'scan_name' not specified, loading first scan found: ", target_scan_name)
        }
    } else if (!(target_scan_name %in% available_scans)) {
        stop("Requested scan_name '", target_scan_name, "' not found in file. Available: ", 
             paste(available_scans, collapse=", "))
    }
    
    # No need to open scan_grp, h5_read handles full path
    embedding_path <- file.path("/scans", target_scan_name, "embedding")
    # NOTE: Mapping HDF5 /scans/.../embedding (temporal, n x k) -> R @basis (temporal, n x k)
    internal_basis <- h5_read(h5obj, embedding_path, missing_ok = FALSE)
    if (length(dim(internal_basis)) != 2) stop("Embedding matrix is not 2-dimensional.")
    nTime_embed <- dim(internal_basis)[1]
    k_embed <- dim(internal_basis)[2]
    
    # Final consistency checks
    if (k_embed != k_basis) {
        stop("Component mismatch: embedding k (", k_embed, ") != basis k (", k_basis, ")")
    }
    if (nTime_embed != nTime_hdr) {
        stop("Time mismatch: embedding time (", nTime_embed, ") != header time (", nTime_hdr, ")")
    }

    # --- 9. Construct LatentNeuroVec --- 
    LatentNeuroVec(basis    = Matrix::Matrix(internal_basis), 
                   loadings = internal_loadings, 
                   space    = full_space, 
                   mask     = mask_vol, 
                   offset   = offset_vec,
                   label    = target_scan_name)
    # --- END OF REMOVED TRY CATCH BLOCK --- 
} # End definition function
) # End setMethod

#' Validate HDF5 File Against Latent Spec
#'
#' @description
#' Performs basic checks on an HDF5 file to verify if it conforms to the
#' essential structure and dimension consistency defined by the
#' `BasisEmbeddingSpec.yaml` specification.
#'
#' @param file_path Character string specifying the path to the HDF5 file.
#'
#' @return Logical `TRUE` if basic checks pass, `FALSE` otherwise. Issues
#'   warnings for inconsistencies found. Throws an error if the file cannot
#'   be opened or fundamental groups/datasets are missing.
#'
#' @details
#' Checks performed:
#' \itemize{
#'   \item File existence and HDF5 readability.
#'   \item Presence of mandatory groups: `/header`, `/basis`, `/scans`.
#'   \item Presence of mandatory datasets: `/header/dim`, `/mask`, `/basis/basis_matrix`,
#'         at least one scan group under `/scans`, and its `embedding` dataset.
#'   \item Dimension consistency:
#'     \itemize{
#'       \item `/header/dim[2:4]` vs `/mask` dimensions.
#'       \item `/basis/basis_matrix` columns vs number of non-zero elements in `/mask`.
#'       \item `/basis/basis_matrix` rows (k) vs embedding columns (k).
#'       \item `/header/dim[5]` (time) vs embedding rows (time).
#'       \item Optional: `/offset` length vs `/basis/basis_matrix` columns.
#'     }
#' }
#' Does NOT validate header field *values* extensively beyond dimensions, nor does it
#' check data types rigorously.
#'
#' @importFrom hdf5r H5File h5attr
#' @export
validate_latent_file <- function(file_path) {
  if (!is.character(file_path) || length(file_path) != 1 || !nzchar(file_path)) {
    stop("[validate_latent_file] 'file_path' must be a single character string.")
  }
  if (!file.exists(file_path)) {
    stop("[validate_latent_file] File not found: '", file_path, "'")
  }

  is_valid <- TRUE
  error_message <- NULL
  fh <- NULL # Initialize fh for potential use in finally block

  tryCatch({
    # --- 1. Handle File Source ---
    fh <- open_h5(file_path, mode = "r") # Use helper
    h5obj <- fh$h5
    # Defer closing only if we opened the file via path
    # CRITICAL: Ensure this defer is correctly placed to run after tryCatch completes
    defer(if (!is.null(fh) && fh$owns) try(h5obj$close_all(), silent = TRUE), envir = parent.frame())

    # --- Check for mandatory groups/datasets using h5obj$exists ---
    required_groups <- c("header", "basis", "scans")
    required_datasets <- c("mask") # Root datasets
    required_header_datasets <- c("dim")
    # required_basis_datasets <- c("basis_matrix") # Or basis_matrix_sparse

    for (grp_name in required_groups) {
      if (!h5obj$exists(grp_name)) {
        stop(paste0("Mandatory group '/", grp_name, "' not found."))
      }
    }
    for (ds_name in required_datasets) {
      if (!h5obj$exists(ds_name)) {
        stop(paste0("Mandatory dataset '/", ds_name, "' not found."))
      }
    }
    # Check header datasets using full path
    for (ds_name in required_header_datasets) {
       hdr_ds_path <- file.path("/header", ds_name)
       if (!h5obj$exists(hdr_ds_path)) {
           stop(paste0("Mandatory dataset '", hdr_ds_path, "' not found."))
       }
    }
    # Check basis dataset/group using full path
    basis_dense_path <- "/basis/basis_matrix"
    basis_sparse_path <- "/basis/basis_matrix_sparse"
    has_dense_basis <- h5obj$exists(basis_dense_path)
    has_sparse_basis <- h5obj$exists(basis_sparse_path)
    if (!has_dense_basis && !has_sparse_basis) {
        stop("Mandatory dataset '/basis/basis_matrix' or group '/basis/basis_matrix_sparse' not found.")
    }
    if (has_dense_basis && has_sparse_basis) {
         stop("Found both dense '/basis/basis_matrix' and sparse '/basis/basis_matrix_sparse'. File is invalid.")
    }
    # Check scans and embedding
    if (!h5obj$exists("/scans")) stop("Mandatory group '/scans' not found.")
    # Need to list scans without opening scans_grp handle long-term
    scan_names <- tryCatch(h5obj[["scans"]]$ls()$name, finally = try(h5obj[["scans"]]$close(), silent=TRUE))
    if (length(scan_names) == 0) {
        stop("No scan subgroups found under '/scans'.")
    }
    first_scan_name <- scan_names[1]
    embed_path <- file.path("/scans", first_scan_name, "embedding")
    if (!h5obj$exists(embed_path)) {
        stop(paste0("Mandatory dataset '", embed_path, "' not found."))
    }

    # --- Read dimensions for consistency checks using h5_read or direct dim access ---
    header_dim <- h5_read(h5obj, "/header/dim", missing_ok = FALSE)
    mask_data <- h5_read(h5obj, "/mask", missing_ok = FALSE)
    mask_dim <- dim(mask_data)

    basis_dim <- NULL
    k_basis <- NA_integer_
    nVox_basis <- NA_integer_
    if (has_dense_basis) {
        # Read dims directly from dataset handle for efficiency
        basis_dset <- NULL
        tryCatch({
           basis_dset <- h5obj[[basis_dense_path]]
           basis_dim <- basis_dset$dims
        }, finally= if(!is.null(basis_dset) && basis_dset$is_valid) try(basis_dset$close(), silent=TRUE))
    } else { # has_sparse_basis
         # Read shape attribute
         sparse_grp <- NULL
         tryCatch({
            sparse_grp <- h5obj[[basis_sparse_path]]
            if (!hdf5r::h5attr_exists(sparse_grp, "shape")) stop("Sparse basis group missing 'shape' attribute.")
            basis_dim <- hdf5r::h5attr(sparse_grp, "shape") # Should be [k, nVox_basis]
         }, finally= if(!is.null(sparse_grp) && sparse_grp$is_valid) try(sparse_grp$close(), silent=TRUE))
    }
    if (is.null(basis_dim) || length(basis_dim) != 2) stop("Failed to read valid 2D dimensions for basis.")
    k_basis <- basis_dim[1]
    nVox_basis <- basis_dim[2]

    # Read embedding dims directly
    embed_dset <- NULL
    embed_dim <- NULL
     tryCatch({
       embed_dset <- h5obj[[embed_path]]
       embed_dim <- embed_dset$dims # Should be [T_emb, k]
     }, finally= if(!is.null(embed_dset) && embed_dset$is_valid) try(embed_dset$close(), silent=TRUE))
    if (is.null(embed_dim) || length(embed_dim) != 2) stop("Failed to read valid 2D dimensions for embedding.")
    T_emb <- embed_dim[1]
    k_embed <- embed_dim[2]


    # --- Perform Consistency Checks ---
    valid_checks <- list()

    # Check 1: Header dim[0] should be 4
    if (length(header_dim) < 5 || header_dim[1] != 4) {
        valid_checks$hdr_dim0 <- paste0("'/header/dim' should start with 4, but starts with ", header_dim[1])
    }
    # Check 2: Header spatial dims vs Mask dims
    dim_check_msg <- validate_same_dims(header_dim[2:4], mask_dim, dims_to_compare = 1:3)
    if (!is.null(dim_check_msg)) {
        valid_checks$hdr_mask_dims <- paste0("'/header/dim[2:4]' (", paste(header_dim[2:4], collapse=","),
                                             ") mismatch '/mask' dims (", paste(mask_dim, collapse=","), ")")
    }
    # Check 3: Basis components (k) vs Embedding components (k)
    if (k_basis != k_embed) {
        valid_checks$k_mismatch <- paste0("Component dimension mismatch: basis k (", k_basis,
                                           ") != embedding k (", k_embed, ")")
    }
    # Check 4: Header time (T_hdr) vs Embedding time (T_emb)
    T_hdr <- header_dim[5]
    if (T_hdr != T_emb) {
        valid_checks$time_mismatch <- paste0("Time dimension mismatch: header dim[5] (", T_hdr,
                                             ") != embedding rows (", T_emb, ")")
    }
    # Check 5: Basis nVox vs Mask non-zero count
    nVox_mask <- sum(mask_data > 0) # Assuming mask is 0/1
    if (nVox_basis != nVox_mask) {
        valid_checks$basis_nvox_mask <- paste0("Basis nVox mismatch: basis nVox (", nVox_basis,
                                               ") != non-zero count in /mask (", nVox_mask, ")")
    }
    # Check 6: Optional Offset length
    offset_val <- h5_read(h5obj, "/offset", missing_ok=TRUE)
    if (!is.null(offset_val)) {
        offset_len <- length(offset_val)
        if (offset_len != nVox_basis) {
            valid_checks$offset_len <- paste0("Offset length mismatch: offset length (", offset_len,
                                              ") != basis nVox (", nVox_basis, ")")
        }
    } # else: offset doesn't exist, no check needed

    if (length(valid_checks) > 0) {
        is_valid <- FALSE
        warning("[validate_latent_file] Validation failed for '", file_path, "' with issues:\n",
                paste("  - ", unlist(valid_checks), collapse="\n"))
    }

  }, error = function(e) {
    is_valid <<- FALSE # Modify variable in parent env
    error_message <<- paste("Error during validation: ", e$message) # Capture error message
    # Ensure h5obj is closed by on.exit, which should still trigger
  })

  if (!is.null(error_message)) {
      # Avoid stop() here if defer needs to run reliably? Just issue warning?
      warning(paste("[validate_latent_file] ", error_message)) # Use warning instead of stop
      # Let the function return is_valid=FALSE
  }

  return(is_valid)
}


#' @export
#' @rdname series-methods
setMethod(
  f = "series",
  signature = signature(x="LatentNeuroVec", i="integer"),
  definition = function(x, i, j, k, drop=TRUE) {
    nTime  <- dim(x)[4]
    nels3d <- prod(dim(x)[1:3])

    # Pre-calculate transposed loadings (k x nVox_in_mask)
    t_loadings <- t(x@loadings)

    # CASE A: user gave only i -> interpret as multiple 3D voxel indices
    if (missing(j) && missing(k)) {
      if (any(i < 1 | i > nels3d)) {
        stop("Some voxel index in 'i' is out of range [1..(X*Y*Z)].")
      }
      n_vox_req <- length(i) # Number of requested voxels

      # Map requested 3D indices (i) to rows in loadings/offset (mask indices)
      # rowmap will have 0 for voxels outside the mask
      rowmap <- lookup(x@map, i)
      valid_mask_indices <- rowmap[rowmap > 0] # Indices within loadings/offset that are needed
      map_req_to_valid <- which(rowmap > 0)   # Mapping from requested index pos to valid index pos

      if (length(valid_mask_indices) == 0) {
          # All requested voxels are outside the mask
          if (n_vox_req == 1) {
            return(numeric(nTime)) # Return vector if only 1 voxel requested
          } else {
            return(matrix(0, nrow = nTime, ncol = n_vox_req)) # Return matrix otherwise
          }
      }

      # Use tcrossprod: basis (nTime x k) %*% loadings[valid_mask_indices, k]^T => (nTime x length(valid_mask_indices))
      valid_vox_series <- tcrossprod(x@basis, x@loadings[valid_mask_indices, , drop = FALSE])

      # Add offset (only for the valid mask indices)
      # Need to transpose offset subset to broadcast correctly with sweep
      valid_vox_series <- sweep(valid_vox_series, 2, x@offset[valid_mask_indices], "+")

      # Create the final output matrix, filled with 0s
      out_mat <- matrix(0, nrow = nTime, ncol = n_vox_req)
      # Place the calculated series into the correct columns
      out_mat[, map_req_to_valid] <- as.numeric(valid_vox_series)

      if (drop && n_vox_req == 1) {
        return(drop(out_mat))
      } else {
        return(out_mat)
      }

    } else {
      # CASE B: user gave i,j,k => each must be length 1 => single voxel
      if (!(length(i) == 1 && length(j) == 1 && length(k) == 1)) {
        stop("series(x, i,j,k): i,j,k must each be a single integer for one voxel.")
      }
      # convert (i,j,k) => 3D linear index
      idx_1d <- i + (j-1)*dim(x)[1] + (k-1)*dim(x)[1]*dim(x)[2]
      if (idx_1d<1 || idx_1d>nels3d) {
        stop("Voxel subscript (i,j,k) out of range for LatentNeuroVec.")
      }
      # map => row in loadings
      mr <- lookup(x@map, idx_1d)
      # if out-of-mask => entire time series is 0
      if (mr < 1) {
        return(numeric(nTime))
      }

      # Use tcrossprod: basis (nTime x k) %*% loadings[mr, k]^T => (nTime x 1)
      out_vec <- tcrossprod(x@basis, x@loadings[mr, , drop = FALSE])
      # Add the single offset value
      out_vec <- out_vec + x@offset[mr]

      if (drop) drop(out_vec) else out_vec
    }
  }
)

#' @export
#' @rdname series-methods
setMethod(
  f = "series",
  signature = signature(x="LatentNeuroVec", i="numeric"),
  definition = function(x, i, j, k, drop=TRUE) {
    # Cast i,j,k to integer if provided
    if (!missing(i)) i <- as.integer(i)
    if (!missing(j)) j <- as.integer(j)
    if (!missing(k)) k <- as.integer(k)
    callGeneric(x, i=i, j=j, k=k, drop=drop)
  }
)


#' @importFrom crayon bold red green blue yellow silver
#' @importFrom utils object.size
#' @rdname show-methods
#' @export
setMethod(
  f = "show",
  signature = "LatentNeuroVec",
  definition = function(object) {
    # Header
    cat("\n", crayon::bold(crayon::blue("LatentNeuroVec Object")), "\n")
    cat(crayon::silver("══════════════════════\n"))

    # Dimensions
    dims <- dim(object)
    spatial_dims <- paste(dims[1:3], collapse=" × ")
    cat("\n", crayon::yellow("Dimensions:"), "\n")
    cat(" ", crayon::silver("•"), " Spatial: ", crayon::green(spatial_dims), "\n")
    cat(" ", crayon::silver("•"), " Temporal: ", crayon::green(dims[4]), "\n")

    # Components
    n_components <- ncol(object@basis)
    first_basis_coeffs <- format(object@basis[1:min(5, nrow(object@basis)), 1], digits=3)
    cat("\n", crayon::yellow("Components:"), "\n")
    cat(" ", crayon::silver("•"), " Number: ", crayon::green(n_components), "\n")
    cat(" ", crayon::silver("•"), " First component, first 5 coeffs: ",
        crayon::green(paste(first_basis_coeffs, collapse=", ")), 
        if (length(first_basis_coeffs) < 5) "" else "...", "\n") # Adjusted description

    # Memory Usage
    basis_size    <- format(object.size(object@basis), units="auto")
    loadings_size <- format(object.size(object@loadings), units="auto")
    total_size    <- format(object.size(object),          units="auto")

    cat("\n", crayon::yellow("Memory Usage:"), "\n")
    cat(" ", crayon::silver("•"), " Basis: ",    crayon::green(basis_size),    "\n")
    cat(" ", crayon::silver("•"), " Loadings: ", crayon::green(loadings_size), "\n")
    cat(" ", crayon::silver("•"), " Total: ",    crayon::green(total_size),    "\n")

    # Sparsity
    n_nonzero <- sum(object@mask)
    sparsity <- round(100 * n_nonzero / prod(dims[1:3]), 2)
    cat("\n", crayon::yellow("Sparsity:"), "\n")
    cat(" ", crayon::silver("•"), " Non-zero voxels: ", crayon::green(n_nonzero), "\n")
    cat(" ", crayon::silver("•"), " Coverage: ",       crayon::green(sparsity), "%\n")

    # Space Info
    sp <- space(object)
    spacing_str <- paste(round(spacing(sp), 2), collapse=" × ")
    origin_str  <- paste(round(origin(sp), 2), collapse=" × ")
    cat("\n", crayon::yellow("Space Information:"), "\n")
    cat(" ", crayon::silver("•"), " Spacing: ", crayon::green(spacing_str), "\n")
    cat(" ", crayon::silver("•"), " Origin:  ", crayon::green(origin_str),  "\n")

    # Footer
    cat("\n", crayon::bold("Data Access:"), "\n")
    cat("\n", crayon::yellow("Reconstructed Space Access:"), "\n")
    cat(" ", crayon::silver("•"), " Extract volume: ",
        crayon::blue("object[[i]]"),
        crayon::silver("  # 3D volume at timepoint i\n"))
    cat(" ", crayon::silver("•"), " Get value: ",
        crayon::blue("object[i]"),
        crayon::silver("  # Value at linear index i\n"))
    cat(" ", crayon::silver("•"), " Subset: ",
        crayon::blue("object[mask]"),
        crayon::silver("  # Values at mask positions\n"))

    cat("\n", crayon::yellow("Latent Space Access:"), "\n")
    cat(" ", crayon::silver("•"), " Basis vectors: ",
        crayon::blue("basis(object)"),
        crayon::silver("  # n×k temporal basis\n"))
    cat(" ", crayon::silver("•"), " Loadings: ",
        crayon::blue("loadings(object)"),
        crayon::silver("  # p×k spatial loadings\n"))
    cat(" ", crayon::silver("•"), " Components: ",
        crayon::blue("components(object)"),
        crayon::silver("  # List of k component volumes\n"))

    cat("\n", crayon::yellow("Conversions:"), "\n")
    cat(" ", crayon::silver("•"), " as.array(object): ",
        crayon::silver("4D reconstruction\n"))
    cat(" ", crayon::silver("•"), " as.matrix(object): ",
        crayon::silver("n×p matrix of reconstructed values\n"))

    cat("\n", crayon::silver("Note: All access methods reconstruct data (X = B × L^T + offset)"),
        "\n", crayon::silver("unless you're explicitly accessing latent space."), "\n\n")
  }
)

#' Validity Check for LatentNeuroVec Objects
#' 
#' @param object A LatentNeuroVec object
#' @return TRUE if the object is valid, otherwise a character vector of error messages.
#' @keywords internal
#' @noRd
.validate_LatentNeuroVec <- function(object) {
    
    errors <- character()
    
    # Check types
    if (!inherits(object@basis, "Matrix")) {
        errors <- c(errors, "Slot @basis must be a Matrix object.")
    }
    if (!inherits(object@loadings, "Matrix")) {
        errors <- c(errors, "Slot @loadings must be a Matrix object.")
    }
    if (!is.numeric(object@offset)) {
        errors <- c(errors, "Slot @offset must be numeric.")
    }
    if (!inherits(object@mask, "LogicalNeuroVol")) {
        errors <- c(errors, "Slot @mask must be a LogicalNeuroVol object.")
    }
    if (!inherits(object@map, "IndexLookupVol")) {
        errors <- c(errors, "Slot @map must be an IndexLookupVol object.")
    }
    if (!is.character(object@label) || length(object@label) != 1) {
        errors <- c(errors, "Slot @label must be a single character string.")
    }
    if (!inherits(object@space, "NeuroSpace")) {
         errors <- c(errors, "Slot @space must be a NeuroSpace object.")
         # Stop further checks if space is invalid, as dims depend on it
         return(errors)
    }
    
    # If types are okay, check dimensions and consistency
    if (length(errors) == 0) {
        s_dims <- dim(object@space)
        if (length(s_dims) != 4) {
            errors <- c(errors, "Slot @space must have 4 dimensions.")
        } else {
            if (ncol(object@basis) != ncol(object@loadings)) {
                errors <- c(errors, paste0("Component mismatch: ncol(@basis) = ", ncol(object@basis), 
                                           " != ncol(@loadings) = ", ncol(object@loadings)))
            }
            
            if (nrow(object@basis) != s_dims[4]) {
                 errors <- c(errors, paste0("Time mismatch: nrow(@basis) = ", nrow(object@basis), 
                                            " != dim(@space)[4] = ", s_dims[4]))
            }
            
            dim_check_result <- validate_same_dims(
                object@mask, 
                object@space, 
                dims_to_compare = 1:3, 
                msg = "[.validate_LatentNeuroVec] Mask/Space dim mismatch:"
            )
            if (!is.null(dim_check_result)) {
                errors <- c(errors, dim_check_result)
            }

           
            nVox_mask <- sum(object@mask)
            if (nrow(object@loadings) != nVox_mask) {
                errors <- c(errors, paste0("Loadings rows (", nrow(object@loadings), 
                                           ") mismatch non-zero count in mask (", nVox_mask, ")"))
            }
           
            if (length(object@offset) > 0 && length(object@offset) != nrow(object@loadings)) {
                errors <- c(errors, paste0("Offset length (", length(object@offset), 
                                           ") mismatch number of rows in loadings (", nrow(object@loadings), ")"))
            }
           
            if (length(object@map@indices) != nVox_mask) {
                 errors <- c(errors, paste0("Map indices length (", length(object@map@indices), 
                                            ") mismatch non-zero count in mask (", nVox_mask, ")"))
            }
        }
    }

    if (length(errors) == 0) TRUE else errors
}


#' @keywords internal
setValidity("LatentNeuroVec", .validate_LatentNeuroVec)

# --- Accessor Methods for LatentNeuroVec --- 

#' @export
#' @rdname basis-methods
setMethod("basis", "LatentNeuroVec", function(x) x@basis)

#' @export
#' @rdname loadings-methods
setMethod("loadings", "LatentNeuroVec", function(x) x@loadings)

#' @export
#' @rdname offset-methods
setMethod("offset", "LatentNeuroVec", function(x) x@offset)

#' @export
#' @rdname mask-methods
setMethod("mask", "LatentNeuroVec", function(x) x@mask)

#' @export
#' @rdname map-methods
setMethod("map", "LatentNeuroVec", function(x) x@map)



#' Create HDF5 Dataset Property List
#' 
#' Internal helper to create and configure an HDF5 dataset creation property list (H5P_DATASET_CREATE).
#' Handles setting fill value, chunking, and compression (deflate/gzip) or nbit filter.
#'
#' @param dims The dimensions of the dataset for which the plist is being created.
#' @param dtype An H5T object representing the dataset data type.
#' @param chunk_dims A numeric vector specifying chunk dimensions. If NULL, no chunking is set. 
#'                   Must match the length of `dims`. Values are capped by `dims`.
#' @param compression Integer [0..9] specifying gzip compression level. 0 means no compression. Default 6. Ignored if nbit=TRUE.
#' @param nbit Logical, whether to use n-bit filter. Default FALSE. Takes precedence over compression.
#' @return A *copy* of the configured H5P_DATASET_CREATE object. The original is closed via on.exit.
#' @keywords internal
#' @noRd
.create_dset_plist <- function(dims, dtype, chunk_dims = NULL, compression = 6, nbit = FALSE) {
    plist <- hdf5r::H5P_DATASET_CREATE$new()
    # IMPORTANT: Ensure plist is closed when this function exits, even on error
    # Do NOT add the plist itself to the calling function's on.exit stack if returning a copy
    on.exit(plist$close(), add = TRUE) 
    
    # Set fill value based on dtype
    fill_val <- tryCatch({
        # First check that dtype is an H5T object with methods
        if (!inherits(dtype, "H5T") || !is.function(dtype$get_class)) {
            # For non-H5T objects, use a default value and issue a message
            message("Using default fill value 0 for non-H5T dtype")
            return(0)
        }
        
        dtype_class_char <- dtype$get_class()$to_text()
        # Expanded handling of HDF5 types with additional type classes 
        switch(dtype_class_char,
               # Float types (32-bit)
               "H5T_FLOAT"      = 0.0,
               "H5T_NATIVE_FLOAT" = 0.0,
               "H5T_IEEE_F32LE" = 0.0,
               "H5T_IEEE_F32BE" = 0.0,
               
               # Double types (64-bit)
               "H5T_DOUBLE"     = 0.0,
               "H5T_NATIVE_DOUBLE" = 0.0,
               "H5T_IEEE_F64LE" = 0.0,
               "H5T_IEEE_F64BE" = 0.0,
               
               # Integer types
               "H5T_INTEGER"    = 0L,
               "H5T_NATIVE_INT" = 0L,
               "H5T_NATIVE_INT8" = 0L,
               "H5T_NATIVE_INT16" = 0L,
               "H5T_NATIVE_INT32" = 0L,
               "H5T_NATIVE_INT64" = 0L,
               "H5T_STD_I8LE"   = 0L,
               "H5T_STD_I16LE"  = 0L,
               "H5T_STD_I32LE"  = 0L,
               "H5T_STD_I64LE"  = 0L,
               
               # Unsigned integer types
               "H5T_NATIVE_UINT" = 0L,
               "H5T_NATIVE_UINT8" = 0L,
               "H5T_NATIVE_UINT16" = 0L,
               "H5T_NATIVE_UINT32" = 0L,
               "H5T_NATIVE_UINT64" = 0L,
               "H5T_STD_U8LE"   = 0L,
               "H5T_STD_U16LE"  = 0L,
               "H5T_STD_U32LE"  = 0L,
               "H5T_STD_U64LE"  = 0L,
               
               # Character/string types
               "H5T_STRING"     = "",
               "H5T_C_S1"       = "",
               
               # Catch-all for other types
               {
                 # More informative message with dtype info
                 # Safely check for the existence of get_size method
                 dtype_size <- if (is.function(dtype$get_size)) dtype$get_size() else "unknown"
                 message("Using default fill value 0 for HDF5 dtype: ", dtype_class_char, 
                         " (", dtype_size, " bytes)")
                 0L  # Default to integer zero for unknown types
               }
        )
        }, error = function(e) {
            # Improved error handling
            message(paste0("Could not determine HDF5 fill value, using default 0. Error: ", e$message))
            0  # Default to numeric zero on error
        })
    
    tryCatch(plist$set_fill_value(dtype = dtype, value = fill_val), 
             error = function(e) warning("Could not set fill value for HDF5 dataset: ", e$message))
    
    # Set chunking if provided and valid
    if (!is.null(chunk_dims)) {
          # Get rank, handling NULL dims (scalar) and vectors (rank 1)
          data_rank <- if (is.null(dims)) 0 else length(dims)
          chunk_rank <- length(chunk_dims)

          if (chunk_rank == data_rank) {
              # Ensure chunk dims don't exceed dataset dims and are >= 1
              dims_vec <- dims # Use original dims for pmin
              safe_chunks <- pmax(1L, as.integer(pmin(chunk_dims, dims_vec)))
              plist$set_chunk(safe_chunks)
          } else {
              warning("Chunk dimensions length (", chunk_rank, ") does not match dataset rank (", data_rank, "). Ignoring chunking.")
          }
     }

    # Set compression: nbit takes precedence
    if (nbit) {
        # hdf5r doesn't have a direct set_nbit(). User needs external plugin.
        # We warn but don't explicitly set a filter here.
        warning("N-bit filter requested (nbit=TRUE). This requires a registered HDF5 filter plugin. Ensure it is available in your HDF5 library.")
        # Placeholder: If hdf5r gains support or a known filter ID exists:
        # tryCatch(plist$set_filter(filter_id = H5FILTER_NBIT, dcpl = ...), error = ... ) 
        if (compression > 0) {
            warning("N-bit filter requested; gzip compression level (", compression, ") will be ignored.")
        }
    } else if (compression > 0 && compression <= 9) {
        # Only set deflate if nbit is FALSE and compression > 0
        plist$set_deflate(as.integer(compression))
    }
    
    # Return a copy for the caller to use (caller is responsible for closing the copy)
    return(plist$copy())
}

#' Write Dense HDF5 Dataset
#'
#' Internal helper function to write a dense R array/vector to an HDF5 dataset.
#'
#' @param h5_group An open H5Group or H5File object where the dataset will be created.
#' @param name The name for the new dataset.
#' @param data The R data (vector, matrix, array) to write.
#' @param dtype An H5T object for the dataset datatype.
#' @param chunk_dims Optional chunk dimensions (numeric vector). See `.create_dset_plist`.
#' @param compression Integer [0..9] for gzip level. Default 6. Ignored if nbit=TRUE.
#' @param nbit Logical, whether to use n-bit filter. Default FALSE.
#' @return Invisible NULL. Called for side effects.
#' @keywords internal
#' @noRd
.write_h5_dataset <- function(h5_group, name, data, dtype, chunk_dims = NULL, compression = 6, nbit = FALSE) {
    dims <- dim(data)
    if (is.null(dims)) dims <- length(data) # Handle 1D vectors
    
    if (any(dims == 0) && !is.null(dims)) { # Handle zero-extent dimensions (e.g., empty matrix) 
        message(paste0("  Skipping write for dataset '", h5_group$get_obj_name(), "/", name, "' due to zero-length dimension."))
        # Create empty dataset for spec compliance
        space <- hdf5r::H5S$new(dims = dims, maxdims = dims)
        on.exit(space$close(), add = TRUE)
        # No chunk/compress/nbit for empty datasets
        plist <- .create_dset_plist(dims, dtype, chunk_dims = NULL, compression = 0, nbit = FALSE) 
        on.exit(plist$close(), add = TRUE) # Close the *copied* plist
        dset <- h5_group$create_dataset(name = name, space = space, dtype = dtype, dataset_create_pl = plist)
        dset$close()
        return(invisible(NULL))
    }    

    space <- NULL; plist <- NULL; dset <- NULL # Initialize for on.exit
    on.exit({
        if (!is.null(dset) && dset$is_valid) dset$close()
        if (!is.null(plist) && plist$is_valid) plist$close() # Close the *copied* plist
        if (!is.null(space) && space$is_valid) space$close()
    }, add = TRUE)

    space <- hdf5r::H5S$new(dims = dims, maxdims = dims)
    plist <- .create_dset_plist(dims, dtype, chunk_dims, compression, nbit) # Pass nbit
    dset <- h5_group$create_dataset(name = name, space = space, dtype = dtype, dataset_create_pl = plist)
    
    # Write data using appropriate subsetting based on rank
    rank <- length(dims)
    if (rank == 0 || rank == 1) { 
        dset[] <- data 
    } else if (rank == 2) {
        dset[,] <- data
    } else if (rank == 3) {
        dset[,,] <- data
    } else if (rank == 4) {
        dset[,,,] <- data
    } else {
        stop(".write_h5_dataset currently only supports up to 4 dimensions.")
    }
    message(paste0("  Dataset '", h5_group$get_obj_name(), "/", name, "' written."))
    invisible(NULL)
}

#' Convert LatentNeuroVec to Array
#'
#' @description
#' Converts a \code{LatentNeuroVec} to a standard R 4D array.
#' 
#' @param x A \code{LatentNeuroVec} object to convert.
#' @param ... Not used.
#'
#' @return A 4D array representing the full reconstructed data.
#'
#' @details
#' This method reconstructs the full 4D array from the latent representation, which can be 
#' memory-intensive for large datasets. It calculates:
#' \deqn{array[,,, t] = Map(basis[t,] \%*\% t(loadings)) + offset}
#' for each time point, where values outside the mask are set to zero.
#'
#' @export
setMethod(
  f = "as.array",
  signature = signature(x = "LatentNeuroVec"),
  definition = function(x, ...) {
    # Get dimensions from the space
    space_dims <- dim(x)
    
    # Initialize the output array
    result_array <- array(0, dim = space_dims)
    
    # Mask information
    mask_array <- as.logical(as.array(x@mask))
    mask_indices <- which(mask_array)

    for (t in seq_len(space_dims[4])) {
      # Get the basis vector for this time point
      basis_t <- x@basis[t, , drop = FALSE]
      
      # Calculate time point using matrix multiplication: basis_t %*% t(loadings)
      # tcrossprod is more efficient for this operation
      values_in_mask <- as.vector(tcrossprod(basis_t, x@loadings)) + x@offset
      
      # Map these values back to 3D space using the mask 
      # Initialize a 3D array for this time point
      vol_3d <- array(0, dim = space_dims[1:3])
      
      # Place values at the correct indices
      vol_3d[mask_indices] <- values_in_mask
      
      # Assign to the corresponding time slice in the result
      result_array[,,,t] <- vol_3d
    }
    
    return(result_array)
  }
)
</file>

</files>
